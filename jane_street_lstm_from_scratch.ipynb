{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street Real-Time Market Data Forecasting: LSTM from Scratch\n",
    "\n",
    "This notebook implements an LSTM from scratch using PyTorch for the Jane Street competition.\n",
    "\n",
    "## Competition Overview\n",
    "\n",
    "**Goal:** Predict financial market responses for real-time trading decisions\n",
    "\n",
    "**Challenge:**\n",
    "- **130 anonymized features** representing market data\n",
    "- **Multiple responders** (resp_1, resp_2, resp_3, resp_4, resp) - different time horizons\n",
    "- **Time-ordered data** with date_id and time_id\n",
    "- **Weight column** - importance of each prediction\n",
    "- **Real-time inference** constraint\n",
    "\n",
    "**Key Difficulties:**\n",
    "1. Concept drift - relationships change over time\n",
    "2. Low signal-to-noise ratio\n",
    "3. High-frequency data with volatility\n",
    "4. Multicollinearity among features\n",
    "5. Need for fast inference\n",
    "\n",
    "## Why LSTM?\n",
    "\n",
    "**Advantages for this problem:**\n",
    "- Captures temporal dependencies in market data\n",
    "- Cell state provides long-term memory\n",
    "- Gates handle regime changes (different market conditions)\n",
    "- Can learn which features matter over time\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "1. **LSTM Cell from scratch** - understand gating mechanisms\n",
    "2. **Complete LSTM model** for sequence modeling\n",
    "3. **Data preprocessing** - feature engineering & normalization\n",
    "4. **Training pipeline** - with proper validation\n",
    "5. **Evaluation metrics** - weighted RÂ² score\n",
    "6. **Inference optimization** - for real-time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Jane Street-like Data\n",
    "\n",
    "Since we don't have the actual competition data, we'll create synthetic data that mimics the characteristics:\n",
    "- 130 features (mix of trend, stationary, and noisy)\n",
    "- Time-ordered sequences\n",
    "- Multiple responders\n",
    "- Weights\n",
    "- Concept drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jane_street_like_data(n_samples=100000, n_features=130, seq_length=10):\n",
    "    \"\"\"\n",
    "    Generate synthetic data mimicking Jane Street competition.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of time points\n",
    "        n_features: Number of features (130 in actual competition)\n",
    "        seq_length: Sequence length for LSTM\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with features and responders\n",
    "    \"\"\"\n",
    "    print(\"Generating synthetic Jane Street-like data...\")\n",
    "    \n",
    "    # Time indices\n",
    "    time = np.arange(n_samples)\n",
    "    date_id = time // 500  # ~500 observations per day\n",
    "    time_id = time % 500\n",
    "    \n",
    "    # Feature categories (mimicking actual data patterns)\n",
    "    features = {}\n",
    "    \n",
    "    # Group 1: Features with trend (40 features)\n",
    "    for i in range(40):\n",
    "        trend = 0.001 * time * np.random.randn()\n",
    "        seasonal = 2 * np.sin(2 * np.pi * time / (50 + i * 10))\n",
    "        noise = np.random.randn(n_samples) * 0.5\n",
    "        features[f'feature_{i}'] = trend + seasonal + noise\n",
    "    \n",
    "    # Group 2: Stationary features (40 features)\n",
    "    for i in range(40, 80):\n",
    "        features[f'feature_{i}'] = np.random.randn(n_samples)\n",
    "    \n",
    "    # Group 3: Features with heteroscedasticity (30 features)\n",
    "    for i in range(80, 110):\n",
    "        volatility = 0.5 + 0.5 * np.abs(np.sin(2 * np.pi * time / 1000))\n",
    "        features[f'feature_{i}'] = np.random.randn(n_samples) * volatility\n",
    "    \n",
    "    # Group 4: Correlated features (20 features)\n",
    "    base_feature = np.random.randn(n_samples)\n",
    "    for i in range(110, 130):\n",
    "        noise = np.random.randn(n_samples) * 0.3\n",
    "        features[f'feature_{i}'] = base_feature + noise\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(features)\n",
    "    df['date_id'] = date_id\n",
    "    df['time_id'] = time_id\n",
    "    \n",
    "    # Generate responders (different time horizons)\n",
    "    # Response depends on recent feature values with some noise\n",
    "    feature_matrix = df[[f'feature_{i}' for i in range(130)]].values\n",
    "    \n",
    "    # Weights for different features (some more important)\n",
    "    importance = np.random.exponential(0.1, 130)\n",
    "    importance = importance / importance.sum()\n",
    "    \n",
    "    # resp_1: short-term (next step)\n",
    "    df['resp_1'] = feature_matrix @ importance + np.random.randn(n_samples) * 0.5\n",
    "    \n",
    "    # resp_2: medium-term (rolling average)\n",
    "    df['resp_2'] = pd.Series(feature_matrix @ importance).rolling(5, min_periods=1).mean() + \\\n",
    "                   np.random.randn(n_samples) * 0.3\n",
    "    \n",
    "    # resp_3: longer-term\n",
    "    df['resp_3'] = pd.Series(feature_matrix @ importance).rolling(10, min_periods=1).mean() + \\\n",
    "                   np.random.randn(n_samples) * 0.2\n",
    "    \n",
    "    # resp_4: longest-term\n",
    "    df['resp_4'] = pd.Series(feature_matrix @ importance).rolling(20, min_periods=1).mean() + \\\n",
    "                   np.random.randn(n_samples) * 0.15\n",
    "    \n",
    "    # Main responder (combination)\n",
    "    df['resp'] = (df['resp_1'] + df['resp_2'] + df['resp_3'] + df['resp_4']) / 4\n",
    "    \n",
    "    # Weight (importance of prediction)\n",
    "    # Lower weight for more volatile predictions\n",
    "    volatility_proxy = df[['feature_0', 'feature_1', 'feature_2']].std(axis=1)\n",
    "    df['weight'] = 1.0 / (1.0 + volatility_proxy)\n",
    "    df['weight'] = df['weight'] / df['weight'].mean()  # Normalize\n",
    "    \n",
    "    # About 17% of samples have weight = 0 (as in actual competition)\n",
    "    zero_weight_mask = np.random.rand(n_samples) < 0.17\n",
    "    df.loc[zero_weight_mask, 'weight'] = 0\n",
    "    \n",
    "    print(f\"Generated data shape: {df.shape}\")\n",
    "    print(f\"Features: {n_features}\")\n",
    "    print(f\"Responders: 5 (resp_1, resp_2, resp_3, resp_4, resp)\")\n",
    "    print(f\"Zero weight samples: {(df['weight'] == 0).sum()} ({(df['weight'] == 0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Generate data\n",
    "n_samples = 100000\n",
    "n_features = 130\n",
    "df = generate_jane_street_like_data(n_samples, n_features)\n",
    "\n",
    "print(\"\\nData statistics:\")\n",
    "print(df[['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4', 'weight']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data characteristics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Sample features over time\n",
    "sample_features = ['feature_0', 'feature_50', 'feature_100']\n",
    "for feat in sample_features:\n",
    "    axes[0, 0].plot(df[feat][:2000], alpha=0.7, label=feat)\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].set_title('Sample Features Over Time', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Responders\n",
    "responders = ['resp_1', 'resp_2', 'resp_3', 'resp_4']\n",
    "for resp in responders:\n",
    "    axes[0, 1].plot(df[resp][:2000], alpha=0.7, label=resp)\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Response')\n",
    "axes[0, 1].set_title('Responders (Different Time Horizons)', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Weight distribution\n",
    "axes[0, 2].hist(df[df['weight'] > 0]['weight'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_xlabel('Weight')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('Weight Distribution (non-zero)', fontweight='bold')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature correlation heatmap (subset)\n",
    "feature_subset = [f'feature_{i}' for i in [0, 10, 20, 30, 40, 50]]\n",
    "corr = df[feature_subset].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 0], \n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "axes[1, 0].set_title('Feature Correlation (Subset)', fontweight='bold')\n",
    "\n",
    "# Plot 5: Resp vs weight scatter\n",
    "sample_idx = np.random.choice(len(df), 5000, replace=False)\n",
    "axes[1, 1].scatter(df.loc[sample_idx, 'resp'], \n",
    "                   df.loc[sample_idx, 'weight'], \n",
    "                   alpha=0.3, s=10)\n",
    "axes[1, 1].set_xlabel('Response')\n",
    "axes[1, 1].set_ylabel('Weight')\n",
    "axes[1, 1].set_title('Response vs Weight', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Daily response distribution\n",
    "daily_resp = df.groupby('date_id')['resp'].mean()\n",
    "axes[1, 2].plot(daily_resp.values)\n",
    "axes[1, 2].set_xlabel('Date ID')\n",
    "axes[1, 2].set_ylabel('Mean Response')\n",
    "axes[1, 2].set_title('Daily Mean Response (Trend)', fontweight='bold')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LSTM Cell from Scratch\n",
    "\n",
    "### LSTM Mathematics\n",
    "\n",
    "LSTM has **4 gates** that control information flow:\n",
    "\n",
    "**1. Forget Gate** (what to remove from cell state):\n",
    "```\n",
    "f_t = Ïƒ(W_f @ [h_{t-1}, x_t] + b_f)\n",
    "```\n",
    "\n",
    "**2. Input Gate** (what new information to add):\n",
    "```\n",
    "i_t = Ïƒ(W_i @ [h_{t-1}, x_t] + b_i)\n",
    "g_t = tanh(W_g @ [h_{t-1}, x_t] + b_g)  # Candidate values\n",
    "```\n",
    "\n",
    "**3. Cell State Update** (combine forget and input):\n",
    "```\n",
    "c_t = f_t âŠ™ c_{t-1} + i_t âŠ™ g_t\n",
    "```\n",
    "\n",
    "**4. Output Gate** (what to output):\n",
    "```\n",
    "o_t = Ïƒ(W_o @ [h_{t-1}, x_t] + b_o)\n",
    "h_t = o_t âŠ™ tanh(c_t)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Ïƒ = sigmoid (0 to 1, acts as gate)\n",
    "- tanh = hyperbolic tangent (-1 to 1)\n",
    "- âŠ™ = element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Cell implemented from scratch.\n",
    "    \n",
    "    Key insight: Gates (forget, input, output) control information flow,\n",
    "    preventing vanishing gradients and enabling long-term dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Dimension of input features\n",
    "            hidden_size: Dimension of hidden state and cell state\n",
    "        \"\"\"\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined weight matrix for all gates\n",
    "        # Concatenate [h_{t-1}, x_t], so input dimension is hidden_size + input_size\n",
    "        combined_size = hidden_size + input_size\n",
    "        \n",
    "        # Forget gate\n",
    "        self.W_f = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        \n",
    "        # Input gate\n",
    "        self.W_i = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        \n",
    "        # Candidate cell state (sometimes called g)\n",
    "        self.W_g = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        \n",
    "        # Output gate\n",
    "        self.W_o = nn.Linear(combined_size, hidden_size, bias=True)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier initialization for stable gradients\"\"\"\n",
    "        for weight in [self.W_f, self.W_i, self.W_g, self.W_o]:\n",
    "            nn.init.xavier_uniform_(weight.weight)\n",
    "            nn.init.zeros_(weight.bias)\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Forward pass for one time step.\n",
    "        \n",
    "        Args:\n",
    "            x_t: Input at time t [batch, input_size]\n",
    "            h_prev: Previous hidden state [batch, hidden_size]\n",
    "            c_prev: Previous cell state [batch, hidden_size]\n",
    "            \n",
    "        Returns:\n",
    "            h_t: New hidden state [batch, hidden_size]\n",
    "            c_t: New cell state [batch, hidden_size]\n",
    "        \"\"\"\n",
    "        # Concatenate previous hidden state and current input\n",
    "        combined = torch.cat([h_prev, x_t], dim=1)  # [batch, hidden_size + input_size]\n",
    "        \n",
    "        # === GATE COMPUTATIONS ===\n",
    "        \n",
    "        # Forget gate: what to forget from previous cell state\n",
    "        f_t = torch.sigmoid(self.W_f(combined))\n",
    "        \n",
    "        # Input gate: what new information to add\n",
    "        i_t = torch.sigmoid(self.W_i(combined))\n",
    "        \n",
    "        # Candidate cell state: new information to potentially add\n",
    "        g_t = torch.tanh(self.W_g(combined))\n",
    "        \n",
    "        # Output gate: what to output from cell state\n",
    "        o_t = torch.sigmoid(self.W_o(combined))\n",
    "        \n",
    "        # === STATE UPDATES ===\n",
    "        \n",
    "        # Update cell state: forget old + add new\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        \n",
    "        # Update hidden state: output gate applied to cell state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "print(\"âœ“ LSTM Cell implemented!\")\n",
    "print(\"\\nLSTM has 4 components:\")\n",
    "print(\"  1. Forget gate - what to remove from memory\")\n",
    "print(\"  2. Input gate - what new info to store\")\n",
    "print(\"  3. Cell state - long-term memory\")\n",
    "print(\"  4. Output gate - what to expose as hidden state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Complete LSTM Model for Jane Street\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input: [batch, seq_len, n_features]\n",
    "  â†“\n",
    "LSTM Layers (2 layers, 128 hidden units)\n",
    "  â†“\n",
    "Dropout (0.3)\n",
    "  â†“\n",
    "Fully Connected (128 â†’ 64)\n",
    "  â†“\n",
    "Dropout (0.2)\n",
    "  â†“\n",
    "Output (64 â†’ 5)  [resp_1, resp_2, resp_3, resp_4, resp]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_JaneStreet(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete LSTM model for Jane Street competition.\n",
    "    Built from scratch using our custom LSTM cells.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, \n",
    "                 output_size=5, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of features (130 for Jane Street)\n",
    "            hidden_size: Hidden state dimension\n",
    "            num_layers: Number of stacked LSTM layers\n",
    "            output_size: Number of responders to predict (5)\n",
    "            dropout: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(LSTM_JaneStreet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Stack of LSTM cells\n",
    "        self.lstm_cells = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            cell_input_size = input_size if i == 0 else hidden_size\n",
    "            self.lstm_cells.append(LSTMCell(cell_input_size, hidden_size))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        \n",
    "        # Batch normalization (helps with training stability)\n",
    "        self.bn = nn.BatchNorm1d(64)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM.\n",
    "        \n",
    "        Args:\n",
    "            x: Input sequences [batch, seq_len, input_size]\n",
    "            hidden: Initial (h, c) states (optional)\n",
    "            \n",
    "        Returns:\n",
    "            output: Predictions [batch, output_size]\n",
    "            (h_n, c_n): Final hidden and cell states\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Initialize hidden and cell states if not provided\n",
    "        if hidden is None:\n",
    "            h, c = self._init_hidden(batch_size, x.device)\n",
    "        else:\n",
    "            h, c = hidden\n",
    "        \n",
    "        # Process sequence one time step at a time\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]  # [batch, input_size]\n",
    "            \n",
    "            # Pass through each LSTM layer\n",
    "            for layer in range(self.num_layers):\n",
    "                h[layer], c[layer] = self.lstm_cells[layer](x_t, h[layer], c[layer])\n",
    "                x_t = h[layer]  # Output of this layer â†’ input to next\n",
    "                \n",
    "                # Apply dropout between layers (not on last layer)\n",
    "                if layer < self.num_layers - 1:\n",
    "                    x_t = self.dropout(x_t)\n",
    "        \n",
    "        # Use final hidden state for prediction\n",
    "        final_hidden = h[-1]  # [batch, hidden_size]\n",
    "        \n",
    "        # Output layers\n",
    "        out = torch.relu(self.fc1(final_hidden))\n",
    "        out = self.bn(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)  # [batch, output_size]\n",
    "        \n",
    "        return out, (h, c)\n",
    "    \n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize hidden and cell states to zeros\"\"\"\n",
    "        h = [torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "             for _ in range(self.num_layers)]\n",
    "        c = [torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "             for _ in range(self.num_layers)]\n",
    "        return h, c\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = LSTM_JaneStreet(\n",
    "    input_size=n_features,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    output_size=5,  # Predict all 5 responders\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 70)\n",
    "print(model)\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaneStreetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Jane Street time series.\n",
    "    Creates sequences of features â†’ multiple responders.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, seq_length=10, target_cols=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with features and responders\n",
    "            seq_length: Length of input sequences\n",
    "            target_cols: List of responder column names\n",
    "        \"\"\"\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        if target_cols is None:\n",
    "            target_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']\n",
    "        \n",
    "        # Extract features\n",
    "        feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "        self.features = df[feature_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Extract targets\n",
    "        self.targets = df[target_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Extract weights\n",
    "        self.weights = df['weight'].values.astype(np.float32)\n",
    "        \n",
    "        # Normalize features (important!)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "        \n",
    "        # Normalize targets\n",
    "        self.target_mean = self.targets.mean(axis=0)\n",
    "        self.target_std = self.targets.std(axis=0)\n",
    "        self.targets = (self.targets - self.target_mean) / (self.target_std + 1e-8)\n",
    "        \n",
    "        print(f\"Dataset created: {len(self)} samples\")\n",
    "        print(f\"Feature shape: {self.features.shape}\")\n",
    "        print(f\"Target shape: {self.targets.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features\n",
    "        x = self.features[idx : idx + self.seq_length]  # [seq_length, n_features]\n",
    "        \n",
    "        # Get target (next time step after sequence)\n",
    "        y = self.targets[idx + self.seq_length]  # [n_responders]\n",
    "        \n",
    "        # Get weight\n",
    "        w = self.weights[idx + self.seq_length]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(x),\n",
    "            torch.FloatTensor(y),\n",
    "            torch.FloatTensor([w])\n",
    "        )\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "seq_length = 10\n",
    "\n",
    "# Split data: 70% train, 15% val, 15% test (time-ordered)\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.15 * len(df))\n",
    "\n",
    "train_df = df[:train_size].reset_index(drop=True)\n",
    "val_df = df[train_size:train_size + val_size].reset_index(drop=True)\n",
    "test_df = df[train_size + val_size:].reset_index(drop=True)\n",
    "\n",
    "train_dataset = JaneStreetDataset(train_df, seq_length)\n",
    "val_dataset = JaneStreetDataset(val_df, seq_length)\n",
    "test_dataset = JaneStreetDataset(test_df, seq_length)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Setup\n",
    "\n",
    "**Loss Function:** Weighted MSE\n",
    "- Different weights for different predictions\n",
    "- Zero-weight samples don't contribute to loss\n",
    "\n",
    "**Metric:** Weighted RÂ² Score (competition metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(predictions, targets, weights):\n",
    "    \"\"\"\n",
    "    Weighted Mean Squared Error.\n",
    "    Samples with higher weight contribute more to loss.\n",
    "    \"\"\"\n",
    "    squared_errors = (predictions - targets) ** 2\n",
    "    weighted_errors = squared_errors * weights.unsqueeze(1)\n",
    "    return weighted_errors.sum() / (weights.sum() * targets.size(1) + 1e-8)\n",
    "\n",
    "\n",
    "def weighted_r2_score(predictions, targets, weights):\n",
    "    \"\"\"\n",
    "    Weighted RÂ² score (competition metric).\n",
    "    RÂ² = 1 - (SS_res / SS_tot)\n",
    "    \"\"\"\n",
    "    # Remove zero-weight samples\n",
    "    mask = weights.squeeze() > 0\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    pred = predictions[mask]\n",
    "    true = targets[mask]\n",
    "    w = weights[mask]\n",
    "    \n",
    "    # Weighted mean\n",
    "    weighted_mean = (true * w).sum(dim=0) / w.sum()\n",
    "    \n",
    "    # Sum of squared residuals (weighted)\n",
    "    ss_res = ((true - pred) ** 2 * w).sum()\n",
    "    \n",
    "    # Total sum of squares (weighted)\n",
    "    ss_tot = ((true - weighted_mean) ** 2 * w).sum()\n",
    "    \n",
    "    r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "    return r2.item()\n",
    "\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Loss: Weighted MSE\")\n",
    "print(f\"  Metric: Weighted RÂ² Score\")\n",
    "print(f\"  Optimizer: AdamW (lr=0.001, weight_decay=1e-5)\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for x, y, w in tqdm(dataloader, desc='Training', leave=False):\n",
    "        x, y, w = x.to(device), y.to(device), w.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions, _ = model(x)\n",
    "        \n",
    "        # Compute weighted loss\n",
    "        loss = weighted_mse_loss(predictions, y, w)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(predictions.detach())\n",
    "        all_targets.append(y)\n",
    "        all_weights.append(w)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_weights = torch.cat(all_weights)\n",
    "    r2 = weighted_r2_score(all_preds, all_targets, all_weights)\n",
    "    \n",
    "    return avg_loss, r2\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, w in dataloader:\n",
    "            x, y, w = x.to(device), y.to(device), w.to(device)\n",
    "            \n",
    "            predictions, _ = model(x)\n",
    "            loss = weighted_mse_loss(predictions, y, w)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.append(predictions)\n",
    "            all_targets.append(y)\n",
    "            all_weights.append(w)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_weights = torch.cat(all_weights)\n",
    "    r2 = weighted_r2_score(all_preds, all_targets, all_weights)\n",
    "    \n",
    "    return avg_loss, r2\n",
    "\n",
    "\n",
    "# Training\n",
    "n_epochs = 30\n",
    "train_losses, train_r2s = [], []\n",
    "val_losses, val_r2s = [], []\n",
    "best_val_r2 = -float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Train\n",
    "    train_loss, train_r2 = train_epoch(model, train_loader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2s.append(train_r2)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_r2 = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2s.append(val_r2)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_r2)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_r2 > best_val_r2:\n",
    "        best_val_r2 = val_r2\n",
    "        torch.save(model.state_dict(), 'best_jane_street_lstm.pth')\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    # Print progress\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch [{epoch+1:2d}/{n_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.6f} RÂ²: {train_r2:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.6f} RÂ²: {val_r2:.4f} | \"\n",
    "          f\"LR: {current_lr:.6f}\"\n",
    "          f\"{' â† BEST' if val_r2 == best_val_r2 else ''}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation RÂ²: {best_val_r2:.4f} (epoch {best_epoch + 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Weighted MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Training Progress - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot 2: RÂ² scores\n",
    "axes[1].plot(train_r2s, label='Train RÂ²', linewidth=2)\n",
    "axes[1].plot(val_r2s, label='Val RÂ²', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Weighted RÂ² Score', fontsize=12)\n",
    "axes[1].set_title('Training Progress - RÂ² Score', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_jane_street_lstm.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_r2 = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Test Loss (Weighted MSE): {test_loss:.6f}\")\n",
    "print(f\"Test RÂ² Score: {test_r2:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get predictions for visualization\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_weights = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, w in test_loader:\n",
    "        x = x.to(device)\n",
    "        pred, _ = model(x)\n",
    "        all_predictions.append(pred.cpu().numpy())\n",
    "        all_targets.append(y.numpy())\n",
    "        all_weights.append(w.numpy())\n",
    "\n",
    "predictions = np.concatenate(all_predictions)\n",
    "targets = np.concatenate(all_targets)\n",
    "weights = np.concatenate(all_weights)\n",
    "\n",
    "# Denormalize predictions\n",
    "predictions_denorm = predictions * test_dataset.target_std + test_dataset.target_mean\n",
    "targets_denorm = targets * test_dataset.target_std + test_dataset.target_mean\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions_denorm.shape}\")\n",
    "print(f\"Targets shape: {targets_denorm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "responder_names = ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, resp_name in enumerate(responder_names):\n",
    "    # Scatter plot: predicted vs actual\n",
    "    mask = weights.squeeze() > 0  # Only non-zero weights\n",
    "    sample_size = min(2000, mask.sum())\n",
    "    sample_idx = np.random.choice(np.where(mask)[0], sample_size, replace=False)\n",
    "    \n",
    "    axes[i].scatter(targets_denorm[sample_idx, i], \n",
    "                   predictions_denorm[sample_idx, i],\n",
    "                   alpha=0.3, s=10)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(targets_denorm[sample_idx, i].min(), \n",
    "                  predictions_denorm[sample_idx, i].min())\n",
    "    max_val = max(targets_denorm[sample_idx, i].max(), \n",
    "                  predictions_denorm[sample_idx, i].max())\n",
    "    axes[i].plot([min_val, max_val], [min_val, max_val], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    axes[i].set_xlabel('Actual', fontsize=11)\n",
    "    axes[i].set_ylabel('Predicted', fontsize=11)\n",
    "    axes[i].set_title(f'{resp_name}', fontsize=12, fontweight='bold')\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Predictions vs Actual (Test Set)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Importance Analysis\n",
    "\n",
    "Analyze which features the LSTM is focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Compute feature importance using gradient-based method.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    feature_grads = []\n",
    "    \n",
    "    for x, y, w in dataloader:\n",
    "        x = x.to(device)\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        pred, _ = model(x)\n",
    "        \n",
    "        # Compute gradient of output with respect to input\n",
    "        pred.sum().backward()\n",
    "        \n",
    "        # Store gradients\n",
    "        feature_grads.append(x.grad.abs().mean(dim=(0, 1)).cpu().numpy())\n",
    "        \n",
    "        if len(feature_grads) >= 10:  # Sample 10 batches\n",
    "            break\n",
    "    \n",
    "    # Average across batches\n",
    "    importance = np.mean(feature_grads, axis=0)\n",
    "    return importance\n",
    "\n",
    "\n",
    "# Compute importance\n",
    "print(\"Computing feature importance...\")\n",
    "importance = compute_feature_importance(model, test_loader, device)\n",
    "\n",
    "# Plot top 20 important features\n",
    "top_k = 20\n",
    "top_indices = np.argsort(importance)[-top_k:][::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(top_k), importance[top_indices])\n",
    "plt.xlabel('Feature Index', fontsize=12)\n",
    "plt.ylabel('Importance (Gradient Magnitude)', fontsize=12)\n",
    "plt.title(f'Top {top_k} Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(top_k), [f'F{i}' for i in top_indices], rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 most important features:\")\n",
    "for i, idx in enumerate(top_indices[:10]):\n",
    "    print(f\"  {i+1}. feature_{idx}: {importance[idx]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Time Series Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions over time\n",
    "n_plot = 500\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, resp_name in enumerate(responder_names):\n",
    "    axes[i].plot(targets_denorm[:n_plot, i], label='Actual', linewidth=2, alpha=0.7)\n",
    "    axes[i].plot(predictions_denorm[:n_plot, i], label='Predicted', \n",
    "                linewidth=2, alpha=0.7, linestyle='--')\n",
    "    axes[i].set_xlabel('Time Step', fontsize=11)\n",
    "    axes[i].set_ylabel('Value', fontsize=11)\n",
    "    axes[i].set_title(f'{resp_name} Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[i].legend(fontsize=10)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Insights\n",
    "\n",
    "### 1. LSTM Architecture for Finance\n",
    "\n",
    "**Why LSTM works for this problem:**\n",
    "- **Gating mechanism** handles concept drift (changing market regimes)\n",
    "- **Cell state** maintains long-term dependencies across time\n",
    "- **Input/forget gates** learn which features matter when\n",
    "- **Output gate** controls what information to expose\n",
    "\n",
    "### 2. Implementation Details\n",
    "\n",
    "**From-scratch components:**\n",
    "- Custom LSTM cell with all 4 gates\n",
    "- Manual time step iteration\n",
    "- Explicit state management (h_t, c_t)\n",
    "\n",
    "**Key techniques:**\n",
    "- Batch normalization (training stability)\n",
    "- Dropout (prevent overfitting)\n",
    "- Gradient clipping (handle exploding gradients)\n",
    "- Feature normalization (StandardScaler)\n",
    "\n",
    "### 3. Jane Street Specific Challenges\n",
    "\n",
    "**Data characteristics:**\n",
    "- 130 anonymized features with different patterns\n",
    "- Multiple time horizons (resp_1 to resp_4)\n",
    "- Weighted samples (importance varies)\n",
    "- High noise-to-signal ratio\n",
    "\n",
    "**Solutions implemented:**\n",
    "- Weighted loss function\n",
    "- Sequence modeling (capture temporal patterns)\n",
    "- Multi-output prediction (all responders)\n",
    "- Proper time-series split (no data leakage)\n",
    "\n",
    "### 4. Performance Metrics\n",
    "\n",
    "**Weighted RÂ² Score:**\n",
    "- Competition metric\n",
    "- Accounts for sample importance\n",
    "- RÂ² > 0: Better than mean prediction\n",
    "- RÂ² close to 1: Perfect prediction\n",
    "\n",
    "**Expected results on real data:**\n",
    "- RÂ² ~ 0.001 - 0.005 is competitive\n",
    "- Finance data is extremely noisy\n",
    "- Small improvements = big money\n",
    "\n",
    "### 5. LSTM vs Other Approaches\n",
    "\n",
    "**LSTM advantages:**\n",
    "- Captures temporal dependencies\n",
    "- Handles variable-length sequences\n",
    "- Robust to different market regimes\n",
    "\n",
    "**Alternatives:**\n",
    "- GRU: Simpler, faster, similar performance\n",
    "- Transformer: Better for very long sequences\n",
    "- Tree models (XGBoost): Often strong baseline\n",
    "- Ensemble: Combine multiple approaches\n",
    "\n",
    "### 6. Real Competition Tips\n",
    "\n",
    "**Feature engineering:**\n",
    "- Rolling statistics (mean, std, min, max)\n",
    "- Lag features (previous values)\n",
    "- Technical indicators (momentum, volatility)\n",
    "- Cross-feature interactions\n",
    "\n",
    "**Model improvements:**\n",
    "- Attention mechanism (focus on important time steps)\n",
    "- Multi-task learning (auxiliary tasks)\n",
    "- Adversarial validation (detect distribution shift)\n",
    "- Pseudo-labeling (use test set)\n",
    "\n",
    "**Validation strategy:**\n",
    "- Time-series cross-validation\n",
    "- Out-of-time validation\n",
    "- Walk-forward analysis\n",
    "- Monitor stability across folds\n",
    "\n",
    "### 7. Production Considerations\n",
    "\n",
    "**Real-time inference:**\n",
    "- Fast forward pass (<10ms)\n",
    "- Batch predictions when possible\n",
    "- Cache hidden states\n",
    "- Model quantization (FP16)\n",
    "\n",
    "**Monitoring:**\n",
    "- Track prediction distribution\n",
    "- Detect concept drift\n",
    "- Retrain regularly\n",
    "- A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've built an LSTM from scratch for financial forecasting! ðŸŽ‰**\n",
    "\n",
    "This implementation demonstrates:\n",
    "- Complete LSTM cell mathematics\n",
    "- Proper time-series modeling\n",
    "- Financial data handling\n",
    "- Competition-grade evaluation\n",
    "- Production-ready considerations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
