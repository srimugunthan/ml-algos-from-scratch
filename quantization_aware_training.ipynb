{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization-Aware Training (QAT) on a 3-Layer Neural Network\n",
    "\n",
    "**Author**: Srimugunthan  \n",
    "**Date**: February 2026\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Quantization-Aware Training (QAT)**, a technique where quantization effects are simulated during training to help the model learn to be robust to reduced precision.\n",
    "\n",
    "### What is Quantization?\n",
    "\n",
    "Quantization converts high-precision floating-point weights (32-bit) to lower-precision integers (8-bit, 4-bit, etc.) to:\n",
    "- Reduce model size (4x smaller for 8-bit)\n",
    "- Speed up inference\n",
    "- Enable deployment on edge devices\n",
    "\n",
    "### Why QAT vs Post-Training Quantization (PTQ)?\n",
    "\n",
    "- **PTQ**: Quantize after training ‚Üí Can cause significant accuracy drop\n",
    "- **QAT**: Simulate quantization during training ‚Üí Model learns to be robust to quantization noise\n",
    "\n",
    "### Key Technique: Fake Quantization\n",
    "\n",
    "- **Forward pass**: Quantize weights/activations to simulate low precision\n",
    "- **Backward pass**: Use full precision gradients (straight-through estimator)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Quantization\n",
    "\n",
    "### 2.1 Quantization Formula\n",
    "\n",
    "For a floating-point value $x$, quantization to $n$ bits:\n",
    "\n",
    "$$x_{\\text{quantized}} = \\text{clamp}\\left(\\left\\lfloor \\frac{x}{s} \\right\\rfloor + z, 0, 2^n - 1\\right)$$\n",
    "\n",
    "Where:\n",
    "- $s$ = scale = $\\frac{x_{\\max} - x_{\\min}}{2^n - 1}$\n",
    "- $z$ = zero point = $-\\frac{x_{\\min}}{s}$\n",
    "\n",
    "### 2.2 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quantization(num_bits=8):\n",
    "    \"\"\"\n",
    "    Visualize how quantization affects a continuous signal\n",
    "    \"\"\"\n",
    "    # Generate continuous signal\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    y = np.sin(x) * 2 + np.random.randn(1000) * 0.1\n",
    "    \n",
    "    # Quantize\n",
    "    qmin, qmax = 0, 2**num_bits - 1\n",
    "    scale = (y.max() - y.min()) / (qmax - qmin)\n",
    "    zero_point = qmin - y.min() / scale\n",
    "    \n",
    "    y_int = np.clip(np.round(y / scale + zero_point), qmin, qmax)\n",
    "    y_quant = (y_int - zero_point) * scale\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Original vs Quantized\n",
    "    axes[0].plot(x, y, label='Original (FP32)', alpha=0.7, linewidth=2)\n",
    "    axes[0].plot(x, y_quant, label=f'Quantized ({num_bits}-bit)', alpha=0.7, linewidth=2)\n",
    "    axes[0].set_xlabel('X')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].set_title('Original vs Quantized Signal')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Quantization error\n",
    "    error = np.abs(y - y_quant)\n",
    "    axes[1].hist(error, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_xlabel('Absolute Error')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'Quantization Error Distribution\\nMean Error: {error.mean():.6f}')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nQuantization Statistics ({num_bits}-bit):\")\n",
    "    print(f\"  Scale: {scale:.6f}\")\n",
    "    print(f\"  Zero point: {zero_point:.2f}\")\n",
    "    print(f\"  Quantization levels: {2**num_bits}\")\n",
    "    print(f\"  Mean absolute error: {error.mean():.6f}\")\n",
    "    print(f\"  Max absolute error: {error.max():.6f}\")\n",
    "\n",
    "visualize_quantization(num_bits=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Fake Quantization\n",
    "\n",
    "The core of QAT is **fake quantization**: quantize in the forward pass, but pass gradients unchanged in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeQuantize(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Fake Quantization: Simulates quantization in forward pass,\n",
    "    but passes gradients unchanged in backward pass (Straight-Through Estimator).\n",
    "    \n",
    "    This allows the network to learn to be robust to quantization noise.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, num_bits=8):\n",
    "        \"\"\"\n",
    "        Forward pass: Quantize the input\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (float32)\n",
    "            num_bits: Number of bits for quantization (default: 8)\n",
    "            \n",
    "        Returns:\n",
    "            Quantized tensor (still in float32, but with reduced precision)\n",
    "        \"\"\"\n",
    "        # Define quantization range\n",
    "        qmin = 0\n",
    "        qmax = 2**num_bits - 1\n",
    "        \n",
    "        # Calculate scale and zero point\n",
    "        min_val = x.min()\n",
    "        max_val = x.max()\n",
    "        \n",
    "        scale = (max_val - min_val) / (qmax - qmin)\n",
    "        scale = scale if scale > 1e-8 else 1e-8  # Avoid division by zero\n",
    "        \n",
    "        zero_point = qmin - min_val / scale\n",
    "        zero_point = torch.clamp(torch.round(zero_point), qmin, qmax)\n",
    "        \n",
    "        # Quantize: float -> int\n",
    "        x_int = torch.clamp(torch.round(x / scale + zero_point), qmin, qmax)\n",
    "        \n",
    "        # Dequantize: int -> float (simulates low precision inference)\n",
    "        x_quant = (x_int - zero_point) * scale\n",
    "        \n",
    "        return x_quant\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: Straight-Through Estimator (STE)\n",
    "        Pass gradients unchanged, as if quantization didn't happen.\n",
    "        \n",
    "        This is key: we need gradients to flow for training,\n",
    "        but quantization is not differentiable.\n",
    "        \"\"\"\n",
    "        return grad_output, None  # None for num_bits (not trainable)\n",
    "\n",
    "\n",
    "# Test fake quantization\n",
    "print(\"Testing FakeQuantize:\")\n",
    "x = torch.randn(5, 5) * 10\n",
    "print(f\"\\nOriginal tensor:\\n{x}\")\n",
    "\n",
    "x_quant_8bit = FakeQuantize.apply(x, 8)\n",
    "print(f\"\\n8-bit quantized:\\n{x_quant_8bit}\")\n",
    "\n",
    "x_quant_4bit = FakeQuantize.apply(x, 4)\n",
    "print(f\"\\n4-bit quantized:\\n{x_quant_4bit}\")\n",
    "\n",
    "print(f\"\\n8-bit error: {(x - x_quant_8bit).abs().mean().item():.6f}\")\n",
    "print(f\"4-bit error: {(x - x_quant_4bit).abs().mean().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Quantization-Aware Layers\n",
    "\n",
    "Now we create a custom Linear layer that applies fake quantization to both weights and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantization-Aware Linear Layer\n",
    "    \n",
    "    Applies fake quantization to:\n",
    "    1. Weights\n",
    "    2. Biases\n",
    "    3. Output activations\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, num_bits=8):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.num_bits = num_bits\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Quantize weights\n",
    "        weight_quant = FakeQuantize.apply(self.linear.weight, self.num_bits)\n",
    "        \n",
    "        # 2. Quantize bias (if exists)\n",
    "        if self.linear.bias is not None:\n",
    "            bias_quant = FakeQuantize.apply(self.linear.bias, self.num_bits)\n",
    "        else:\n",
    "            bias_quant = None\n",
    "        \n",
    "        # 3. Forward pass with quantized weights\n",
    "        output = nn.functional.linear(x, weight_quant, bias_quant)\n",
    "        \n",
    "        # 4. Quantize output activations\n",
    "        output_quant = FakeQuantize.apply(output, self.num_bits)\n",
    "        \n",
    "        return output_quant\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.linear.in_features}, out_features={self.linear.out_features}, num_bits={self.num_bits}'\n",
    "\n",
    "\n",
    "# Test QATLinear\n",
    "print(\"Testing QATLinear:\")\n",
    "layer = QATLinear(10, 5, num_bits=8)\n",
    "x = torch.randn(3, 10)\n",
    "output = layer(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nLayer: {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Neural Networks\n",
    "\n",
    "We'll create two versions of a 3-layer fully connected network:\n",
    "1. **Normal**: Standard network (for Post-Training Quantization)\n",
    "2. **QAT**: Network with fake quantization layers (for Quantization-Aware Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard 3-layer fully connected network\n",
    "    Architecture: input -> FC -> ReLU -> FC -> ReLU -> FC -> output\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=10, hidden_dim=64, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ThreeLayerNetQAT(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer fully connected network with Quantization-Aware Training\n",
    "    Same architecture, but uses QATLinear layers with fake quantization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=10, hidden_dim=64, output_dim=2, num_bits=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = QATLinear(input_dim, hidden_dim, num_bits)\n",
    "        self.fc2 = QATLinear(hidden_dim, hidden_dim, num_bits)\n",
    "        self.fc3 = QATLinear(hidden_dim, output_dim, num_bits)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.num_bits = num_bits\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize input activations\n",
    "        x = FakeQuantize.apply(x, self.num_bits)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Display architectures\n",
    "print(\"=\"*60)\n",
    "print(\"NORMAL NETWORK ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "model_normal = ThreeLayerNet(input_dim=10, hidden_dim=64, output_dim=2)\n",
    "print(model_normal)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_normal.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QAT NETWORK ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "model_qat = ThreeLayerNetQAT(input_dim=10, hidden_dim=64, output_dim=2, num_bits=8)\n",
    "print(model_qat)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_qat.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Synthetic Dataset\n",
    "\n",
    "We'll create a simple binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=2000, n_features=10, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic binary classification dataset\n",
    "    Label = 1 if sum of features > 0, else 0\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    # Generate features\n",
    "    X = torch.randn(n_samples, n_features)\n",
    "    \n",
    "    # Generate labels (binary classification)\n",
    "    y = (X.sum(dim=1) > 0).long()\n",
    "    \n",
    "    # Split into train and test\n",
    "    n_train = int(n_samples * (1 - test_size))\n",
    "    \n",
    "    X_train, X_test = X[:n_train], X[n_train:]\n",
    "    y_train, y_test = y[:n_train], y[n_train:]\n",
    "    \n",
    "    print(f\"Dataset Generated:\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Test samples: {len(X_test)}\")\n",
    "    print(f\"  Features: {n_features}\")\n",
    "    print(f\"  Classes: {len(torch.unique(y))}\")\n",
    "    print(f\"  Class distribution (train): {torch.bincount(y_train).tolist()}\")\n",
    "    print(f\"  Class distribution (test): {torch.bincount(y_test).tolist()}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Generate data\n",
    "X_train, X_test, y_train, y_test = generate_dataset(\n",
    "    n_samples=2000, \n",
    "    n_features=10, \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoader created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=20, lr=0.001, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train a neural network and track metrics\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        history: Dictionary with training history\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += y_batch.size(0)\n",
    "            train_correct += predicted.eq(y_batch).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                test_total += y_batch.size(0)\n",
    "                test_correct += predicted.eq(y_batch).sum().item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_acc = 100. * test_correct / test_total\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1:2d}/{epochs}] | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def plot_training_history(histories, labels):\n",
    "    \"\"\"\n",
    "    Plot training curves for multiple models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for history, label in zip(histories, labels):\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(epochs, history['train_loss'], label=f'{label} (Train)', linewidth=2)\n",
    "        axes[0].plot(epochs, history['test_loss'], '--', label=f'{label} (Test)', linewidth=2)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[1].plot(epochs, history['train_acc'], label=f'{label} (Train)', linewidth=2)\n",
    "        axes[1].plot(epochs, history['test_acc'], '--', label=f'{label} (Test)', linewidth=2)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Test Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Test Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Post-Training Quantization (PTQ)\n",
    "\n",
    "This function quantizes an already-trained model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model_weights(model, num_bits=8):\n",
    "    \"\"\"\n",
    "    Apply Post-Training Quantization to a trained model\n",
    "    Quantizes all weights and biases in-place\n",
    "    \"\"\"\n",
    "    qmin = 0\n",
    "    qmax = 2**num_bits - 1\n",
    "    \n",
    "    print(f\"\\nApplying {num_bits}-bit Post-Training Quantization...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name or 'bias' in name:\n",
    "                # Calculate scale and zero point\n",
    "                min_val = param.min()\n",
    "                max_val = param.max()\n",
    "                \n",
    "                scale = (max_val - min_val) / (qmax - qmin)\n",
    "                scale = scale if scale > 1e-8 else 1e-8\n",
    "                \n",
    "                zero_point = qmin - min_val / scale\n",
    "                zero_point = torch.clamp(torch.round(zero_point), qmin, qmax)\n",
    "                \n",
    "                # Quantize\n",
    "                param_int = torch.clamp(torch.round(param / scale + zero_point), qmin, qmax)\n",
    "                \n",
    "                # Dequantize and update parameter\n",
    "                param_quant = (param_int - zero_point) * scale\n",
    "                param.copy_(param_quant)\n",
    "                \n",
    "                print(f\"  Quantized: {name} (shape: {param.shape})\")\n",
    "    \n",
    "    print(\"Quantization complete!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment 1: Train Normal Model + Apply PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# EXPERIMENT 1: Normal Model with Post-Training Quantization (PTQ)\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "# Train normal model\n",
    "model_normal = ThreeLayerNet(input_dim=10, hidden_dim=64, output_dim=2)\n",
    "model_normal, history_normal = train_model(\n",
    "    model_normal, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=20, \n",
    "    lr=0.001,\n",
    "    model_name=\"Normal Model\"\n",
    ")\n",
    "\n",
    "# Evaluate before quantization\n",
    "acc_before_ptq = evaluate_model(model_normal, test_loader)\n",
    "print(f\"\\n‚úì Accuracy BEFORE quantization: {acc_before_ptq:.2f}%\")\n",
    "\n",
    "# Apply PTQ\n",
    "model_normal = quantize_model_weights(model_normal, num_bits=8)\n",
    "\n",
    "# Evaluate after quantization\n",
    "acc_after_ptq = evaluate_model(model_normal, test_loader)\n",
    "print(f\"\\n‚úì Accuracy AFTER 8-bit quantization: {acc_after_ptq:.2f}%\")\n",
    "print(f\"‚úó Accuracy drop: {acc_before_ptq - acc_after_ptq:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment 2: Train QAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"# EXPERIMENT 2: Quantization-Aware Training (QAT)\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "# Train QAT model\n",
    "model_qat = ThreeLayerNetQAT(input_dim=10, hidden_dim=64, output_dim=2, num_bits=8)\n",
    "model_qat, history_qat = train_model(\n",
    "    model_qat, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=20, \n",
    "    lr=0.001,\n",
    "    model_name=\"QAT Model\"\n",
    ")\n",
    "\n",
    "# Evaluate QAT model (already trained with quantization)\n",
    "acc_qat = evaluate_model(model_qat, test_loader)\n",
    "print(f\"\\n‚úì QAT Model accuracy: {acc_qat:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON: Post-Training Quantization vs QAT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Method':<30} {'Accuracy':<15} {'Notes'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Normal (FP32 - Before PTQ)':<30} {acc_before_ptq:>6.2f}%        {'Baseline (full precision)'}\")\n",
    "print(f\"{'Normal (8-bit PTQ)':<30} {acc_after_ptq:>6.2f}%        {'After post-training quantization'}\")\n",
    "print(f\"{'QAT (8-bit)':<30} {acc_qat:>6.2f}%        {'Trained with quantization'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "ptq_drop = acc_before_ptq - acc_after_ptq\n",
    "qat_advantage = acc_qat - acc_after_ptq\n",
    "\n",
    "print(f\"\\nüìä Key Insights:\")\n",
    "print(f\"   ‚Ä¢ PTQ accuracy drop: {ptq_drop:.2f}%\")\n",
    "print(f\"   ‚Ä¢ QAT advantage over PTQ: {qat_advantage:.2f}%\")\n",
    "print(f\"   ‚Ä¢ QAT recovery: {(acc_qat/acc_before_ptq)*100:.1f}% of original accuracy\")\n",
    "\n",
    "if qat_advantage > 0:\n",
    "    print(f\"\\n‚úÖ QAT successfully maintains higher accuracy than PTQ!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  In this case, PTQ performed similarly to QAT (dataset may be too simple)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(\n",
    "    [history_normal, history_qat],\n",
    "    ['Normal Model', 'QAT Model']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Analyze Weight Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weight_distributions(model_normal, model_qat):\n",
    "    \"\"\"\n",
    "    Compare weight distributions between normal and QAT models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.suptitle('Weight Distributions: Normal vs QAT Models', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    layers = ['fc1', 'fc2', 'fc3']\n",
    "    \n",
    "    for idx, layer_name in enumerate(layers):\n",
    "        # Get weights\n",
    "        normal_weights = getattr(model_normal, layer_name).weight.detach().cpu().numpy().flatten()\n",
    "        qat_weights = getattr(model_qat, layer_name).linear.weight.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # Normal model\n",
    "        axes[0, idx].hist(normal_weights, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[0, idx].set_title(f'Normal Model - {layer_name.upper()}')\n",
    "        axes[0, idx].set_xlabel('Weight Value')\n",
    "        axes[0, idx].set_ylabel('Frequency')\n",
    "        axes[0, idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # QAT model\n",
    "        axes[1, idx].hist(qat_weights, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "        axes[1, idx].set_title(f'QAT Model - {layer_name.upper()}')\n",
    "        axes[1, idx].set_xlabel('Weight Value')\n",
    "        axes[1, idx].set_ylabel('Frequency')\n",
    "        axes[1, idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_weight_distributions(model_normal, model_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculate model size in MB (assuming float32)\n",
    "    \"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.numel() * 4  # 4 bytes per float32\n",
    "    \n",
    "    size_mb = param_size / (1024 * 1024)\n",
    "    return size_mb\n",
    "\n",
    "# Calculate sizes\n",
    "size_fp32 = calculate_model_size(model_normal)\n",
    "size_int8 = size_fp32 / 4  # 8-bit is 4x smaller\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL SIZE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Precision':<20} {'Size (MB)':<15} {'Compression Ratio'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'FP32 (Original)':<20} {size_fp32:>8.4f}        {'1.0x (baseline)'}\")\n",
    "print(f\"{'INT8 (Quantized)':<20} {size_int8:>8.4f}        {f'{size_fp32/size_int8:.1f}x smaller'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nüíæ Storage savings: {size_fp32 - size_int8:.4f} MB ({((size_fp32-size_int8)/size_fp32)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Key Takeaways\n",
    "\n",
    "### What is QAT?\n",
    "- **Quantization-Aware Training** simulates low-precision inference during training\n",
    "- Uses **fake quantization**: quantize forward pass, full precision backward pass\n",
    "- Allows model to learn to be robust to quantization noise\n",
    "\n",
    "### QAT vs Post-Training Quantization (PTQ)\n",
    "1. **PTQ**: Train in FP32 ‚Üí Quantize afterwards ‚Üí Can lose significant accuracy\n",
    "2. **QAT**: Train with quantization simulation ‚Üí Better accuracy retention\n",
    "\n",
    "### When to Use QAT?\n",
    "‚úÖ **Use QAT when:**\n",
    "- Deploying to edge devices (mobile, IoT)\n",
    "- Need aggressive quantization (4-bit, lower)\n",
    "- Accuracy is critical\n",
    "- Have time/resources for retraining\n",
    "\n",
    "‚úÖ **Use PTQ when:**\n",
    "- Quick quantization needed\n",
    "- Model already robust to noise\n",
    "- Limited training resources\n",
    "- 8-bit quantization sufficient\n",
    "\n",
    "### Implementation Details\n",
    "1. **Straight-Through Estimator (STE)**: Pass gradients through non-differentiable quantization\n",
    "2. **Quantize everything**: Weights, biases, and activations\n",
    "3. **Scale and zero-point**: Map float range to integer range\n",
    "\n",
    "### Benefits\n",
    "- **4x smaller models** (FP32 ‚Üí INT8)\n",
    "- **Faster inference** (integer operations)\n",
    "- **Lower power consumption**\n",
    "- **Better accuracy** than PTQ\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "- [PyTorch Quantization Documentation](https://pytorch.org/docs/stable/quantization.html)\n",
    "- [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877)\n",
    "- [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
