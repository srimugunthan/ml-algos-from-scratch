{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost-Style Gradient Boosting from Scratch\n",
    "\n",
    "Side-by-side implementation comparing:\n",
    "- **Traditional Gradient Boosting** (First-Order)\n",
    "- **XGBoost-Style** (Second-Order + L1/L2 Regularization)\n",
    "\n",
    "**Key XGBoost Innovations:**\n",
    "1. Uses **Hessian (second derivative)** for better optimization\n",
    "2. Explicit **L1 and L2 regularization** in objective\n",
    "3. **Optimal leaf weights** via closed-form solution\n",
    "4. **Tree complexity penalty** (gamma parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING: TRADITIONAL vs XGBOOST-STYLE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "### **Traditional GB vs XGBoost**\n",
    "\n",
    "#### **1. Taylor Expansion:**\n",
    "\n",
    "**Traditional (First-Order):**\n",
    "$$L(y, F + f) \\approx L(y, F) + g \\cdot f$$\n",
    "\n",
    "**XGBoost (Second-Order):**\n",
    "$$L(y, F + f) \\approx L(y, F) + g \\cdot f + \\frac{1}{2}h \\cdot f^2$$\n",
    "\n",
    "where:\n",
    "- $g = \\frac{\\partial L}{\\partial F}$ (gradient)\n",
    "- $h = \\frac{\\partial^2 L}{\\partial F^2}$ (Hessian)\n",
    "\n",
    "#### **2. For Log Loss:**\n",
    "$$g_i = p_i - y_i, \\quad h_i = p_i(1-p_i)$$\n",
    "\n",
    "#### **3. Leaf Weight:**\n",
    "\n",
    "**Traditional:** $w = \\text{mean}(\\text{residuals})$\n",
    "\n",
    "**XGBoost:** $w^* = -\\frac{\\sum g_i}{\\sum h_i + \\lambda}$\n",
    "\n",
    "#### **4. Regularization:**\n",
    "$$\\Omega(f) = \\gamma T + \\frac{\\lambda}{2}\\sum w_j^2 + \\alpha\\sum|w_j|$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: Traditional Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraditionalGradientBoosting:\n",
    "    \"\"\"\n",
    "    TRADITIONAL Gradient Boosting Classifier (FIRST-ORDER ONLY)\n",
    "    \n",
    "    Key Characteristics:\n",
    "    -------------------\n",
    "    ❌ Uses ONLY gradient (first derivative)\n",
    "    ❌ NO explicit regularization in objective\n",
    "    ❌ Simple mean-based leaf weights\n",
    "    ❌ NO Hessian information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, verbose=False):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.verbose = verbose\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "        self.train_losses = []\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def _log_loss(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize\n",
    "        mean_y = np.clip(np.mean(y), 1e-15, 1 - 1e-15)\n",
    "        self.init_pred = np.log(mean_y / (1 - mean_y))\n",
    "        F = np.full(len(y), self.init_pred)\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            p = self._sigmoid(F)\n",
    "            \n",
    "            # TRADITIONAL: Only first derivative (gradient)\n",
    "            residuals = y - p  # This is -∂L/∂F (negative gradient)\n",
    "            \n",
    "            # Fit tree to residuals (NO Hessian)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=m)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions (simple, no optimal weighting)\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            self.train_losses.append(self._log_loss(y, self._sigmoid(F)))\n",
    "            \n",
    "            if self.verbose and m % 20 == 0:\n",
    "                print(f\"[Trad GB] Iter {m+1:3d} | Loss: {self.train_losses[-1]:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        F = np.full(len(X), self.init_pred)\n",
    "        for tree in self.trees:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "        return self._sigmoid(F)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "print(\"✓ Traditional GB defined (First-Order Only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2: XGBoost-Style with Second-Order + Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostStyleGradientBoosting:\n",
    "    \"\"\"\n",
    "    XGBOOST-STYLE Gradient Boosting (SECOND-ORDER + REGULARIZATION)\n",
    "    \n",
    "    Key Enhancements:\n",
    "    -----------------\n",
    "    ✅ Uses BOTH gradient AND Hessian (second derivative)\n",
    "    ✅ Explicit L1 and L2 regularization in objective\n",
    "    ✅ Optimal leaf weights via closed-form: w* = -G/(H+λ)\n",
    "    ✅ Tree complexity penalty (gamma)\n",
    "    ✅ Sample weighting by Hessian (confidence-based)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                 reg_lambda=1.0,    # ← L2 regularization (NEW!)\n",
    "                 reg_alpha=0.0,     # ← L1 regularization (NEW!)\n",
    "                 gamma=0.0,         # ← Complexity penalty (NEW!)\n",
    "                 verbose=False):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda  # L2 on leaf weights\n",
    "        self.reg_alpha = reg_alpha    # L1 on leaf weights  \n",
    "        self.gamma = gamma            # Min gain to split\n",
    "        self.verbose = verbose\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "        self.train_losses = []\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def _log_loss(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "    \n",
    "    def _compute_gradients_hessians(self, y, p):\n",
    "        \"\"\"\n",
    "        XGBOOST KEY DIFFERENCE #1: Compute BOTH derivatives\n",
    "        \n",
    "        Traditional GB: Only gradient\n",
    "        XGBoost: Gradient + Hessian\n",
    "        \n",
    "        For log loss:\n",
    "          g = ∂L/∂F = p - y       (gradient)\n",
    "          h = ∂²L/∂F² = p(1-p)    (Hessian - curvature!)\n",
    "        \n",
    "        Hessian tells us how confident the prediction is:\n",
    "        - High when p ≈ 0.5 (uncertain)\n",
    "        - Low when p ≈ 0 or 1 (confident)\n",
    "        \"\"\"\n",
    "        gradients = p - y                    # First derivative\n",
    "        hessians = p * (1 - p)               # Second derivative (NEW!)\n",
    "        hessians = np.maximum(hessians, 1e-16)  # Numerical stability\n",
    "        return gradients, hessians\n",
    "    \n",
    "    def _optimal_leaf_weight(self, g_leaf, h_leaf):\n",
    "        \"\"\"\n",
    "        XGBOOST KEY DIFFERENCE #2: Optimal weight via closed-form\n",
    "        \n",
    "        Traditional GB: w = mean(residuals)\n",
    "        XGBoost: w* = -G/(H+λ)  where G=Σg, H=Σh\n",
    "        \n",
    "        This is derived by minimizing regularized objective:\n",
    "        min_w [ Σ(g*w + 0.5*h*w²) + 0.5*λ*w² + α*|w| ]\n",
    "        \n",
    "        Solution accounts for:\n",
    "        - Gradient information (numerator)\n",
    "        - Prediction confidence via Hessian (denominator)\n",
    "        - L2 regularization (λ in denominator)\n",
    "        - L1 regularization (soft thresholding)\n",
    "        \"\"\"\n",
    "        G = np.sum(g_leaf)  # Sum of gradients\n",
    "        H = np.sum(h_leaf)  # Sum of Hessians\n",
    "        \n",
    "        # Apply L1 regularization (soft thresholding)\n",
    "        if self.reg_alpha > 0:\n",
    "            if G > self.reg_alpha:\n",
    "                G -= self.reg_alpha\n",
    "            elif G < -self.reg_alpha:\n",
    "                G += self.reg_alpha\n",
    "            else:\n",
    "                return 0.0  # Shrink to zero (sparsity!)\n",
    "        \n",
    "        # Optimal weight with L2 regularization\n",
    "        weight = -G / (H + self.reg_lambda)\n",
    "        return weight\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize\n",
    "        mean_y = np.clip(np.mean(y), 1e-15, 1 - 1e-15)\n",
    "        self.init_pred = np.log(mean_y / (1 - mean_y))\n",
    "        F = np.full(len(y), self.init_pred)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Regularization: L1={self.reg_alpha}, L2={self.reg_lambda}, γ={self.gamma}\")\n",
    "        \n",
    "        for m in range(self.n_estimators):\n",
    "            p = self._sigmoid(F)\n",
    "            \n",
    "            # XGBOOST: Compute BOTH gradients and Hessians\n",
    "            g, h = self._compute_gradients_hessians(y, p)\n",
    "            \n",
    "            # XGBOOST KEY DIFFERENCE #3: Weight samples by Hessian\n",
    "            # Higher Hessian = more uncertain → higher weight\n",
    "            # This is like \"importance sampling\" based on prediction confidence\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=m)\n",
    "            tree.fit(X, -g, sample_weight=h)  # ← Hessian weighting!\n",
    "            \n",
    "            # Get leaf assignments\n",
    "            leaf_ids = tree.apply(X)\n",
    "            \n",
    "            # XGBOOST KEY DIFFERENCE #4: Compute optimal weights per leaf\n",
    "            optimal_weights = {}\n",
    "            for leaf_id in np.unique(leaf_ids):\n",
    "                mask = leaf_ids == leaf_id\n",
    "                # Use closed-form solution (not tree's predictions!)\n",
    "                optimal_weights[leaf_id] = self._optimal_leaf_weight(g[mask], h[mask])\n",
    "            \n",
    "            # Create predictions using optimal weights\n",
    "            predictions = np.array([optimal_weights[lid] for lid in leaf_ids])\n",
    "            \n",
    "            # Update\n",
    "            F += self.learning_rate * predictions\n",
    "            self.trees.append((tree, optimal_weights))\n",
    "            \n",
    "            self.train_losses.append(self._log_loss(y, self._sigmoid(F)))\n",
    "            \n",
    "            if self.verbose and m % 20 == 0:\n",
    "                print(f\"[XGB Style] Iter {m+1:3d} | Loss: {self.train_losses[-1]:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        F = np.full(len(X), self.init_pred)\n",
    "        for tree, weights in self.trees:\n",
    "            leaf_ids = tree.apply(X)\n",
    "            predictions = np.array([weights.get(lid, 0.0) for lid in leaf_ids])\n",
    "            F += self.learning_rate * predictions\n",
    "        return self._sigmoid(F)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "print(\"✓ XGBoost-Style defined (Second-Order + L1/L2 + Gamma)\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. ✅ Uses Hessian (2nd derivative) for better optimization\")\n",
    "print(\"2. ✅ L1 regularization (alpha) - promotes sparsity\")\n",
    "print(\"3. ✅ L2 regularization (lambda) - shrinks weights\")\n",
    "print(\"4. ✅ Sample weighting by Hessian (confidence-based)\")\n",
    "print(\"5. ✅ Optimal leaf weights: w* = -G/(H+λ)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15,\n",
    "    n_redundant=5, n_classes=2, random_state=42, flip_y=0.1\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} train, {len(X_test)} test, {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Traditional GB\n",
    "print(\"\\n[1] Traditional GB (First-Order):\")\n",
    "print(\"-\"*80)\n",
    "trad = TraditionalGradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3, verbose=True)\n",
    "trad.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost-Style\n",
    "print(\"\\n[2] XGBoost-Style (Second-Order + Regularization):\")\n",
    "print(\"-\"*80)\n",
    "xgb = XGBoostStyleGradientBoosting(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "    reg_lambda=1.0, reg_alpha=0.1, gamma=0.1, verbose=True\n",
    ")\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate & Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred_trad = trad.predict(X_test)\n",
    "y_proba_trad = trad.predict_proba(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "y_proba_xgb = xgb.predict_proba(X_test)\n",
    "\n",
    "acc_trad = accuracy_score(y_test, y_pred_trad)\n",
    "auc_trad = roc_auc_score(y_test, y_proba_trad)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraditional GB:  Accuracy={acc_trad:.4f}, AUC={auc_trad:.4f}\")\n",
    "print(f\"XGBoost-Style:   Accuracy={acc_xgb:.4f}, AUC={auc_xgb:.4f}\")\n",
    "print(f\"\\nImprovement:     Accuracy={acc_xgb-acc_trad:+.4f}, AUC={auc_xgb-auc_trad:+.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Training curves\n",
    "axes[0].plot(trad.train_losses, label='Traditional GB (1st order)', linewidth=2)\n",
    "axes[0].plot(xgb.train_losses, label='XGBoost-Style (2nd order + reg)', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Log Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC curves\n",
    "fpr_t, tpr_t, _ = roc_curve(y_test, y_proba_trad)\n",
    "fpr_x, tpr_x, _ = roc_curve(y_test, y_proba_xgb)\n",
    "axes[1].plot(fpr_t, tpr_t, label=f'Traditional (AUC={auc_trad:.3f})', linewidth=2)\n",
    "axes[1].plot(fpr_x, tpr_x, label=f'XGBoost-Style (AUC={auc_xgb:.3f})', linewidth=2)\n",
    "axes[1].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "axes[1].set_xlabel('FPR', fontsize=12)\n",
    "axes[1].set_ylabel('TPR', fontsize=12)\n",
    "axes[1].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION EFFECTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "configs = [\n",
    "    ('No Reg', 0.0, 0.0, 0.0),\n",
    "    ('L2 Only', 1.0, 0.0, 0.0),\n",
    "    ('L1 Only', 0.0, 0.5, 0.0),\n",
    "    ('L1+L2', 1.0, 0.1, 0.0),\n",
    "    ('L1+L2+Gamma', 1.0, 0.1, 0.1),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, lam, alpha, gamma in configs:\n",
    "    model = XGBoostStyleGradientBoosting(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "        reg_lambda=lam, reg_alpha=alpha, gamma=gamma\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    results.append({\n",
    "        'Config': name,\n",
    "        'Train': f\"{train_acc:.4f}\",\n",
    "        'Test': f\"{test_acc:.4f}\",\n",
    "        'Gap': f\"{train_acc - test_acc:.4f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n\" + df.to_string(index=False))\n",
    "print(\"\\n✓ Regularization reduces train-test gap (overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual Example: Leaf Weight Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCEPTUAL EXAMPLE: How XGBoost Computes Better Leaf Weights\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example leaf with 10 samples\n",
    "y_ex = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "p_ex = np.array([0.8, 0.7, 0.6, 0.9, 0.85, 0.3, 0.2, 0.4, 0.1, 0.25])\n",
    "\n",
    "print(\"\\nLeaf with 10 samples:\")\n",
    "print(f\"  True labels: {y_ex}\")\n",
    "print(f\"  Predictions: {p_ex}\")\n",
    "\n",
    "# Traditional GB\n",
    "residuals = y_ex - p_ex\n",
    "w_trad = np.mean(residuals)\n",
    "print(f\"\\nTraditional GB (First-Order):\")\n",
    "print(f\"  Residuals: {residuals}\")\n",
    "print(f\"  Weight = mean(residuals) = {w_trad:.4f}\")\n",
    "\n",
    "# XGBoost\n",
    "g = p_ex - y_ex\n",
    "h = p_ex * (1 - p_ex)\n",
    "G, H = np.sum(g), np.sum(h)\n",
    "lam = 1.0\n",
    "w_xgb = -G / (H + lam)\n",
    "\n",
    "print(f\"\\nXGBoost-Style (Second-Order + L2):\")\n",
    "print(f\"  Gradients: {g}\")\n",
    "print(f\"  Hessians:  {h}\")\n",
    "print(f\"  G (Σg) = {G:.4f}\")\n",
    "print(f\"  H (Σh) = {H:.4f}\")\n",
    "print(f\"  Weight = -G/(H+λ) = {w_xgb:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference: {abs(w_trad - w_xgb):.4f}\")\n",
    "print(\"\\nWhy XGBoost weight is better:\")\n",
    "print(\"  • Accounts for prediction confidence (Hessian)\")\n",
    "print(\"  • Applies regularization (λ prevents overfitting)\")\n",
    "print(\"  • Mathematically optimal for quadratic approximation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### **Comparison Table:**\n",
    "\n",
    "| Feature | Traditional GB | XGBoost-Style |\n",
    "|---------|---------------|---------------|\n",
    "| **Taylor Order** | 1st (gradient) | 2nd (gradient + Hessian) |\n",
    "| **Leaf Weight** | mean(residuals) | -G/(H+λ) |\n",
    "| **L2 Regularization** | ❌ | ✅ (lambda) |\n",
    "| **L1 Regularization** | ❌ | ✅ (alpha) |\n",
    "| **Complexity Penalty** | ❌ | ✅ (gamma) |\n",
    "| **Sample Weighting** | Equal | By Hessian |\n",
    "| **Optimization** | Gradient descent | Newton-like |\n",
    "\n",
    "### **Key Advantages of Second-Order:**\n",
    "\n",
    "1. **Better Approximation:** Quadratic vs linear\n",
    "2. **Confidence Weighting:** Hessian = uncertainty measure\n",
    "3. **Adaptive Steps:** Automatically adjusts based on curvature\n",
    "4. **Faster Convergence:** Like Newton's method vs gradient descent\n",
    "\n",
    "### **When XGBoost-Style Helps Most:**\n",
    "- Noisy data\n",
    "- Non-convex loss surfaces  \n",
    "- Variable prediction confidence\n",
    "- Need for regularization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
