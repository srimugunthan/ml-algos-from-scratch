{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch - Binary Classification\n",
    "\n",
    "This notebook implements logistic regression for binary classification without using scikit-learn's LogisticRegression. We'll build everything from the ground up to understand the mathematics and implementation details.\n",
    "\n",
    "**Author:** Educational Implementation  \n",
    "**Date:** February 2026\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOGISTIC REGRESSION FROM SCRATCH\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mathematical Foundation\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Sigmoid Function (Activation):**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Properties:\n",
    "- Maps any real number to (0, 1)\n",
    "- Output can be interpreted as probability\n",
    "- Derivative: $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "**2. Hypothesis:**\n",
    "$$h(x) = \\sigma(w^T x + b)$$\n",
    "\n",
    "where $w$ are weights, $b$ is bias, $x$ is input\n",
    "\n",
    "**3. Binary Cross-Entropy Loss:**\n",
    "$$L(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h(x^{(i)})) + (1-y^{(i)}) \\log(1-h(x^{(i)}))]$$\n",
    "\n",
    "For a single sample:\n",
    "- If $y=1$: loss $= -\\log(h(x))$ (want $h(x)$ close to 1)\n",
    "- If $y=0$: loss $= -\\log(1-h(x))$ (want $h(x)$ close to 0)\n",
    "\n",
    "**4. Gradient Descent Update:**\n",
    "$$w := w - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X^T (h(x) - y)$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum(h(x) - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    \"\"\"\n",
    "    Logistic Regression implemented from scratch using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Step size for gradient descent updates\n",
    "    n_iterations : int, default=1000\n",
    "        Number of gradient descent iterations\n",
    "    regularization : str, default=None\n",
    "        Type of regularization: None, 'l1', or 'l2'\n",
    "    lambda_reg : float, default=0.01\n",
    "        Regularization strength\n",
    "    verbose : bool, default=False\n",
    "        Whether to print training progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, \n",
    "                 regularization=None, lambda_reg=0.01, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Model parameters (initialized during fit)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \n",
    "        Numerical stability: clip z to avoid overflow in exp\n",
    "        \"\"\"\n",
    "        z = np.clip(z, -500, 500)  # Prevent overflow\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred, weights):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss with optional regularization.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            True binary labels\n",
    "        y_pred : array-like, shape (n_samples,)\n",
    "            Predicted probabilities\n",
    "        weights : array-like, shape (n_features,)\n",
    "            Current weight values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Average loss across samples\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Binary cross-entropy\n",
    "        bce_loss = -np.mean(\n",
    "            y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
    "        )\n",
    "        \n",
    "        # Add regularization term\n",
    "        reg_term = 0\n",
    "        if self.regularization == 'l2':\n",
    "            reg_term = (self.lambda_reg / (2 * m)) * np.sum(weights ** 2)\n",
    "        elif self.regularization == 'l1':\n",
    "            reg_term = (self.lambda_reg / m) * np.sum(np.abs(weights))\n",
    "            \n",
    "        return bce_loss + reg_term\n",
    "    \n",
    "    def _compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and bias.\n",
    "        \n",
    "        Mathematical derivation:\n",
    "        For loss L = -1/m Σ[y*log(h) + (1-y)*log(1-h)]\n",
    "        \n",
    "        ∂L/∂w_j = 1/m Σ(h(x_i) - y_i) * x_ij\n",
    "        ∂L/∂b = 1/m Σ(h(x_i) - y_i)\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        error = y_pred - y_true  # Shape: (m,)\n",
    "        \n",
    "        # Gradient for weights\n",
    "        dw = (1/m) * X.T.dot(error)  # Shape: (n_features,)\n",
    "        \n",
    "        # Add regularization gradient\n",
    "        if self.regularization == 'l2':\n",
    "            dw += (self.lambda_reg / m) * self.weights\n",
    "        elif self.regularization == 'l1':\n",
    "            dw += (self.lambda_reg / m) * np.sign(self.weights)\n",
    "        \n",
    "        # Gradient for bias (no regularization on bias)\n",
    "        db = (1/m) * np.sum(error)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit logistic regression model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Binary target values (0 or 1)\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Forward pass: compute predictions\n",
    "            linear_output = X.dot(self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear_output)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self._compute_loss(y, y_pred, self.weights)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "            accuracy = np.mean(y_pred_class == y)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self._compute_gradients(X, y, y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Print progress\n",
    "            if self.verbose and (iteration % 100 == 0 or iteration == self.n_iterations - 1):\n",
    "                print(f\"Iteration {iteration:4d} | Loss: {loss:.6f} | Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        proba : array-like, shape (n_samples,)\n",
    "            Probability of positive class\n",
    "        \"\"\"\n",
    "        linear_output = X.dot(self.weights) + self.bias\n",
    "        return self._sigmoid(linear_output)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict binary class labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        threshold : float, default=0.5\n",
    "            Decision threshold for classification\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return model parameters.\"\"\"\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'bias': self.bias,\n",
    "            'loss_history': self.loss_history,\n",
    "            'accuracy_history': self.accuracy_history\n",
    "        }\n",
    "\n",
    "print(\"\\n✓ LogisticRegressionScratch class defined\")\n",
    "print(\"  - Sigmoid activation\")\n",
    "print(\"  - Binary cross-entropy loss\")\n",
    "print(\"  - Gradient descent optimization\")\n",
    "print(\"  - Optional L1/L2 regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SYNTHETIC DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    "    flip_y=0.1  # Add 10% label noise\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Class 0: {np.sum(y == 0)} samples ({np.mean(y == 0)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {np.sum(y == 1)} samples ({np.mean(y == 1)*100:.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features (important for gradient descent!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING BASIC LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train model\n",
    "model_basic = LogisticRegressionScratch(\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=1000,\n",
    "    regularization=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model_basic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_proba = model_basic.predict_proba(X_test_scaled)\n",
    "y_pred = model_basic.predict(X_test_scaled)\n",
    "\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(model_basic.loss_history, linewidth=2)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(model_basic.accuracy_history, linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves show convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Detailed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random classifier')\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nDetailed Metrics:\")\n",
    "print(f\"  True Positives:  {tp}\")\n",
    "print(f\"  True Negatives:  {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  Sensitivity (Recall): {tp/(tp+fn):.4f}\")\n",
    "print(f\"  Specificity: {tn/(tn+fp):.4f}\")\n",
    "print(f\"  Precision: {tp/(tp+fp):.4f}\")\n",
    "print(f\"  F1-Score: {2*tp/(2*tp+fp+fn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARING REGULARIZATION TECHNIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "models = {}\n",
    "regularizations = [None, 'l1', 'l2']\n",
    "\n",
    "for reg in regularizations:\n",
    "    print(f\"\\nTraining with regularization: {reg if reg else 'None'}\")\n",
    "    \n",
    "    model = LogisticRegressionScratch(\n",
    "        learning_rate=0.1,\n",
    "        n_iterations=1000,\n",
    "        regularization=reg,\n",
    "        lambda_reg=0.01,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    \n",
    "    models[reg if reg else 'none'] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compare regularization effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (reg_type, model_info) in enumerate(models.items()):\n",
    "    model = model_info['model']\n",
    "    \n",
    "    axes[idx].plot(model.loss_history, linewidth=2)\n",
    "    axes[idx].set_xlabel('Iteration', fontsize=12)\n",
    "    axes[idx].set_ylabel('Loss', fontsize=12)\n",
    "    axes[idx].set_title(f'{reg_type.upper()} Regularization\\n(Accuracy: {model_info[\"accuracy\"]:.4f})',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use model without regularization for clearer interpretation\n",
    "weights = model_basic.weights\n",
    "feature_importance = np.abs(weights)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(f\"{'Rank':<6} {'Feature':<12} {'Weight':<12} {'|Weight|':<12}\")\n",
    "print(\"-\" * 48)\n",
    "for i, idx in enumerate(sorted_idx[:10], 1):\n",
    "    print(f\"{i:<6} Feature {idx:<3}   {weights[idx]:>10.4f}   {feature_importance[idx]:>10.4f}\")\n",
    "\n",
    "# Visualize all feature weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot of feature importance\n",
    "axes[0].bar(range(len(feature_importance)), feature_importance[sorted_idx])\n",
    "axes[0].set_xlabel('Feature Rank', fontsize=12)\n",
    "axes[0].set_ylabel('|Weight|', fontsize=12)\n",
    "axes[0].set_title('Feature Importance (Absolute Weight Values)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Weight distribution\n",
    "axes[1].hist(weights, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "axes[1].set_xlabel('Weight Value', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of Feature Weights', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Decision Boundary Visualization (2D Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DECISION BOUNDARY VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Take the two most important features for visualization\n",
    "top_2_features = sorted_idx[:2]\n",
    "X_train_2d = X_train_scaled[:, top_2_features]\n",
    "X_test_2d = X_test_scaled[:, top_2_features]\n",
    "\n",
    "# Train model on 2D data\n",
    "model_2d = LogisticRegressionScratch(\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=1000,\n",
    "    regularization=None,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_2d.fit(X_train_2d, y_train)\n",
    "y_pred_2d = model_2d.predict(X_test_2d)\n",
    "accuracy_2d = np.mean(y_pred_2d == y_test)\n",
    "\n",
    "print(f\"\\n2D Model Accuracy: {accuracy_2d:.4f}\")\n",
    "print(f\"Features used: {top_2_features[0]} and {top_2_features[1]}\")\n",
    "\n",
    "# Create mesh grid for decision boundary\n",
    "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict on mesh\n",
    "Z = model_2d.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Decision boundary and regions\n",
    "contourf = ax.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "contour = ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3)\n",
    "\n",
    "# Scatter plot of data points\n",
    "scatter = ax.scatter(X_test_2d[:, 0], X_test_2d[:, 1], \n",
    "                    c=y_test, cmap='RdYlBu', s=100, \n",
    "                    edgecolors='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(f'Feature {top_2_features[0]}', fontsize=12)\n",
    "ax.set_ylabel(f'Feature {top_2_features[1]}', fontsize=12)\n",
    "ax.set_title(f'Decision Boundary (2D Projection)\\nAccuracy: {accuracy_2d:.4f}', \n",
    "            fontsize=14, fontweight='bold')\n",
    "\n",
    "# Colorbar for probability\n",
    "cbar = plt.colorbar(contourf, ax=ax)\n",
    "cbar.set_label('P(Class = 1)', fontsize=12)\n",
    "\n",
    "# Legend\n",
    "legend_labels = ['Class 0', 'Class 1']\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                            markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                            markersize=10, markeredgecolor='black')\n",
    "                 for i in [0, 1]]\n",
    "ax.legend(legend_handles, legend_labels, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Learning Rate Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LEARNING RATE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = LogisticRegressionScratch(\n",
    "        learning_rate=lr,\n",
    "        n_iterations=1000,\n",
    "        regularization=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    \n",
    "    lr_results[lr] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"Learning Rate: {lr:6.3f} | Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot learning curves for different learning rates\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (lr, results) in enumerate(lr_results.items()):\n",
    "    model = results['model']\n",
    "    \n",
    "    axes[idx].plot(model.loss_history, linewidth=2)\n",
    "    axes[idx].set_xlabel('Iteration', fontsize=11)\n",
    "    axes[idx].set_ylabel('Loss', fontsize=11)\n",
    "    axes[idx].set_title(f'Learning Rate: {lr} (Acc: {results[\"accuracy\"]:.4f})',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of Learning Rate on Convergence', \n",
    "            fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Probability Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROBABILITY CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = model_basic.predict_proba(X_test_scaled)\n",
    "\n",
    "# Create probability bins\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Calculate actual positive rate in each bin\n",
    "bin_true_probs = []\n",
    "bin_pred_probs = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = (y_pred_proba >= bins[i]) & (y_pred_proba < bins[i+1])\n",
    "    if np.sum(mask) > 0:\n",
    "        bin_true_probs.append(np.mean(y_test[mask]))\n",
    "        bin_pred_probs.append(np.mean(y_pred_proba[mask]))\n",
    "        bin_counts.append(np.sum(mask))\n",
    "    else:\n",
    "        bin_true_probs.append(np.nan)\n",
    "        bin_pred_probs.append(np.nan)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Reliability diagram\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect calibration')\n",
    "axes[0].plot(bin_pred_probs, bin_true_probs, 'o-', linewidth=2, \n",
    "            markersize=8, label='Model calibration')\n",
    "axes[0].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('Calibration Curve (Reliability Diagram)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, 1])\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "axes[1].hist([y_pred_proba[y_test == 0], y_pred_proba[y_test == 1]], \n",
    "            bins=20, label=['Class 0', 'Class 1'], alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, \n",
    "               label='Decision threshold')\n",
    "axes[1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Distribution of Predicted Probabilities', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Calibration analysis complete\")\n",
    "print(f\"  - Well-calibrated: points close to diagonal line\")\n",
    "print(f\"  - Overconfident: points below diagonal\")\n",
    "print(f\"  - Underconfident: points above diagonal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Gradient Magnitude Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GRADIENT MAGNITUDE TRACKING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a new model with gradient tracking\n",
    "class LogisticRegressionWithGradients(LogisticRegressionScratch):\n",
    "    \"\"\"Extended version that tracks gradient magnitudes.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gradient_magnitudes = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent with gradient tracking\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            linear_output = X.dot(self.weights) + self.bias\n",
    "            y_pred = self._sigmoid(linear_output)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self._compute_loss(y, y_pred, self.weights)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "            accuracy = np.mean(y_pred_class == y)\n",
    "            self.accuracy_history.append(accuracy)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self._compute_gradients(X, y, y_pred)\n",
    "            \n",
    "            # Track gradient magnitude\n",
    "            grad_magnitude = np.sqrt(np.sum(dw**2) + db**2)\n",
    "            self.gradient_magnitudes.append(grad_magnitude)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            if self.verbose and (iteration % 100 == 0):\n",
    "                print(f\"Iter {iteration:4d} | Loss: {loss:.6f} | \"\n",
    "                      f\"Grad Magnitude: {grad_magnitude:.6f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Train model with gradient tracking\n",
    "model_grad = LogisticRegressionWithGradients(\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=1000,\n",
    "    regularization=None,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_grad.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Plot gradient magnitudes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].plot(model_grad.gradient_magnitudes, linewidth=2, color='purple')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Gradient Magnitude', fontsize=12)\n",
    "axes[0].set_title('Gradient Magnitude Over Time (Linear Scale)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "axes[1].semilogy(model_grad.gradient_magnitudes, linewidth=2, color='purple')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Gradient Magnitude (log scale)', fontsize=12)\n",
    "axes[1].set_title('Gradient Magnitude Over Time (Log Scale)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Gradient magnitude analysis complete\")\n",
    "print(f\"  - Initial gradient: {model_grad.gradient_magnitudes[0]:.6f}\")\n",
    "print(f\"  - Final gradient: {model_grad.gradient_magnitudes[-1]:.6f}\")\n",
    "print(f\"  - Reduction factor: {model_grad.gradient_magnitudes[0]/model_grad.gradient_magnitudes[-1]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY AND KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = {\n",
    "    'Model': ['Basic', 'L1 Regularized', 'L2 Regularized'],\n",
    "    'Test Accuracy': [\n",
    "        models['none']['accuracy'],\n",
    "        models['l1']['accuracy'],\n",
    "        models['l2']['accuracy']\n",
    "    ],\n",
    "    'Final Loss': [\n",
    "        models['none']['model'].loss_history[-1],\n",
    "        models['l1']['model'].loss_history[-1],\n",
    "        models['l2']['model'].loss_history[-1]\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY CONCEPTS DEMONSTRATED:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. SIGMOID FUNCTION\n",
    "   - Maps linear output to (0,1) probability range\n",
    "   - Smooth, differentiable activation function\n",
    "   \n",
    "2. BINARY CROSS-ENTROPY LOSS\n",
    "   - Measures how well probabilities match true labels\n",
    "   - Penalizes confident wrong predictions heavily\n",
    "   \n",
    "3. GRADIENT DESCENT OPTIMIZATION\n",
    "   - Iteratively updates weights to minimize loss\n",
    "   - Learning rate controls step size\n",
    "   - Gradients computed via chain rule\n",
    "   \n",
    "4. REGULARIZATION\n",
    "   - L1: Encourages sparsity (feature selection)\n",
    "   - L2: Encourages small weights (prevents overfitting)\n",
    "   \n",
    "5. MODEL EVALUATION\n",
    "   - Accuracy: Overall correctness\n",
    "   - Precision: Positive predictions that are correct\n",
    "   - Recall: Actual positives that are found\n",
    "   - ROC-AUC: Threshold-independent performance\n",
    "   \n",
    "6. FEATURE IMPORTANCE\n",
    "   - Magnitude of weights indicates importance\n",
    "   - Sign indicates positive/negative correlation\n",
    "   \n",
    "7. PROBABILITY CALIBRATION\n",
    "   - Predicted probabilities should match empirical frequencies\n",
    "   - Important for decision-making under uncertainty\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTATION NOTES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "✓ Numerical Stability:\n",
    "  - Clip sigmoid inputs to prevent overflow\n",
    "  - Add epsilon to log computations to avoid log(0)\n",
    "  \n",
    "✓ Feature Scaling:\n",
    "  - StandardScaler applied before training\n",
    "  - Critical for gradient descent convergence\n",
    "  \n",
    "✓ Vectorization:\n",
    "  - Matrix operations for efficiency\n",
    "  - Avoids slow Python loops\n",
    "  \n",
    "✓ Training Monitoring:\n",
    "  - Track loss and accuracy\n",
    "  - Visualize convergence\n",
    "  - Detect potential issues early\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
