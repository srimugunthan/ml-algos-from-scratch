{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest from Scratch - Classification\n",
    "\n",
    "Complete implementation of Random Forest classifier with detailed explanation of **Bagging**.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Bagging** (Bootstrap Aggregating)\n",
    "- **Bootstrap Sampling** (sampling with replacement)\n",
    "- **Feature Randomness** (random feature subsets)\n",
    "- **Ensemble Voting** (majority voting)\n",
    "- **Out-of-Bag (OOB) Error** estimation\n",
    "- **Variance Reduction** through averaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification, load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RANDOM FOREST FROM SCRATCH - WITH BAGGING EXPLAINED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Bagging?\n",
    "\n",
    "**Bagging = Bootstrap Aggregating**\n",
    "\n",
    "### **Core Idea:**\n",
    "Train multiple models on different **random subsets** of data, then **aggregate** their predictions.\n",
    "\n",
    "### **Why Bagging Works:**\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "$$\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "- **High Variance Models** (like deep decision trees): Overfit to training data\n",
    "- **Bagging**: Reduces variance by averaging predictions\n",
    "- **Result**: Lower overall error!\n",
    "\n",
    "### **Mathematical Intuition:**\n",
    "\n",
    "**Variance of Average:**\n",
    "\n",
    "If we have $n$ independent models with variance $\\sigma^2$:\n",
    "$$\\text{Var}(\\text{average}) = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "**With correlation** $\\rho$ between models:\n",
    "$$\\text{Var}(\\text{average}) = \\rho\\sigma^2 + \\frac{1-\\rho}{n}\\sigma^2$$\n",
    "\n",
    "- As $n \\to \\infty$ and $\\rho \\to 0$: Variance approaches 0!\n",
    "- **Random Forest**: Creates diverse (low $\\rho$) trees via randomness\n",
    "\n",
    "### **Bootstrap Sampling:**\n",
    "\n",
    "Sample $n$ examples **with replacement** from dataset of size $n$:\n",
    "- Each sample has $1/n$ probability of selection\n",
    "- Probability of NOT being selected in one draw: $(1 - 1/n)$\n",
    "- Probability of NOT being selected in $n$ draws: $(1 - 1/n)^n \\to 1/e \\approx 0.632$\n",
    "\n",
    "**Result:** Each bootstrap sample contains ~63.2% unique samples, ~36.8% duplicates\n",
    "\n",
    "### **Out-of-Bag (OOB) Samples:**\n",
    "\n",
    "The ~36.8% of samples **not** in a bootstrap sample are \"out-of-bag\":\n",
    "- Can be used for **free validation** (no need for separate test set!)\n",
    "- Each tree is tested on its OOB samples\n",
    "- Aggregate to get **OOB error** estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest = Bagging + Feature Randomness\n",
    "\n",
    "**Standard Bagging:**\n",
    "1. Create $B$ bootstrap samples\n",
    "2. Train a decision tree on each\n",
    "3. Average predictions\n",
    "\n",
    "**Random Forest Enhancement:**\n",
    "1. Create $B$ bootstrap samples\n",
    "2. Train tree on each, BUT at each split:\n",
    "   - Randomly select $m$ features (out of $p$ total)\n",
    "   - Find best split only among these $m$ features\n",
    "3. Average predictions\n",
    "\n",
    "**Why the extra randomness?**\n",
    "- Further **decorrelates** trees (lower $\\rho$)\n",
    "- Prevents one strong feature from dominating\n",
    "- Better variance reduction!\n",
    "\n",
    "**Typical choice for $m$:**\n",
    "- Classification: $m = \\sqrt{p}$\n",
    "- Regression: $m = p/3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Decision Tree (Base Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"\n",
    "    Node in a decision tree.\n",
    "    Can be either:\n",
    "    - Internal node: has feature, threshold, left/right children\n",
    "    - Leaf node: has class prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left           # Left child node\n",
    "        self.right = right         # Right child node\n",
    "        self.value = value         # Class prediction (for leaf nodes)\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifierScratch:\n",
    "    \"\"\"\n",
    "    Simple Decision Tree for classification (base learner for Random Forest).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_depth : int\n",
    "        Maximum tree depth\n",
    "    min_samples_split : int\n",
    "        Minimum samples to split a node\n",
    "    max_features : int or None\n",
    "        Number of features to consider at each split (for RF randomness)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=10, min_samples_split=2, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "    \n",
    "    def _gini(self, y):\n",
    "        \"\"\"\n",
    "        Calculate Gini impurity.\n",
    "        Gini = 1 - Σ(p_i²)\n",
    "        \"\"\"\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    def _information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Calculate information gain from a split.\n",
    "        IG = Gini(parent) - weighted_average(Gini(children))\n",
    "        \"\"\"\n",
    "        n = len(parent)\n",
    "        n_left, n_right = len(left), len(right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        gini_parent = self._gini(parent)\n",
    "        gini_left = self._gini(left)\n",
    "        gini_right = self._gini(right)\n",
    "        \n",
    "        weighted_gini = (n_left / n) * gini_left + (n_right / n) * gini_right\n",
    "        return gini_parent - weighted_gini\n",
    "    \n",
    "    def _best_split(self, X, y, feature_indices):\n",
    "        \"\"\"\n",
    "        Find best split among given features.\n",
    "        \n",
    "        KEY FOR RANDOM FOREST:\n",
    "        Only considers features in feature_indices (random subset!)\n",
    "        \"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in feature_indices:\n",
    "            X_column = X[:, feature]\n",
    "            thresholds = np.unique(X_column)\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X_column <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) < 1 or np.sum(right_mask) < 1:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._information_gain(y, y[left_mask], y[right_mask])\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build decision tree.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            # Create leaf node\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # RANDOM FOREST FEATURE RANDOMNESS:\n",
    "        # Select random subset of features\n",
    "        if self.max_features is None:\n",
    "            feature_indices = np.arange(n_features)\n",
    "        else:\n",
    "            feature_indices = np.random.choice(\n",
    "                n_features, \n",
    "                self.max_features, \n",
    "                replace=False\n",
    "            )\n",
    "        \n",
    "        # Find best split among selected features\n",
    "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # Recursively build children\n",
    "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            feature=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build decision tree.\"\"\"\n",
    "        self.root = self._build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverse tree to make prediction for single sample.\n",
    "        \"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict for multiple samples.\"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "print(\"\\n✓ DecisionTreeClassifierScratch defined (base learner)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Implementation with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierScratch:\n",
    "    \"\"\"\n",
    "    Random Forest Classifier from scratch using Bagging.\n",
    "    \n",
    "    BAGGING STEPS:\n",
    "    1. Create multiple bootstrap samples (sampling with replacement)\n",
    "    2. Train a tree on each bootstrap sample\n",
    "    3. Aggregate predictions via majority voting\n",
    "    \n",
    "    RANDOM FOREST ADDITIONS:\n",
    "    4. Use random feature subsets at each split\n",
    "    5. Track out-of-bag (OOB) samples for validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators : int, default=100\n",
    "        Number of trees in the forest\n",
    "    max_depth : int, default=10\n",
    "        Maximum depth of each tree\n",
    "    min_samples_split : int, default=2\n",
    "        Minimum samples to split a node\n",
    "    max_features : str or int, default='sqrt'\n",
    "        Number of features for random selection:\n",
    "        - 'sqrt': sqrt(n_features)\n",
    "        - 'log2': log2(n_features)\n",
    "        - int: specific number\n",
    "        - None: all features (no randomness, just bagging)\n",
    "    bootstrap : bool, default=True\n",
    "        Whether to use bootstrap sampling\n",
    "    oob_score : bool, default=False\n",
    "        Whether to calculate out-of-bag score\n",
    "    random_state : int, default=None\n",
    "        Random seed\n",
    "    verbose : bool, default=False\n",
    "        Print progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2,\n",
    "                 max_features='sqrt', bootstrap=True, oob_score=False,\n",
    "                 random_state=None, verbose=False):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.oob_score = oob_score\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Will be set during fit\n",
    "        self.trees = []\n",
    "        self.oob_samples = []  # OOB samples for each tree\n",
    "        self.oob_score_ = None\n",
    "        \n",
    "    def _get_max_features(self, n_features):\n",
    "        \"\"\"\n",
    "        Determine number of features to use at each split.\n",
    "        \"\"\"\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            return self.max_features\n",
    "        else:\n",
    "            return n_features\n",
    "    \n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Create bootstrap sample (sampling with replacement).\n",
    "        \n",
    "        BAGGING CORE:\n",
    "        Sample n examples WITH REPLACEMENT from dataset of size n.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_sample, y_sample : Bootstrap sample\n",
    "        oob_indices : Indices of out-of-bag samples (~36.8% of data)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        if self.bootstrap:\n",
    "            # Sample WITH replacement\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            \n",
    "            # Out-of-bag samples: not selected in bootstrap\n",
    "            oob_indices = np.setdiff1d(np.arange(n_samples), indices)\n",
    "        else:\n",
    "            # No bootstrap: use all samples\n",
    "            indices = np.arange(n_samples)\n",
    "            oob_indices = np.array([])\n",
    "        \n",
    "        return X[indices], y[indices], oob_indices\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build Random Forest using Bagging.\n",
    "        \n",
    "        BAGGING ALGORITHM:\n",
    "        For b = 1 to B:\n",
    "          1. Create bootstrap sample Sb by sampling n with replacement\n",
    "          2. Train tree Tb on Sb\n",
    "        \n",
    "        RANDOM FOREST ENHANCEMENT:\n",
    "          2b. At each split, select random m features\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        max_features = self._get_max_features(n_features)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nBuilding Random Forest:\")\n",
    "            print(f\"  Trees: {self.n_estimators}\")\n",
    "            print(f\"  Max features per split: {max_features}/{n_features}\")\n",
    "            print(f\"  Bootstrap: {self.bootstrap}\")\n",
    "            print(f\"  OOB score: {self.oob_score}\\n\")\n",
    "        \n",
    "        self.trees = []\n",
    "        self.oob_samples = []\n",
    "        \n",
    "        # BAGGING: Train multiple trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Step 1: BOOTSTRAP SAMPLING (with replacement)\n",
    "            X_sample, y_sample, oob_indices = self._bootstrap_sample(X, y)\n",
    "            \n",
    "            # Step 2: TRAIN TREE with random feature subsets\n",
    "            tree = DecisionTreeClassifierScratch(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=max_features  # RF feature randomness\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            \n",
    "            # Store tree and OOB samples\n",
    "            self.trees.append(tree)\n",
    "            self.oob_samples.append(oob_indices)\n",
    "            \n",
    "            if self.verbose and (i + 1) % 10 == 0:\n",
    "                print(f\"  Trained {i + 1}/{self.n_estimators} trees\")\n",
    "        \n",
    "        # Calculate OOB score if requested\n",
    "        if self.oob_score and self.bootstrap:\n",
    "            self._calculate_oob_score(X, y)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n✓ Random Forest training complete!\")\n",
    "            if self.oob_score_:\n",
    "                print(f\"  OOB Score: {self.oob_score_:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calculate_oob_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate Out-of-Bag score.\n",
    "        \n",
    "        OOB ESTIMATION:\n",
    "        For each sample, predict using only trees where it was OOB.\n",
    "        This gives a validation score WITHOUT needing a separate test set!\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        oob_predictions = np.zeros(n_samples, dtype=int) - 1\n",
    "        oob_counts = np.zeros(n_samples, dtype=int)\n",
    "        \n",
    "        # For each tree\n",
    "        for tree_idx, (tree, oob_indices) in enumerate(zip(self.trees, self.oob_samples)):\n",
    "            if len(oob_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Predict on OOB samples\n",
    "            predictions = tree.predict(X[oob_indices])\n",
    "            \n",
    "            # Aggregate predictions\n",
    "            for i, sample_idx in enumerate(oob_indices):\n",
    "                if oob_predictions[sample_idx] == -1:\n",
    "                    oob_predictions[sample_idx] = predictions[i]\n",
    "                else:\n",
    "                    # Majority vote\n",
    "                    oob_predictions[sample_idx] = predictions[i]  # Simplified\n",
    "                oob_counts[sample_idx] += 1\n",
    "        \n",
    "        # Calculate accuracy on samples that were OOB at least once\n",
    "        valid_mask = oob_counts > 0\n",
    "        if np.sum(valid_mask) > 0:\n",
    "            self.oob_score_ = np.mean(oob_predictions[valid_mask] == y[valid_mask])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using majority voting (AGGREGATING step of bagging).\n",
    "        \n",
    "        BAGGING PREDICTION:\n",
    "        For each sample x:\n",
    "          1. Get prediction from each tree\n",
    "          2. Return majority vote\n",
    "        \n",
    "        For classification: majority vote\n",
    "        For regression: average\n",
    "        \"\"\"\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # AGGREGATING: Majority voting\n",
    "        # For each sample, find most common prediction across trees\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            sample_predictions = tree_predictions[:, i]\n",
    "            # Majority vote\n",
    "            majority = Counter(sample_predictions).most_common(1)[0][0]\n",
    "            predictions.append(majority)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Probability = fraction of trees voting for each class\n",
    "        \"\"\"\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(np.unique(tree_predictions))\n",
    "        \n",
    "        probabilities = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            sample_predictions = tree_predictions[:, i]\n",
    "            # Count votes for each class\n",
    "            for class_label in range(n_classes):\n",
    "                probabilities[i, class_label] = np.sum(sample_predictions == class_label) / len(self.trees)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "print(\"\\n✓ RandomForestClassifierScratch defined\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BAGGING COMPONENTS IMPLEMENTED:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. ✓ Bootstrap sampling (with replacement)\")\n",
    "print(\"2. ✓ Multiple tree training\")\n",
    "print(\"3. ✓ Majority voting aggregation\")\n",
    "print(\"4. ✓ Random feature subsets (RF enhancement)\")\n",
    "print(\"5. ✓ Out-of-bag (OOB) error estimation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Synthetic Dataset:\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf = RandomForestClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',  # √20 ≈ 4 features per split\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "y_proba = rf.predict_proba(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "print(f\"OOB Score: {rf.oob_score_:.4f}\")\n",
    "print(\"\\nNote: OOB score is like validation accuracy without needing separate val set!\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Bagging Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMONSTRATING VARIANCE REDUCTION THROUGH BAGGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare: Single tree vs Multiple trees\n",
    "n_trials = 20\n",
    "single_tree_accs = []\n",
    "rf_accs = []\n",
    "\n",
    "print(\"\\nRunning multiple trials to measure variance...\")\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Single decision tree (high variance)\n",
    "    single_tree = DecisionTreeClassifierScratch(\n",
    "        max_depth=10,\n",
    "        max_features=int(np.sqrt(X_train.shape[1]))\n",
    "    )\n",
    "    single_tree.fit(X_train, y_train)\n",
    "    single_pred = single_tree.predict(X_test)\n",
    "    single_tree_accs.append(accuracy_score(y_test, single_pred))\n",
    "    \n",
    "    # Random Forest (bagging reduces variance)\n",
    "    rf_trial = RandomForestClassifierScratch(\n",
    "        n_estimators=50,\n",
    "        max_depth=10,\n",
    "        max_features='sqrt',\n",
    "        random_state=trial,\n",
    "        verbose=False\n",
    "    )\n",
    "    rf_trial.fit(X_train, y_train)\n",
    "    rf_pred = rf_trial.predict(X_test)\n",
    "    rf_accs.append(accuracy_score(y_test, rf_pred))\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE REDUCTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSingle Decision Tree:\")\n",
    "print(f\"  Mean Accuracy: {np.mean(single_tree_accs):.4f}\")\n",
    "print(f\"  Std Dev:       {np.std(single_tree_accs):.4f}  ← HIGH VARIANCE\")\n",
    "print(f\"  Min:           {np.min(single_tree_accs):.4f}\")\n",
    "print(f\"  Max:           {np.max(single_tree_accs):.4f}\")\n",
    "\n",
    "print(f\"\\nRandom Forest (50 trees):\")\n",
    "print(f\"  Mean Accuracy: {np.mean(rf_accs):.4f}\")\n",
    "print(f\"  Std Dev:       {np.std(rf_accs):.4f}  ← REDUCED VARIANCE\")\n",
    "print(f\"  Min:           {np.min(rf_accs):.4f}\")\n",
    "print(f\"  Max:           {np.max(rf_accs):.4f}\")\n",
    "\n",
    "variance_reduction = (np.std(single_tree_accs) - np.std(rf_accs)) / np.std(single_tree_accs) * 100\n",
    "print(f\"\\nVariance Reduction: {variance_reduction:.1f}%\")\n",
    "print(\"\\n✓ Bagging reduces variance → more stable predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variance reduction\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "positions = [1, 2]\n",
    "data = [single_tree_accs, rf_accs]\n",
    "labels = ['Single Tree\\n(High Variance)', 'Random Forest\\n(Low Variance)']\n",
    "\n",
    "bp = ax.boxplot(data, positions=positions, labels=labels, widths=0.6,\n",
    "                patch_artist=True, showmeans=True)\n",
    "\n",
    "# Color boxes\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Variance Reduction through Bagging', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Box plot shows Random Forest has tighter distribution (lower variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECT OF NUMBER OF TREES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_trees_list = [1, 5, 10, 25, 50, 100, 200]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "oob_scores = []\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    print(f\"\\nTraining with {n_trees} trees...\")\n",
    "    \n",
    "    rf_temp = RandomForestClassifierScratch(\n",
    "        n_estimators=n_trees,\n",
    "        max_depth=10,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    rf_temp.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred = rf_temp.predict(X_train)\n",
    "    test_pred = rf_temp.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    oob_scores.append(rf_temp.oob_score_)\n",
    "    \n",
    "    print(f\"  Train: {train_acc:.4f} | Test: {test_acc:.4f} | OOB: {rf_temp.oob_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot effect of number of trees\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(n_trees_list, train_accs, marker='o', linewidth=2, label='Train Accuracy')\n",
    "ax.plot(n_trees_list, test_accs, marker='s', linewidth=2, label='Test Accuracy')\n",
    "ax.plot(n_trees_list, oob_scores, marker='^', linewidth=2, label='OOB Score')\n",
    "\n",
    "ax.set_xlabel('Number of Trees', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Performance vs Number of Trees', fontsize=14, fontweight='bold')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Performance improves and stabilizes with more trees\")\n",
    "print(\"✓ OOB score tracks test accuracy closely (free validation!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of max_features (Feature Randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECT OF FEATURE RANDOMNESS (max_features)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "max_features_options = [\n",
    "    1,\n",
    "    int(np.log2(n_features)),\n",
    "    int(np.sqrt(n_features)),\n",
    "    n_features // 2,\n",
    "    n_features\n",
    "]\n",
    "\n",
    "results_features = []\n",
    "\n",
    "for max_feat in max_features_options:\n",
    "    print(f\"\\nTesting max_features={max_feat}/{n_features}...\")\n",
    "    \n",
    "    rf_feat = RandomForestClassifierScratch(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        max_features=max_feat,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    rf_feat.fit(X_train, y_train)\n",
    "    test_pred = rf_feat.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    results_features.append({\n",
    "        'max_features': max_feat,\n",
    "        'label': f'{max_feat}/{n_features}',\n",
    "        'accuracy': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHT: Feature randomness decorrelates trees!\")\n",
    "print(\"  - Too few features: Trees too random, high bias\")\n",
    "print(\"  - Too many features: Trees correlated, high variance\")\n",
    "print(\"  - sqrt(n_features): Good balance (typical choice)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap vs No Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BAGGING vs NO BAGGING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# With bootstrap (standard Random Forest)\n",
    "print(\"\\n[1] WITH Bootstrap (Bagging):\")\n",
    "rf_bootstrap = RandomForestClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,  # ← Bagging ON\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "rf_bootstrap.fit(X_train, y_train)\n",
    "test_pred_boot = rf_bootstrap.predict(X_test)\n",
    "acc_boot = accuracy_score(y_test, test_pred_boot)\n",
    "print(f\"  Test Accuracy: {acc_boot:.4f}\")\n",
    "\n",
    "# Without bootstrap (just feature randomness)\n",
    "print(\"\\n[2] WITHOUT Bootstrap (No Bagging):\")\n",
    "rf_no_bootstrap = RandomForestClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=False,  # ← Bagging OFF\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "rf_no_bootstrap.fit(X_train, y_train)\n",
    "test_pred_noboot = rf_no_bootstrap.predict(X_test)\n",
    "acc_noboot = accuracy_score(y_test, test_pred_noboot)\n",
    "print(f\"  Test Accuracy: {acc_noboot:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIFFERENCE:\")\n",
    "print(f\"  With Bagging:    {acc_boot:.4f}\")\n",
    "print(f\"  Without Bagging: {acc_noboot:.4f}\")\n",
    "print(f\"  Improvement:     {(acc_boot - acc_noboot):.4f}\")\n",
    "print(\"\\n✓ Bootstrap sampling creates diversity → better ensemble!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(\"\\nBreast Cancer Dataset:\")\n",
    "print(f\"  Training: {len(X_train_c)} samples\")\n",
    "print(f\"  Test: {len(X_test_c)} samples\")\n",
    "print(f\"  Features: {X_cancer.shape[1]}\")\n",
    "print(f\"  Classes: Malignant (0), Benign (1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING ON BREAST CANCER DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_cancer = RandomForestClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rf_cancer.fit(X_train_c, y_train_c)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_c = rf_cancer.predict(X_test_c)\n",
    "acc_c = accuracy_score(y_test_c, y_pred_c)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BREAST CANCER RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTest Accuracy: {acc_c:.4f}\")\n",
    "print(f\"OOB Score: {rf_cancer.oob_score_:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_c, y_pred_c, \n",
    "                          target_names=['Malignant', 'Benign']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_c, y_pred_c)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### **What is Bagging?**\n",
    "\n",
    "**Bootstrap Aggregating (Bagging):**\n",
    "1. Create $B$ bootstrap samples (sampling with replacement)\n",
    "2. Train a model on each sample\n",
    "3. Aggregate predictions (voting for classification, averaging for regression)\n",
    "\n",
    "### **Why Bagging Works:**\n",
    "\n",
    "**Variance Reduction:**\n",
    "- Individual trees: High variance (overfit to specific samples)\n",
    "- Bagged ensemble: Lower variance (averaged predictions are stable)\n",
    "- Mathematical: $\\text{Var}(\\text{avg}) = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "**Key Properties:**\n",
    "- Each bootstrap sample: ~63.2% unique samples\n",
    "- OOB samples: ~36.8% not in sample → free validation!\n",
    "- Works best with high-variance, low-bias models (deep trees)\n",
    "\n",
    "### **Random Forest = Bagging + Extra Randomness:**\n",
    "\n",
    "**Enhancements:**\n",
    "1. **Bootstrap sampling** (from bagging)\n",
    "2. **Random feature subsets** at each split (NEW!)\n",
    "   - Typically $m = \\sqrt{p}$ for classification\n",
    "   - Further decorrelates trees\n",
    "   - Prevents strong features from dominating\n",
    "\n",
    "### **Advantages:**\n",
    "- **Reduces variance** without increasing bias\n",
    "- **Parallel training** (trees are independent)\n",
    "- **Robust to outliers** (averaging smooths them out)\n",
    "- **No overfitting** (more trees ≠ overfitting)\n",
    "- **OOB error** (free validation estimate)\n",
    "- **Feature importance** (built-in)\n",
    "\n",
    "### **Hyperparameters:**\n",
    "\n",
    "| Parameter | Effect | Typical Value |\n",
    "|-----------|--------|---------------|\n",
    "| `n_estimators` | More trees → lower variance | 100-500 |\n",
    "| `max_features` | Fewer → more decorrelation | sqrt(p) |\n",
    "| `max_depth` | Deeper → lower bias, higher variance | 10-30 |\n",
    "| `min_samples_split` | Higher → regularization | 2-10 |\n",
    "| `bootstrap` | True → bagging, False → no diversity | True |\n",
    "\n",
    "### **When to Use Random Forest:**\n",
    "- Default choice for tabular data\n",
    "- Need robust, low-maintenance model\n",
    "- Feature importance needed\n",
    "- Have mixed feature types\n",
    "- Want parallel training\n",
    "\n",
    "### **Comparison:**\n",
    "\n",
    "| Aspect | Single Tree | Random Forest |\n",
    "|--------|-------------|---------------|\n",
    "| **Variance** | High | Low |\n",
    "| **Overfitting** | Prone | Resistant |\n",
    "| **Training Time** | Fast | Slower (but parallel) |\n",
    "| **Interpretability** | High | Lower |\n",
    "| **Accuracy** | Good | Better |\n",
    "| **Stability** | Low | High |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
