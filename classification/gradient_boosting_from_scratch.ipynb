{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting from Scratch - Binary Classification\n",
    "\n",
    "Bare bones implementation of Gradient Boosting for binary classification.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Sequential ensemble learning\n",
    "- Fitting trees to residuals (gradients)\n",
    "- Additive model building\n",
    "- Learning rate (shrinkage)\n",
    "- Decision tree as weak learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING FROM SCRATCH - BINARY CLASSIFICATION\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "**Gradient Boosting for Binary Classification:**\n",
    "\n",
    "**1. Model:**\n",
    "$$F_M(x) = F_0(x) + \\sum_{m=1}^{M} \\nu \\cdot h_m(x)$$\n",
    "\n",
    "where:\n",
    "- $F_M(x)$ = final model (log-odds)\n",
    "- $F_0(x)$ = initial prediction (constant)\n",
    "- $h_m(x)$ = weak learner at iteration $m$ (decision tree)\n",
    "- $\\nu$ = learning rate (shrinkage)\n",
    "- $M$ = number of boosting iterations\n",
    "\n",
    "**2. Loss Function (Log Loss / Cross-Entropy):**\n",
    "$$L(y, F(x)) = -[y \\log(p) + (1-y) \\log(1-p)]$$\n",
    "\n",
    "where $p = \\frac{1}{1 + e^{-F(x)}}$ (sigmoid)\n",
    "\n",
    "**3. Gradient (Negative Gradient = Residuals):**\n",
    "$$r_{im} = -\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} = y_i - p_i$$\n",
    "\n",
    "**4. Algorithm:**\n",
    "1. Initialize: $F_0(x) = \\log\\left(\\frac{\\bar{p}}{1-\\bar{p}}\\right)$ (log-odds of mean)\n",
    "2. For $m = 1$ to $M$:\n",
    "   - Compute pseudo-residuals: $r_i = y_i - p_i$ where $p_i = \\sigma(F_{m-1}(x_i))$\n",
    "   - Fit weak learner $h_m(x)$ to residuals $r_i$\n",
    "   - Update: $F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$\n",
    "3. Output: $\\hat{p}(x) = \\sigma(F_M(x))$\n",
    "\n",
    "**5. Prediction:**\n",
    "$$\\hat{y} = \\begin{cases} \n",
    "1 & \\text{if } \\sigma(F_M(x)) \\geq 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingClassifierScratch:\n",
    "    \"\"\"\n",
    "    Gradient Boosting for binary classification implemented from scratch.\n",
    "    \n",
    "    Uses decision trees as weak learners and fits them sequentially\n",
    "    to negative gradients (residuals) of the log loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators : int, default=100\n",
    "        Number of boosting iterations (trees)\n",
    "    learning_rate : float, default=0.1\n",
    "        Shrinkage parameter (0 < lr <= 1)\n",
    "    max_depth : int, default=3\n",
    "        Maximum depth of individual trees\n",
    "    min_samples_split : int, default=2\n",
    "        Minimum samples required to split a node\n",
    "    subsample : float, default=1.0\n",
    "        Fraction of samples to use for fitting each tree (stochastic GB)\n",
    "    verbose : bool, default=False\n",
    "        Whether to print progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                 min_samples_split=2, subsample=1.0, verbose=False):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.subsample = subsample\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Model components\n",
    "        self.trees = []\n",
    "        self.init_pred = None  # F_0(x)\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.train_scores = []\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function (logistic function).\n",
    "        Converts log-odds to probabilities.\n",
    "        \"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _log_loss(self, y_true, y_pred_proba):\n",
    "        \"\"\"\n",
    "        Binary cross-entropy loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred_proba) + \n",
    "                       (1 - y_true) * np.log(1 - y_pred_proba))\n",
    "    \n",
    "    def _compute_residuals(self, y_true, y_pred_proba):\n",
    "        \"\"\"\n",
    "        Compute negative gradient (pseudo-residuals).\n",
    "        \n",
    "        For log loss: -∂L/∂F = y - p\n",
    "        \"\"\"\n",
    "        return y_true - y_pred_proba\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit gradient boosting classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Binary target values (0 or 1)\n",
    "        \"\"\"\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Step 1: Initialize with constant (log-odds of mean)\n",
    "        mean_y = np.mean(y)\n",
    "        # Avoid log(0) or log(inf)\n",
    "        mean_y = np.clip(mean_y, 1e-15, 1 - 1e-15)\n",
    "        self.init_pred = np.log(mean_y / (1 - mean_y))\n",
    "        \n",
    "        # Initialize predictions (log-odds)\n",
    "        F = np.full(n_samples, self.init_pred)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Initial prediction (log-odds): {self.init_pred:.4f}\")\n",
    "            print(f\"Initial probability: {self._sigmoid(self.init_pred):.4f}\")\n",
    "            print(\"\\nStarting boosting iterations...\\n\")\n",
    "        \n",
    "        # Step 2: Boosting iterations\n",
    "        for m in range(self.n_estimators):\n",
    "            # Convert log-odds to probabilities\n",
    "            probabilities = self._sigmoid(F)\n",
    "            \n",
    "            # Compute pseudo-residuals (negative gradient)\n",
    "            residuals = self._compute_residuals(y, probabilities)\n",
    "            \n",
    "            # Subsample if needed (Stochastic Gradient Boosting)\n",
    "            if self.subsample < 1.0:\n",
    "                sample_indices = np.random.choice(\n",
    "                    n_samples, \n",
    "                    size=int(n_samples * self.subsample),\n",
    "                    replace=False\n",
    "                )\n",
    "                X_sample = X[sample_indices]\n",
    "                residuals_sample = residuals[sample_indices]\n",
    "            else:\n",
    "                X_sample = X\n",
    "                residuals_sample = residuals\n",
    "            \n",
    "            # Fit a regression tree to residuals\n",
    "            tree = DecisionTreeRegressor(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                random_state=m\n",
    "            )\n",
    "            tree.fit(X_sample, residuals_sample)\n",
    "            \n",
    "            # Update predictions (all samples)\n",
    "            update = self.learning_rate * tree.predict(X)\n",
    "            F += update\n",
    "            \n",
    "            # Store tree\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Track training performance\n",
    "            train_proba = self._sigmoid(F)\n",
    "            train_loss = self._log_loss(y, train_proba)\n",
    "            train_acc = np.mean((train_proba >= 0.5).astype(int) == y)\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_scores.append(train_acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if self.verbose and (m % 10 == 0 or m == self.n_estimators - 1):\n",
    "                print(f\"Iteration {m+1:3d} | Loss: {train_loss:.6f} | \"\n",
    "                      f\"Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _predict_raw(self, X):\n",
    "        \"\"\"\n",
    "        Predict raw values (log-odds / margins).\n",
    "        F(x) = F_0 + learning_rate * sum(trees)\n",
    "        \"\"\"\n",
    "        # Start with initial prediction\n",
    "        F = np.full(len(X), self.init_pred)\n",
    "        \n",
    "        # Add predictions from all trees\n",
    "        for tree in self.trees:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        proba : array, shape (n_samples,)\n",
    "            Probability of positive class\n",
    "        \"\"\"\n",
    "        raw_predictions = self._predict_raw(X)\n",
    "        return self._sigmoid(raw_predictions)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        threshold : float, default=0.5\n",
    "            Decision threshold\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    def get_feature_importance(self, X):\n",
    "        \"\"\"\n",
    "        Calculate feature importance as average across all trees.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        importance = np.zeros(n_features)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            importance += tree.feature_importances_\n",
    "        \n",
    "        # Normalize\n",
    "        importance /= len(self.trees)\n",
    "        return importance\n",
    "\n",
    "print(\"\\n✓ GradientBoostingClassifierScratch class defined\")\n",
    "print(\"  - Sequential tree building\")\n",
    "print(\"  - Fits trees to residuals\")\n",
    "print(\"  - Learning rate for shrinkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X_simple, y_simple = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    "    flip_y=0.1  # Add 10% label noise\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.2, random_state=42, stratify=y_simple\n",
    ")\n",
    "\n",
    "print(\"Synthetic Dataset:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Classes: {len(np.unique(y_simple))}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Class 0: {np.sum(y_train == 0)} samples\")\n",
    "print(f\"  Class 1: {np.sum(y_train == 1)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train gradient boosting\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING GRADIENT BOOSTING CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gb_model = GradientBoostingClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    subsample=1.0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_proba = gb_model.predict_proba(X_test)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"ROC-AUC:  {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(gb_model.train_losses, linewidth=2)\n",
    "axes[0].set_xlabel('Boosting Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Log Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Iterations', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot(gb_model.train_scores, linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Boosting Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training Accuracy Over Iterations', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Loss decreases and accuracy increases with more trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {test_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = gb_model.get_feature_importance(X_train)\n",
    "feature_names = [f'Feature {i}' for i in range(len(feature_importance))]\n",
    "\n",
    "# Sort by importance\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print(\"\\nFeature Importance (sorted):\")\n",
    "print(f\"{'Rank':<6} {'Feature':<12} {'Importance':<12}\")\n",
    "print(\"-\" * 35)\n",
    "for i, idx in enumerate(indices, 1):\n",
    "    print(f\"{i:<6} {feature_names[idx]:<12} {feature_importance[idx]:.6f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(feature_importance)), feature_importance[indices])\n",
    "plt.yticks(range(len(feature_importance)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EFFECT OF LEARNING RATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.5]\n",
    "lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with learning_rate={lr}...\")\n",
    "    \n",
    "    model = GradientBoostingClassifierScratch(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    lr_results[lr] = {\n",
    "        'model': model,\n",
    "        'accuracy': test_acc,\n",
    "        'auc': test_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rate comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (lr, results) in enumerate(lr_results.items()):\n",
    "    model = results['model']\n",
    "    \n",
    "    axes[idx].plot(model.train_losses, linewidth=2)\n",
    "    axes[idx].set_xlabel('Boosting Iteration', fontsize=11)\n",
    "    axes[idx].set_ylabel('Log Loss', fontsize=11)\n",
    "    axes[idx].set_title(f'Learning Rate: {lr} (Test Acc: {results[\"accuracy\"]:.4f})',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of Learning Rate on Convergence', \n",
    "            fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Lower learning rate → smoother convergence, may need more trees\")\n",
    "print(\"✓ Higher learning rate → faster initial progress, risk of overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EFFECT OF NUMBER OF TREES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train model with many trees\n",
    "model_many_trees = GradientBoostingClassifierScratch(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_many_trees.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate at different stages\n",
    "n_trees_to_test = [10, 25, 50, 100, 150, 200]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for n_trees in n_trees_to_test:\n",
    "    # Temporarily limit trees\n",
    "    original_trees = model_many_trees.trees\n",
    "    model_many_trees.trees = original_trees[:n_trees]\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = model_many_trees.predict(X_train)\n",
    "    test_pred = model_many_trees.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Trees: {n_trees:3d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Restore all trees\n",
    "    model_many_trees.trees = original_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train vs test performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_trees_to_test, train_accs, marker='o', linewidth=2, \n",
    "         markersize=8, label='Training Accuracy')\n",
    "plt.plot(n_trees_to_test, test_accs, marker='s', linewidth=2, \n",
    "         markersize=8, label='Test Accuracy')\n",
    "plt.xlabel('Number of Trees', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Performance vs Number of Trees', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Monitor train vs test to detect overfitting\")\n",
    "print(\"✓ If test performance plateaus/degrades, stop adding trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "\n",
    "# Split data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(\"\\nBreast Cancer Dataset:\")\n",
    "print(f\"  Training samples: {len(X_train_c)}\")\n",
    "print(f\"  Test samples: {len(X_test_c)}\")\n",
    "print(f\"  Features: {X_train_c.shape[1]}\")\n",
    "print(f\"  Classes: Malignant (0), Benign (1)\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(f\"  Malignant: {np.sum(y_train_c == 0)}\")\n",
    "print(f\"  Benign: {np.sum(y_train_c == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train gradient boosting\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING ON BREAST CANCER DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gb_cancer = GradientBoostingClassifierScratch(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "gb_cancer.fit(X_train_c, y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred_c = gb_cancer.predict(X_test_c)\n",
    "y_pred_proba_c = gb_cancer.predict_proba(X_test_c)\n",
    "\n",
    "test_acc_c = accuracy_score(y_test_c, y_pred_c)\n",
    "test_auc_c = roc_auc_score(y_test_c, y_pred_proba_c)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BREAST CANCER TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy: {test_acc_c:.4f}\")\n",
    "print(f\"ROC-AUC:  {test_auc_c:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_c, y_pred_c, \n",
    "                          target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features\n",
    "feature_importance_c = gb_cancer.get_feature_importance(X_train_c)\n",
    "top_k = 10\n",
    "top_indices = np.argsort(feature_importance_c)[::-1][:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} Most Important Features:\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{i:2d}. {cancer.feature_names[idx]:<30s} {feature_importance_c[idx]:.6f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(top_k), feature_importance_c[top_indices])\n",
    "plt.yticks(range(top_k), [cancer.feature_names[i] for i in top_indices])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title(f'Top {top_k} Feature Importances - Breast Cancer', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Gradient Boosting - Key Concepts:**\n",
    "\n",
    "1. **Sequential Learning:**\n",
    "   - Trees added one at a time\n",
    "   - Each new tree corrects errors of previous ensemble\n",
    "   - Cannot be parallelized across trees\n",
    "\n",
    "2. **Gradient Descent in Function Space:**\n",
    "   - Instead of optimizing parameters, optimize predictions\n",
    "   - Negative gradient = residuals (for squared loss) or pseudo-residuals (for other losses)\n",
    "   - New tree approximates negative gradient\n",
    "\n",
    "3. **Loss Function:**\n",
    "   - Binary classification: Log loss (cross-entropy)\n",
    "   - Regression: MSE, MAE, Huber\n",
    "   - Ranking: Pairwise loss\n",
    "\n",
    "4. **Learning Rate (Shrinkage):**\n",
    "   - Scale contribution of each tree\n",
    "   - Smaller → better generalization, need more trees\n",
    "   - Larger → faster convergence, risk overfitting\n",
    "   - Typical values: 0.01 - 0.3\n",
    "\n",
    "5. **Hyperparameters:**\n",
    "   - `n_estimators`: Number of trees\n",
    "   - `learning_rate`: Shrinkage factor\n",
    "   - `max_depth`: Tree complexity (usually 3-8)\n",
    "   - `min_samples_split`: Minimum samples to split\n",
    "   - `subsample`: Fraction for stochastic GB\n",
    "\n",
    "6. **Advantages:**\n",
    "   - High predictive accuracy\n",
    "   - Handles mixed data types\n",
    "   - Robust to outliers (with appropriate loss)\n",
    "   - Feature importance built-in\n",
    "   - Flexible (custom loss functions)\n",
    "\n",
    "7. **Disadvantages:**\n",
    "   - Sequential (hard to parallelize)\n",
    "   - Sensitive to overfitting\n",
    "   - Requires careful tuning\n",
    "   - Slower training than Random Forest\n",
    "\n",
    "8. **Best Practices:**\n",
    "   - Start with small learning rate (0.01-0.1)\n",
    "   - Use cross-validation for tuning\n",
    "   - Monitor train vs validation performance\n",
    "   - Consider early stopping\n",
    "   - Use subsample < 1.0 for regularization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
