{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree from Scratch - Regression\n",
    "\n",
    "Barebones implementation of decision tree regressor using CART algorithm.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Mean Squared Error (MSE) / Mean Absolute Error (MAE)\n",
    "- Recursive tree building\n",
    "- Split selection\n",
    "- Prediction via tree traversal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node in the decision tree.\n",
    "    \n",
    "    Can be either:\n",
    "    - Leaf node: has a value (predicted target value)\n",
    "    - Internal node: has feature, threshold, and left/right children\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature      # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left           # Left child node\n",
    "        self.right = right         # Right child node\n",
    "        self.value = value         # Predicted value (for leaf nodes)\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor:\n",
    "    \"\"\"\n",
    "    Decision Tree Regressor from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_depth : int, default=10\n",
    "        Maximum depth of the tree\n",
    "    min_samples_split : int, default=2\n",
    "        Minimum samples required to split a node\n",
    "    criterion : str, default='mse'\n",
    "        Split criterion: 'mse' (mean squared error) or 'mae' (mean absolute error)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=10, min_samples_split=2, criterion='mse'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build decision tree from training data.\n",
    "        \"\"\"\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively grow the decision tree.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or\n",
    "            self._all_same_values(y)):\n",
    "            leaf_value = self._leaf_value(y)\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        # Split data\n",
    "        left_idxs = X[:, best_feature] <= best_threshold\n",
    "        right_idxs = X[:, best_feature] > best_threshold\n",
    "        \n",
    "        # Recursively grow children\n",
    "        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split on.\n",
    "        \"\"\"\n",
    "        best_gain = -float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            X_column = X[:, feature_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Calculate variance reduction (information gain for regression)\n",
    "                gain = self._variance_reduction(y, X_column, threshold)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _variance_reduction(self, y, X_column, threshold):\n",
    "        \"\"\"\n",
    "        Calculate variance reduction (information gain) from a split.\n",
    "        \"\"\"\n",
    "        # Parent error\n",
    "        parent_error = self._calculate_error(y)\n",
    "        \n",
    "        # Split\n",
    "        left_idxs = X_column <= threshold\n",
    "        right_idxs = X_column > threshold\n",
    "        \n",
    "        if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Weighted average of children error\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_idxs]), len(y[right_idxs])\n",
    "        error_left = self._calculate_error(y[left_idxs])\n",
    "        error_right = self._calculate_error(y[right_idxs])\n",
    "        child_error = (n_left / n) * error_left + (n_right / n) * error_right\n",
    "        \n",
    "        # Variance reduction (information gain)\n",
    "        variance_reduction = parent_error - child_error\n",
    "        return variance_reduction\n",
    "    \n",
    "    def _calculate_error(self, y):\n",
    "        \"\"\"\n",
    "        Calculate error (MSE or MAE).\n",
    "        \"\"\"\n",
    "        if self.criterion == 'mse':\n",
    "            return self._mse(y)\n",
    "        else:\n",
    "            return self._mae(y)\n",
    "    \n",
    "    def _mse(self, y):\n",
    "        \"\"\"\n",
    "        Calculate Mean Squared Error.\n",
    "        MSE = (1/n) * Σ(y_i - ȳ)^2\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        mean = np.mean(y)\n",
    "        return np.mean((y - mean) ** 2)\n",
    "    \n",
    "    def _mae(self, y):\n",
    "        \"\"\"\n",
    "        Calculate Mean Absolute Error.\n",
    "        MAE = (1/n) * Σ|y_i - median(y)|\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        median = np.median(y)\n",
    "        return np.mean(np.abs(y - median))\n",
    "    \n",
    "    def _leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the value to predict at a leaf node.\n",
    "        - For MSE: mean\n",
    "        - For MAE: median\n",
    "        \"\"\"\n",
    "        if self.criterion == 'mse':\n",
    "            return np.mean(y)\n",
    "        else:\n",
    "            return np.median(y)\n",
    "    \n",
    "    def _all_same_values(self, y):\n",
    "        \"\"\"\n",
    "        Check if all values in y are the same.\n",
    "        \"\"\"\n",
    "        return len(np.unique(y)) == 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverse tree to predict single sample.\n",
    "        \"\"\"\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    def print_tree(self, node=None, depth=0):\n",
    "        \"\"\"\n",
    "        Print tree structure.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        if node.is_leaf_node():\n",
    "            print(\"  \" * depth + f\"Leaf: {node.value:.2f}\")\n",
    "        else:\n",
    "            print(\"  \" * depth + f\"Feature {node.feature} <= {node.threshold:.2f}\")\n",
    "            print(\"  \" * depth + \"Left:\")\n",
    "            self.print_tree(node.left, depth + 1)\n",
    "            print(\"  \" * depth + \"Right:\")\n",
    "            self.print_tree(node.right, depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Diabetes Dataset\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision tree with MSE\n",
    "dt_mse = DecisionTreeRegressor(max_depth=5, criterion='mse')\n",
    "dt_mse.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_mse = dt_mse.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test, y_pred_mse)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_mse)\n",
    "r2 = r2_score(y_test, y_pred_mse)\n",
    "\n",
    "print(f\"\\nDecision Tree (MSE) Results:\")\n",
    "print(f\"  MSE:  {mse:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAE:  {mae:.2f}\")\n",
    "print(f\"  R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision tree with MAE\n",
    "dt_mae = DecisionTreeRegressor(max_depth=5, criterion='mae')\n",
    "dt_mae.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_mae = dt_mae.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test, y_pred_mae)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_mae)\n",
    "r2 = r2_score(y_test, y_pred_mae)\n",
    "\n",
    "print(f\"\\nDecision Tree (MAE) Results:\")\n",
    "print(f\"  MSE:  {mse:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAE:  {mae:.2f}\")\n",
    "print(f\"  R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tree structure (small depth for readability)\n",
    "dt_small = DecisionTreeRegressor(max_depth=3, criterion='mse')\n",
    "dt_small.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nTree Structure (max_depth=3):\")\n",
    "dt_small.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X_syn, y_syn = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(\n",
    "    X_syn, y_syn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Synthetic Dataset\")\n",
    "print(f\"Training samples: {len(X_train_syn)}\")\n",
    "print(f\"Test samples: {len(X_test_syn)}\")\n",
    "print(f\"Features: {X_syn.shape[1]}\")\n",
    "print(f\"Target range: [{y_syn.min():.2f}, {y_syn.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "dt_syn = DecisionTreeRegressor(max_depth=10, min_samples_split=5, criterion='mse')\n",
    "dt_syn.fit(X_train_syn, y_train_syn)\n",
    "\n",
    "y_pred_syn = dt_syn.predict(X_test_syn)\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_test_syn, y_pred_syn)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_syn, y_pred_syn)\n",
    "r2 = r2_score(y_test_syn, y_pred_syn)\n",
    "\n",
    "print(f\"\\nDecision Tree Results:\")\n",
    "print(f\"  MSE:  {mse:.2f}\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  MAE:  {mae:.2f}\")\n",
    "print(f\"  R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Max Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different max_depth values\n",
    "depths = [2, 3, 5, 10, 15]\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt = DecisionTreeRegressor(max_depth=depth, criterion='mse')\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = dt.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    results.append({\n",
    "        'max_depth': depth,\n",
    "        'R²': r2,\n",
    "        'RMSE': rmse\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nPerformance vs Max Depth:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Summary\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n",
    "\n",
    "where $\\bar{y}$ is the mean of target values\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\text{median}(y)|$$\n",
    "\n",
    "**Variance Reduction (Information Gain for Regression):**\n",
    "$$VR = Error_{parent} - \\sum_{children} \\frac{N_{child}}{N_{parent}} \\times Error_{child}$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. For each feature and threshold, calculate variance reduction\n",
    "2. Choose split that maximizes variance reduction\n",
    "3. Recursively split until stopping criteria\n",
    "4. Assign mean (MSE) or median (MAE) at leaf nodes\n",
    "\n",
    "**Key Differences from Classification Trees:**\n",
    "- Leaf nodes predict continuous values (mean/median) instead of class labels\n",
    "- Split criterion uses MSE/MAE instead of Gini/Entropy\n",
    "- Evaluation uses regression metrics (R², RMSE, MAE) instead of accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
