{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA (Low-Rank Adaptation) Implementation from Scratch\n",
    "\n",
    "## Educational Notebook - Minimal Resource Requirements\n",
    "\n",
    "This notebook implements LoRA from first principles using the smallest possible model (GPT-2 small - 124M parameters).\n",
    "\n",
    "**Resource Requirements:**\n",
    "- RAM: 4GB minimum\n",
    "- GPU: Optional (works on CPU)\n",
    "- Storage: ~500MB\n",
    "\n",
    "**What is LoRA?**\n",
    "\n",
    "LoRA freezes the pre-trained model weights and injects trainable low-rank decomposition matrices into each layer. Instead of fine-tuning all parameters, we only train small matrices that are added to the original weights.\n",
    "\n",
    "**Key Idea:**\n",
    "```\n",
    "Original: W ∈ R^(d×k)\n",
    "LoRA: W' = W + BA where B ∈ R^(d×r), A ∈ R^(r×k), r << min(d,k)\n",
    "```\n",
    "\n",
    "This reduces trainable parameters from d×k to (d+k)×r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Optional, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LoRA Layer Implementation\n",
    "\n",
    "We'll implement LoRA as a wrapper around existing Linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA implementation for a linear layer.\n",
    "    \n",
    "    Args:\n",
    "        in_features: Input dimension\n",
    "        out_features: Output dimension  \n",
    "        rank: Rank of the low-rank decomposition (r)\n",
    "        alpha: Scaling factor (typically set to rank)\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA matrices: W' = W + BA\n",
    "        # B: (out_features, rank) - initialized to zeros\n",
    "        # A: (rank, in_features) - initialized with Kaiming uniform\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize A with Kaiming uniform (same as nn.Linear)\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        # B is initialized to zero, so initially W' = W (no change)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: x @ (W + scaling * B @ A)^T\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (..., in_features)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (..., out_features)\n",
    "        \"\"\"\n",
    "        # Compute LoRA adaptation: (B @ A) with scaling\n",
    "        lora_output = (self.dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "        return lora_output\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, rank={self.rank}, alpha={self.alpha}'\n",
    "\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper that combines a frozen Linear layer with LoRA adaptation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        linear: nn.Linear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Freeze the original linear layer\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if self.linear.bias is not None:\n",
    "            self.linear.bias.requires_grad = False\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original output + LoRA adaptation\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "# Test the LoRA layer\n",
    "print(\"Testing LoRA Layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple linear layer\n",
    "linear = nn.Linear(768, 768)\n",
    "print(f\"Original Linear layer parameters: {sum(p.numel() for p in linear.parameters()):,}\")\n",
    "\n",
    "# Wrap with LoRA\n",
    "lora_linear = LinearWithLoRA(linear, rank=8, alpha=16)\n",
    "trainable_params = sum(p.numel() for p in lora_linear.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in lora_linear.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters (LoRA): {trainable_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"Parameter reduction: {total_params / trainable_params:.1f}x fewer trainable params\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 10, 768)  # (batch, seq_len, hidden_dim)\n",
    "output = lora_linear(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Apply LoRA to Pre-trained Model\n",
    "\n",
    "Now we'll apply LoRA to specific layers in a GPT-2 model. Typically, we target:\n",
    "- Query and Value projection matrices in attention\n",
    "- Sometimes Key projections\n",
    "- Optionally, feed-forward layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_model(\n",
    "    model: nn.Module,\n",
    "    target_modules: List[str] = ['c_attn'],  # GPT-2 uses 'c_attn' for QKV projection\n",
    "    rank: int = 8,\n",
    "    alpha: float = 16,\n",
    "    dropout: float = 0.1\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Apply LoRA to specified modules in the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained model\n",
    "        target_modules: Names of modules to apply LoRA to\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if this module should have LoRA applied\n",
    "        if any(target in name for target in target_modules):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Get parent module and attribute name\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                attr_name = name.split('.')[-1]\n",
    "                parent = model.get_submodule(parent_name) if parent_name else model\n",
    "                \n",
    "                # Replace with LoRA version\n",
    "                lora_layer = LinearWithLoRA(module, rank=rank, alpha=alpha, dropout=dropout)\n",
    "                setattr(parent, attr_name, lora_layer)\n",
    "                print(f\"Applied LoRA to: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> dict:\n",
    "    \"\"\"\n",
    "    Count total and trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return {\n",
    "        'total': total_params,\n",
    "        'trainable': trainable_params,\n",
    "        'frozen': total_params - trainable_params,\n",
    "        'trainable_pct': 100 * trainable_params / total_params\n",
    "    }\n",
    "\n",
    "\n",
    "# Load GPT-2 small model (smallest possible)\n",
    "print(\"Loading GPT-2 small model...\")\n",
    "model_name = \"gpt2\"  # 124M parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print original model stats\n",
    "print(\"\\nOriginal Model:\")\n",
    "print(\"=\" * 50)\n",
    "stats = count_parameters(model)\n",
    "print(f\"Total parameters: {stats['total']:,}\")\n",
    "\n",
    "# Apply LoRA to attention layers\n",
    "print(\"\\nApplying LoRA to model...\")\n",
    "print(\"=\" * 50)\n",
    "model = apply_lora_to_model(\n",
    "    model,\n",
    "    target_modules=['c_attn'],  # Apply to QKV projections in GPT-2\n",
    "    rank=8,\n",
    "    alpha=16,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Print LoRA model stats\n",
    "print(\"\\nLoRA Model:\")\n",
    "print(\"=\" * 50)\n",
    "stats = count_parameters(model)\n",
    "print(f\"Total parameters: {stats['total']:,}\")\n",
    "print(f\"Trainable parameters: {stats['trainable']:,}\")\n",
    "print(f\"Frozen parameters: {stats['frozen']:,}\")\n",
    "print(f\"Trainable percentage: {stats['trainable_pct']:.2f}%\")\n",
    "print(f\"Parameter reduction: {stats['total'] / stats['trainable']:.1f}x\")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel moved to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prepare Training Data\n",
    "\n",
    "We'll create a simple dataset for demonstration. For real applications, use larger datasets like WikiText or your custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple text dataset for causal language modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize and truncate\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.encodings.append(encoded)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[0] for key, val in self.encodings[idx].items()}\n",
    "        # For causal LM, labels are the same as input_ids\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "\n",
    "# Create a small synthetic dataset for demonstration\n",
    "# In practice, use a real dataset like WikiText, C4, or your domain-specific data\n",
    "train_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to learn complex patterns.\",\n",
    "    \"Natural language processing helps computers understand and generate human language.\",\n",
    "    \"Transformers revolutionized NLP with their attention mechanism and parallel processing.\",\n",
    "    \"Transfer learning allows models to leverage knowledge from pre-training on large datasets.\",\n",
    "    \"Fine-tuning adapts pre-trained models to specific downstream tasks efficiently.\",\n",
    "    \"LoRA reduces the number of trainable parameters during fine-tuning significantly.\",\n",
    "    \"Parameter-efficient fine-tuning methods enable training on limited computational resources.\",\n",
    "    \"Attention mechanisms allow models to focus on relevant parts of the input sequence.\",\n",
    "    \"Self-supervised learning has become crucial for training large language models.\",\n",
    "    \"The GPT architecture uses decoder-only transformers for autoregressive text generation.\",\n",
    "    \"Embedding layers convert discrete tokens into continuous vector representations.\",\n",
    "    \"Positional encodings help transformers understand the order of tokens in sequences.\",\n",
    "    \"Layer normalization stabilizes training in deep neural networks.\",\n",
    "    \"Dropout prevents overfitting by randomly disabling neurons during training.\",\n",
    "    \"The Adam optimizer adapts learning rates for each parameter independently.\",\n",
    "    \"Gradient clipping prevents exploding gradients in deep networks.\",\n",
    "    \"Batch normalization normalizes activations across mini-batches.\",\n",
    "    \"Residual connections allow gradients to flow through very deep networks.\",\n",
    "    \"Multi-head attention enables models to attend to different representation subspaces.\",\n",
    "] * 5  # Repeat to get more training examples\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = SimpleTextDataset(train_texts, tokenizer, max_length=64)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Show a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Loop\n",
    "\n",
    "Implement a simple training loop to fine-tune the model with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    weight_decay: float = 0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model with LoRA.\n",
    "    \"\"\"\n",
    "    # Only optimize LoRA parameters\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                      f\"Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}, \"\n",
    "                      f\"Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        epoch_loss = total_loss / num_batches\n",
    "        print(f\"\\nEpoch {epoch+1} completed. Average Loss: {epoch_loss:.4f}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting LoRA training...\")\n",
    "print(\"=\" * 50)\n",
    "train_lora(model, train_loader, num_epochs=3, learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Text Generation and Evaluation\n",
    "\n",
    "Test the fine-tuned model by generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 100,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.9\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test generation with different prompts\n",
    "prompts = [\n",
    "    \"Machine learning is\",\n",
    "    \"Deep learning uses\",\n",
    "    \"Transfer learning allows\",\n",
    "    \"LoRA reduces\",\n",
    "    \"Attention mechanisms\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating text with fine-tuned model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=80)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Save and Load LoRA Weights\n",
    "\n",
    "Save only the LoRA weights (not the entire model) for efficient storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora_weights(model: nn.Module, save_path: str):\n",
    "    \"\"\"\n",
    "    Save only the LoRA weights.\n",
    "    \"\"\"\n",
    "    lora_state_dict = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and param.requires_grad:\n",
    "            lora_state_dict[name] = param.cpu()\n",
    "    \n",
    "    torch.save(lora_state_dict, save_path)\n",
    "    print(f\"LoRA weights saved to: {save_path}\")\n",
    "    print(f\"Number of LoRA parameters: {len(lora_state_dict)}\")\n",
    "    \n",
    "    # Print size\n",
    "    import os\n",
    "    size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
    "    print(f\"File size: {size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "def load_lora_weights(model: nn.Module, load_path: str):\n",
    "    \"\"\"\n",
    "    Load LoRA weights into the model.\n",
    "    \"\"\"\n",
    "    lora_state_dict = torch.load(load_path, map_location=device)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(lora_state_dict, strict=False)\n",
    "    print(f\"LoRA weights loaded from: {load_path}\")\n",
    "    print(f\"Number of LoRA parameters loaded: {len(lora_state_dict)}\")\n",
    "\n",
    "\n",
    "# Save LoRA weights\n",
    "print(\"\\nSaving LoRA weights...\")\n",
    "print(\"=\" * 50)\n",
    "save_lora_weights(model, '/home/claude/lora_weights.pt')\n",
    "\n",
    "# Compare with full model size\n",
    "print(\"\\nFor comparison:\")\n",
    "print(f\"Full GPT-2 model would be ~500 MB\")\n",
    "print(f\"LoRA weights are only a small fraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Merge LoRA Weights (Optional)\n",
    "\n",
    "For inference efficiency, we can merge LoRA weights back into the original weights: W' = W + BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Merge LoRA weights into the base model for inference.\n",
    "    After merging, LoRA layers can be removed.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            # Get the parent module\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            attr_name = name.split('.')[-1]\n",
    "            parent = model.get_submodule(parent_name) if parent_name else model\n",
    "            \n",
    "            # Merge: W' = W + scaling * B @ A\n",
    "            with torch.no_grad():\n",
    "                merged_weight = module.linear.weight.clone()\n",
    "                lora_weight = (module.lora.lora_B @ module.lora.lora_A) * module.lora.scaling\n",
    "                merged_weight += lora_weight\n",
    "                \n",
    "                # Create new linear layer with merged weights\n",
    "                new_linear = nn.Linear(\n",
    "                    module.linear.in_features,\n",
    "                    module.linear.out_features,\n",
    "                    bias=module.linear.bias is not None\n",
    "                )\n",
    "                new_linear.weight.data = merged_weight\n",
    "                if module.linear.bias is not None:\n",
    "                    new_linear.bias.data = module.linear.bias.clone()\n",
    "                \n",
    "                # Replace module\n",
    "                setattr(parent, attr_name, new_linear)\n",
    "                print(f\"Merged LoRA weights for: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a copy for merging (optional)\n",
    "print(\"\\nMerging LoRA weights into base model...\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Note: This is optional and only for inference optimization.\")\n",
    "print(\"After merging, the model has no separate LoRA parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "1. **LoRA Layer**: Low-rank decomposition matrices (A and B) that adapt pre-trained weights\n",
    "2. **Model Integration**: Applied LoRA to attention layers in GPT-2\n",
    "3. **Training**: Fine-tuned only LoRA parameters (~0.3% of total parameters)\n",
    "4. **Inference**: Generated text with the adapted model\n",
    "5. **Weight Management**: Saved/loaded only LoRA weights (much smaller files)\n",
    "\n",
    "### Key Advantages of LoRA:\n",
    "\n",
    "1. **Memory Efficient**: Only train ~0.1-1% of parameters\n",
    "2. **Storage Efficient**: LoRA weights are tiny (few MBs vs GBs)\n",
    "3. **No Catastrophic Forgetting**: Base model remains frozen\n",
    "4. **Modular**: Easy to swap different LoRA adapters\n",
    "5. **Fast Training**: Fewer parameters = faster convergence\n",
    "\n",
    "### Hyperparameters to Tune:\n",
    "\n",
    "- **Rank (r)**: 8-64 (lower = fewer params, higher = more capacity)\n",
    "- **Alpha**: Usually 2×rank or 1×rank\n",
    "- **Target Modules**: Which layers to apply LoRA to\n",
    "- **Dropout**: Regularization (0.05-0.1)\n",
    "- **Learning Rate**: 1e-4 to 5e-4 for LoRA\n",
    "\n",
    "### Extensions:\n",
    "\n",
    "1. Apply LoRA to more layers (feed-forward networks)\n",
    "2. Use QLoRA (quantized LoRA) for even lower memory\n",
    "3. Implement multi-task LoRA (multiple adapters)\n",
    "4. Add evaluation metrics (perplexity, downstream tasks)\n",
    "5. Use larger models (Llama, Mistral) with same approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Deep Dive\n",
    "\n",
    "### LoRA Formulation:\n",
    "\n",
    "For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$:\n",
    "\n",
    "$$h = W_0 x + \\Delta W x = W_0 x + BAx$$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$: Down-projection matrix\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$: Up-projection matrix  \n",
    "- $r \\ll \\min(d, k)$: Rank (bottleneck dimension)\n",
    "\n",
    "### Parameter Count:\n",
    "\n",
    "- Original: $d \\times k$ parameters\n",
    "- LoRA: $(d + k) \\times r$ parameters\n",
    "- Reduction factor: $\\frac{d \\times k}{(d+k) \\times r}$\n",
    "\n",
    "Example: For $d=k=768$, $r=8$:\n",
    "- Original: 589,824 parameters\n",
    "- LoRA: 12,288 parameters (48× reduction!)\n",
    "\n",
    "### Scaling Factor:\n",
    "\n",
    "$$\\Delta W = \\frac{\\alpha}{r} BA$$\n",
    "\n",
    "The scaling factor $\\frac{\\alpha}{r}$ helps:\n",
    "- Stabilize training across different ranks\n",
    "- Control the magnitude of updates\n",
    "- Typically set $\\alpha = r$ or $\\alpha = 2r$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
