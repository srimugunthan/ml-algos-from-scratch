{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch - Gradient Descent\n",
    "\n",
    "Implementation of linear regression using gradient descent with L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Mean Squared Error (MSE) loss\n",
    "- Gradient descent optimization\n",
    "- L1 regularization (Lasso)\n",
    "- L2 regularization (Ridge)\n",
    "- Feature scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LINEAR REGRESSION FROM SCRATCH\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "**Linear Regression Hypothesis:**\n",
    "$$h(x) = w^T x + b$$\n",
    "\n",
    "where $w$ are weights, $b$ is bias, $x$ is input\n",
    "\n",
    "**Mean Squared Error Loss:**\n",
    "$$L(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**L2 Regularization (Ridge):**\n",
    "$$L_{Ridge} = L_{MSE} + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "**L1 Regularization (Lasso):**\n",
    "$$L_{Lasso} = L_{MSE} + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |w_j|$$\n",
    "\n",
    "**Gradient Descent Update:**\n",
    "$$w := w - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "where:\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X^T (h(x) - y)$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum(h(x) - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from scratch using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Step size for gradient descent updates\n",
    "    n_iterations : int, default=1000\n",
    "        Number of gradient descent iterations\n",
    "    regularization : str, default=None\n",
    "        Type of regularization: None, 'l1' (Lasso), or 'l2' (Ridge)\n",
    "    lambda_reg : float, default=0.01\n",
    "        Regularization strength\n",
    "    verbose : bool, default=False\n",
    "        Whether to print training progress\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, \n",
    "                 regularization=None, lambda_reg=0.01, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Model parameters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Forward pass: compute predictions\n",
    "            y_pred = self._predict(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self._compute_gradients(X, y, y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Print progress\n",
    "            if self.verbose and (iteration % 100 == 0 or iteration == self.n_iterations - 1):\n",
    "                print(f\"Iteration {iteration:4d} | Loss: {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using current parameters.\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights) + self.bias\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error loss with optional regularization.\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        \n",
    "        # MSE loss\n",
    "        mse_loss = (1 / (2 * m)) * np.sum((y_pred - y_true) ** 2)\n",
    "        \n",
    "        # Add regularization term\n",
    "        reg_term = 0\n",
    "        if self.regularization == 'l2':\n",
    "            reg_term = (self.lambda_reg / (2 * m)) * np.sum(self.weights ** 2)\n",
    "        elif self.regularization == 'l1':\n",
    "            reg_term = (self.lambda_reg / m) * np.sum(np.abs(self.weights))\n",
    "        \n",
    "        return mse_loss + reg_term\n",
    "    \n",
    "    def _compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradients for weights and bias.\n",
    "        \"\"\"\n",
    "        m = len(y_true)\n",
    "        error = y_pred - y_true\n",
    "        \n",
    "        # Gradient for weights\n",
    "        dw = (1 / m) * X.T.dot(error)\n",
    "        \n",
    "        # Add regularization gradient\n",
    "        if self.regularization == 'l2':\n",
    "            dw += (self.lambda_reg / m) * self.weights\n",
    "        elif self.regularization == 'l1':\n",
    "            dw += (self.lambda_reg / m) * np.sign(self.weights)\n",
    "        \n",
    "        # Gradient for bias (no regularization on bias)\n",
    "        db = (1 / m) * np.sum(error)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X.\n",
    "        \"\"\"\n",
    "        return self._predict(X)\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Return model parameters.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'bias': self.bias,\n",
    "            'loss_history': self.loss_history\n",
    "        }\n",
    "\n",
    "print(\"\\n✓ LinearRegressionScratch class defined\")\n",
    "print(\"  - Gradient descent optimization\")\n",
    "print(\"  - MSE loss function\")\n",
    "print(\"  - Optional L1/L2 regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple 1D data for visualization\n",
    "np.random.seed(42)\n",
    "X_simple = 2 * np.random.rand(100, 1)\n",
    "y_simple = 4 + 3 * X_simple.ravel() + np.random.randn(100)\n",
    "\n",
    "print(\"Simple 1D Dataset\")\n",
    "print(f\"Samples: {len(X_simple)}\")\n",
    "print(f\"Features: {X_simple.shape[1]}\")\n",
    "print(f\"True equation: y ≈ 4 + 3x + noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model_simple = LinearRegressionScratch(\n",
    "    learning_rate=0.1,\n",
    "    n_iterations=1000,\n",
    "    regularization=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model_simple.fit(X_simple, y_simple)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_simple = model_simple.predict(X_simple)\n",
    "\n",
    "# Learned parameters\n",
    "print(f\"\\nLearned parameters:\")\n",
    "print(f\"  Weight: {model_simple.weights[0]:.4f} (true: 3.0)\")\n",
    "print(f\"  Bias: {model_simple.bias:.4f} (true: 4.0)\")\n",
    "\n",
    "# Metrics\n",
    "mse = mean_squared_error(y_simple, y_pred_simple)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_simple, y_pred_simple)\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  MSE:  {mse:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Scatter plot with regression line\n",
    "axes[0].scatter(X_simple, y_simple, alpha=0.5, label='Data')\n",
    "axes[0].plot(X_simple, y_pred_simple, color='red', linewidth=2, label='Fitted line')\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('y', fontsize=12)\n",
    "axes[0].set_title('Linear Regression Fit', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curve\n",
    "axes[1].plot(model_simple.loss_history, linewidth=2)\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features (important for gradient descent!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nDiabetes Dataset\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different regularization\n",
    "models = {}\n",
    "regularizations = [None, 'l1', 'l2']\n",
    "\n",
    "for reg in regularizations:\n",
    "    print(f\"\\nTraining with regularization: {reg if reg else 'None'}\")\n",
    "    \n",
    "    model = LinearRegressionScratch(\n",
    "        learning_rate=0.1,\n",
    "        n_iterations=1000,\n",
    "        regularization=reg,\n",
    "        lambda_reg=0.1,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    models[reg if reg else 'none'] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test R²:  {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loss curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (reg_type, model_info) in enumerate(models.items()):\n",
    "    model = model_info['model']\n",
    "    \n",
    "    axes[idx].plot(model.loss_history, linewidth=2)\n",
    "    axes[idx].set_xlabel('Iteration', fontsize=12)\n",
    "    axes[idx].set_ylabel('Loss', fontsize=12)\n",
    "    axes[idx].set_title(\n",
    "        f'{reg_type.upper()} Regularization\\n(Test R²: {model_info[\"test_r2\"]:.4f})',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weights across regularization methods\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Create dataframe with weights\n",
    "weights_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'No Reg': models['none']['model'].weights,\n",
    "    'L1 (Lasso)': models['l1']['model'].weights,\n",
    "    'L2 (Ridge)': models['l2']['model'].weights\n",
    "})\n",
    "\n",
    "print(\"\\nFeature Weights Comparison:\")\n",
    "print(weights_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (reg_type, model_info) in enumerate(models.items()):\n",
    "    weights = model_info['model'].weights\n",
    "    \n",
    "    axes[idx].barh(feature_names, weights)\n",
    "    axes[idx].set_xlabel('Weight Value', fontsize=12)\n",
    "    axes[idx].set_title(\n",
    "        f'{reg_type.upper()} Regularization',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "    axes[idx].axvline(x=0, color='red', linestyle='--', linewidth=1)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Regularization Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different lambda values for L2 regularization\n",
    "lambda_values = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "lambda_results = []\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "    model = LinearRegressionScratch(\n",
    "        learning_rate=0.1,\n",
    "        n_iterations=1000,\n",
    "        regularization='l2',\n",
    "        lambda_reg=lambda_val,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    lambda_results.append({\n",
    "        'lambda': lambda_val,\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'weight_sum': np.sum(np.abs(model.weights))\n",
    "    })\n",
    "\n",
    "lambda_df = pd.DataFrame(lambda_results)\n",
    "print(\"\\nEffect of Lambda (L2 Regularization):\")\n",
    "print(lambda_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of lambda\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# R² vs Lambda\n",
    "axes[0].plot(lambda_df['lambda'], lambda_df['train_r2'], \n",
    "            marker='o', linewidth=2, label='Train R²')\n",
    "axes[0].plot(lambda_df['lambda'], lambda_df['test_r2'], \n",
    "            marker='s', linewidth=2, label='Test R²')\n",
    "axes[0].set_xlabel('Lambda (Regularization Strength)', fontsize=12)\n",
    "axes[0].set_ylabel('R² Score', fontsize=12)\n",
    "axes[0].set_title('Model Performance vs Lambda', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight magnitude vs Lambda\n",
    "axes[1].plot(lambda_df['lambda'], lambda_df['weight_sum'], \n",
    "            marker='o', linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Lambda (Regularization Strength)', fontsize=12)\n",
    "axes[1].set_ylabel('Sum of Absolute Weights', fontsize=12)\n",
    "axes[1].set_title('Weight Magnitude vs Lambda', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "lr_models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = LinearRegressionScratch(\n",
    "        learning_rate=lr,\n",
    "        n_iterations=1000,\n",
    "        regularization=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    lr_models[lr] = {\n",
    "        'model': model,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    print(f\"Learning Rate: {lr:6.3f} | Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (lr, results) in enumerate(lr_models.items()):\n",
    "    model = results['model']\n",
    "    \n",
    "    axes[idx].plot(model.loss_history, linewidth=2)\n",
    "    axes[idx].set_xlabel('Iteration', fontsize=11)\n",
    "    axes[idx].set_ylabel('Loss', fontsize=11)\n",
    "    axes[idx].set_title(\n",
    "        f'Learning Rate: {lr} (R²: {results[\"test_r2\"]:.4f})',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of Learning Rate on Convergence', \n",
    "            fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: High-Dimensional Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-dimensional dataset\n",
    "X_syn, y_syn = make_regression(\n",
    "    n_samples=500,\n",
    "    n_features=50,\n",
    "    n_informative=20,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(\n",
    "    X_syn, y_syn, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler_syn = StandardScaler()\n",
    "X_train_syn_scaled = scaler_syn.fit_transform(X_train_syn)\n",
    "X_test_syn_scaled = scaler_syn.transform(X_test_syn)\n",
    "\n",
    "print(\"\\nHigh-Dimensional Synthetic Dataset\")\n",
    "print(f\"Training samples: {len(X_train_syn)}\")\n",
    "print(f\"Test samples: {len(X_test_syn)}\")\n",
    "print(f\"Features: {X_syn.shape[1]}\")\n",
    "print(f\"Informative features: 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different regularization\n",
    "models_syn = {}\n",
    "\n",
    "for reg in [None, 'l1', 'l2']:\n",
    "    model = LinearRegressionScratch(\n",
    "        learning_rate=0.1,\n",
    "        n_iterations=1000,\n",
    "        regularization=reg,\n",
    "        lambda_reg=0.5,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_syn_scaled, y_train_syn)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train_syn_scaled)\n",
    "    y_pred_test = model.predict(X_test_syn_scaled)\n",
    "    \n",
    "    models_syn[reg if reg else 'none'] = {\n",
    "        'model': model,\n",
    "        'train_r2': r2_score(y_train_syn, y_pred_train),\n",
    "        'test_r2': r2_score(y_test_syn, y_pred_test),\n",
    "        'non_zero_weights': np.sum(np.abs(model.weights) > 0.01)\n",
    "    }\n",
    "\n",
    "# Summary\n",
    "print(\"\\nResults on High-Dimensional Data:\")\n",
    "for reg_type, results in models_syn.items():\n",
    "    print(f\"\\n{reg_type.upper()} Regularization:\")\n",
    "    print(f\"  Train R²: {results['train_r2']:.4f}\")\n",
    "    print(f\"  Test R²:  {results['test_r2']:.4f}\")\n",
    "    print(f\"  Non-zero weights: {results['non_zero_weights']}/50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions vs Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for diabetes dataset\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (reg_type, model_info) in enumerate(models.items()):\n",
    "    model = model_info['model']\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    axes[idx].scatter(y_test, y_pred, alpha=0.5)\n",
    "    axes[idx].plot([y_test.min(), y_test.max()], \n",
    "                   [y_test.min(), y_test.max()], \n",
    "                   'r--', linewidth=2, label='Perfect prediction')\n",
    "    axes[idx].set_xlabel('Actual Values', fontsize=12)\n",
    "    axes[idx].set_ylabel('Predicted Values', fontsize=12)\n",
    "    axes[idx].set_title(\n",
    "        f'{reg_type.upper()} (R²: {model_info[\"test_r2\"]:.4f})',\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Gradient Descent**:\n",
    "   - Iteratively updates weights to minimize loss\n",
    "   - Learning rate controls convergence speed\n",
    "   - Feature scaling is crucial for good performance\n",
    "\n",
    "2. **L2 Regularization (Ridge)**:\n",
    "   - Penalizes large weights: $\\lambda \\sum w_j^2$\n",
    "   - Shrinks all weights proportionally\n",
    "   - Good for correlated features\n",
    "   - Never sets weights exactly to zero\n",
    "\n",
    "3. **L1 Regularization (Lasso)**:\n",
    "   - Penalizes absolute weights: $\\lambda \\sum |w_j|$\n",
    "   - Encourages sparsity (feature selection)\n",
    "   - Sets some weights to exactly zero\n",
    "   - Useful for high-dimensional data\n",
    "\n",
    "4. **Regularization Strength (Lambda)**:\n",
    "   - Higher λ → stronger regularization → simpler model\n",
    "   - Lower λ → weaker regularization → more complex model\n",
    "   - Tune via cross-validation\n",
    "\n",
    "5. **When to Use Which**:\n",
    "   - No regularization: Small datasets, low noise\n",
    "   - Ridge (L2): Correlated features, prevent overfitting\n",
    "   - Lasso (L1): Feature selection, sparse solutions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
