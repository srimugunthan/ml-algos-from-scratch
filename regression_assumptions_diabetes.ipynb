{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Linear Regression Assumptions - Diabetes Dataset\n",
    "\n",
    "This notebook tests the key assumptions of linear regression:\n",
    "\n",
    "1. **Linearity**: Relationship between predictors and target is linear\n",
    "2. **Independence**: Observations are independent\n",
    "3. **Homoscedasticity**: Constant variance of residuals\n",
    "4. **Normality**: Residuals are normally distributed\n",
    "5. **No Multicollinearity**: Features are not highly correlated\n",
    "6. **No Autocorrelation**: Residuals are not correlated with each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, jarque_bera, anderson\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# For VIF calculation\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LINEAR REGRESSION ASSUMPTIONS TESTING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"  Samples: {len(df)}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "print(f\"\\nFeature names: {', '.join(feature_names)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "print(df['target'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_train = y_train - y_train_pred\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Model performance\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  Training R²:   {train_r2:.4f}\")\n",
    "print(f\"  Test R²:       {test_r2:.4f}\")\n",
    "print(f\"  Training RMSE: {train_rmse:.2f}\")\n",
    "print(f\"  Test RMSE:     {test_rmse:.2f}\")\n",
    "\n",
    "# Model coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nModel Coefficients (sorted by absolute value):\")\n",
    "print(coef_df.to_string(index=False))\n",
    "print(f\"\\nIntercept: {model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 1: Linearity\n",
    "\n",
    "**Test**: The relationship between each predictor and the target should be linear.\n",
    "\n",
    "**Methods**:\n",
    "- Residuals vs Fitted values plot (should show random scatter)\n",
    "- Partial regression plots for each feature\n",
    "- Scatter plots of features vs target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ASSUMPTION 1: LINEARITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Residuals vs Fitted values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residuals vs Fitted (Training)\n",
    "axes[0].scatter(y_train_pred, residuals_train, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Fitted Values', fontsize=12)\n",
    "axes[0].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0].set_title('Residuals vs Fitted Values (Training)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals vs Fitted (Test)\n",
    "axes[1].scatter(y_test_pred, residuals_test, alpha=0.5, color='orange')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Fitted Values', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residuals vs Fitted Values (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Interpretation:\")\n",
    "print(\"  - Random scatter around zero line → Linearity assumption satisfied\")\n",
    "print(\"  - Patterns (curved, funnel) → Linearity assumption violated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature vs Target scatter plots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(feature_names):\n",
    "    axes[idx].scatter(df[feature], df['target'], alpha=0.5)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[feature], df['target'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(df[feature], p(df[feature]), \"r--\", linewidth=2)\n",
    "    \n",
    "    axes[idx].set_xlabel(feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('Target', fontsize=10)\n",
    "    axes[idx].set_title(f'{feature} vs Target', fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature vs Target Relationships', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 2: Independence of Observations\n",
    "\n",
    "**Test**: Observations should be independent of each other.\n",
    "\n",
    "**Methods**:\n",
    "- Durbin-Watson test (tests for autocorrelation)\n",
    "- Residuals vs Index plot\n",
    "\n",
    "**Durbin-Watson Statistic**:\n",
    "- Values range from 0 to 4\n",
    "- ~2 indicates no autocorrelation\n",
    "- <2 indicates positive autocorrelation\n",
    "- >2 indicates negative autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ASSUMPTION 2: INDEPENDENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Durbin-Watson test\n",
    "dw_stat = durbin_watson(residuals_train)\n",
    "\n",
    "print(f\"\\nDurbin-Watson Statistic: {dw_stat:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if 1.5 < dw_stat < 2.5:\n",
    "    print(\"  ✓ No significant autocorrelation (independence satisfied)\")\n",
    "elif dw_stat <= 1.5:\n",
    "    print(\"  ✗ Positive autocorrelation detected (independence violated)\")\n",
    "else:\n",
    "    print(\"  ✗ Negative autocorrelation detected (independence violated)\")\n",
    "\n",
    "# Residuals vs Index plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training residuals\n",
    "axes[0].scatter(range(len(residuals_train)), residuals_train, alpha=0.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Observation Index', fontsize=12)\n",
    "axes[0].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0].set_title('Residuals vs Index (Training)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test residuals\n",
    "axes[1].scatter(range(len(residuals_test)), residuals_test, alpha=0.5, color='orange')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Observation Index', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residuals vs Index (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Look for patterns or trends in residuals over index\")\n",
    "print(\"  - Random scatter → Independence satisfied\")\n",
    "print(\"  - Patterns/trends → May indicate autocorrelation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 3: Homoscedasticity (Constant Variance)\n",
    "\n",
    "**Test**: Residuals should have constant variance across all levels of fitted values.\n",
    "\n",
    "**Methods**:\n",
    "- Scale-Location plot (sqrt of standardized residuals vs fitted values)\n",
    "- Breusch-Pagan test\n",
    "- Residuals vs Fitted plot (already shown in Linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ASSUMPTION 3: HOMOSCEDASTICITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Standardized residuals\n",
    "std_residuals_train = residuals_train / np.std(residuals_train)\n",
    "std_residuals_test = residuals_test / np.std(residuals_test)\n",
    "\n",
    "# Scale-Location plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training\n",
    "axes[0].scatter(y_train_pred, np.sqrt(np.abs(std_residuals_train)), alpha=0.5)\n",
    "axes[0].set_xlabel('Fitted Values', fontsize=12)\n",
    "axes[0].set_ylabel('√|Standardized Residuals|', fontsize=12)\n",
    "axes[0].set_title('Scale-Location Plot (Training)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add smoothed line\n",
    "from scipy.interpolate import make_interp_spline\n",
    "sorted_idx = np.argsort(y_train_pred)\n",
    "x_smooth = y_train_pred[sorted_idx]\n",
    "y_smooth = np.sqrt(np.abs(std_residuals_train[sorted_idx]))\n",
    "# Simple moving average for smoothing\n",
    "window = 20\n",
    "y_ma = np.convolve(y_smooth, np.ones(window)/window, mode='valid')\n",
    "x_ma = x_smooth[window-1:]\n",
    "axes[0].plot(x_ma, y_ma, 'r-', linewidth=2, label='Trend')\n",
    "axes[0].legend()\n",
    "\n",
    "# Test\n",
    "axes[1].scatter(y_test_pred, np.sqrt(np.abs(std_residuals_test)), alpha=0.5, color='orange')\n",
    "axes[1].set_xlabel('Fitted Values', fontsize=12)\n",
    "axes[1].set_ylabel('√|Standardized Residuals|', fontsize=12)\n",
    "axes[1].set_title('Scale-Location Plot (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Interpretation:\")\n",
    "print(\"  - Horizontal line with equal spread → Homoscedasticity satisfied\")\n",
    "print(\"  - Funnel shape or trend → Heteroscedasticity (violation)\")\n",
    "\n",
    "# Statistical test for homoscedasticity\n",
    "# Simple variance comparison across fitted value ranges\n",
    "low_fitted = y_train_pred < np.median(y_train_pred)\n",
    "high_fitted = y_train_pred >= np.median(y_train_pred)\n",
    "\n",
    "var_low = np.var(residuals_train[low_fitted])\n",
    "var_high = np.var(residuals_train[high_fitted])\n",
    "variance_ratio = max(var_low, var_high) / min(var_low, var_high)\n",
    "\n",
    "print(f\"\\nVariance Ratio (High/Low fitted values): {variance_ratio:.4f}\")\n",
    "print(\"  - Ratio close to 1 → Homoscedasticity\")\n",
    "print(\"  - Ratio >> 1 → Heteroscedasticity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 4: Normality of Residuals\n",
    "\n",
    "**Test**: Residuals should be normally distributed.\n",
    "\n",
    "**Methods**:\n",
    "- Q-Q plot (quantile-quantile plot)\n",
    "- Histogram of residuals\n",
    "- Shapiro-Wilk test\n",
    "- Anderson-Darling test\n",
    "- Jarque-Bera test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ASSUMPTION 4: NORMALITY OF RESIDUALS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visual tests\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Q-Q plot (Training)\n",
    "stats.probplot(residuals_train, dist=\"norm\", plot=axes[0, 0])\n",
    "axes[0, 0].set_title('Q-Q Plot (Training)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot (Test)\n",
    "stats.probplot(residuals_test, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot (Test)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram (Training)\n",
    "axes[1, 0].hist(residuals_train, bins=30, edgecolor='black', alpha=0.7, density=True)\n",
    "# Overlay normal distribution\n",
    "mu, sigma = residuals_train.mean(), residuals_train.std()\n",
    "x = np.linspace(residuals_train.min(), residuals_train.max(), 100)\n",
    "axes[1, 0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal dist')\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Density', fontsize=12)\n",
    "axes[1, 0].set_title('Histogram of Residuals (Training)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram (Test)\n",
    "axes[1, 1].hist(residuals_test, bins=20, edgecolor='black', alpha=0.7, density=True, color='orange')\n",
    "mu_test, sigma_test = residuals_test.mean(), residuals_test.std()\n",
    "x_test = np.linspace(residuals_test.min(), residuals_test.max(), 100)\n",
    "axes[1, 1].plot(x_test, stats.norm.pdf(x_test, mu_test, sigma_test), 'r-', linewidth=2, label='Normal dist')\n",
    "axes[1, 1].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Density', fontsize=12)\n",
    "axes[1, 1].set_title('Histogram of Residuals (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Q-Q Plot Interpretation:\")\n",
    "print(\"  - Points along diagonal line → Normality satisfied\")\n",
    "print(\"  - Deviation from diagonal → Non-normality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for normality\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NORMALITY TESTS (Training Residuals)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Shapiro-Wilk Test\n",
    "shapiro_stat, shapiro_p = shapiro(residuals_train)\n",
    "print(f\"\\n1. Shapiro-Wilk Test:\")\n",
    "print(f\"   Statistic: {shapiro_stat:.6f}\")\n",
    "print(f\"   P-value: {shapiro_p:.6f}\")\n",
    "if shapiro_p > 0.05:\n",
    "    print(\"   ✓ Fail to reject H0: Residuals are normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"   ✗ Reject H0: Residuals are NOT normally distributed (p < 0.05)\")\n",
    "\n",
    "# D'Agostino's K-squared Test\n",
    "dagostino_stat, dagostino_p = normaltest(residuals_train)\n",
    "print(f\"\\n2. D'Agostino's K² Test:\")\n",
    "print(f\"   Statistic: {dagostino_stat:.6f}\")\n",
    "print(f\"   P-value: {dagostino_p:.6f}\")\n",
    "if dagostino_p > 0.05:\n",
    "    print(\"   ✓ Fail to reject H0: Residuals are normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"   ✗ Reject H0: Residuals are NOT normally distributed (p < 0.05)\")\n",
    "\n",
    "# Jarque-Bera Test\n",
    "jb_stat, jb_p = jarque_bera(residuals_train)\n",
    "print(f\"\\n3. Jarque-Bera Test:\")\n",
    "print(f\"   Statistic: {jb_stat:.6f}\")\n",
    "print(f\"   P-value: {jb_p:.6f}\")\n",
    "if jb_p > 0.05:\n",
    "    print(\"   ✓ Fail to reject H0: Residuals are normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"   ✗ Reject H0: Residuals are NOT normally distributed (p < 0.05)\")\n",
    "\n",
    "# Anderson-Darling Test\n",
    "anderson_result = anderson(residuals_train)\n",
    "print(f\"\\n4. Anderson-Darling Test:\")\n",
    "print(f\"   Statistic: {anderson_result.statistic:.6f}\")\n",
    "print(\"   Critical Values:\")\n",
    "for i, (sl, cv) in enumerate(zip(anderson_result.significance_level, anderson_result.critical_values)):\n",
    "    print(f\"     {sl}%: {cv:.6f}\", end=\"\")\n",
    "    if anderson_result.statistic < cv:\n",
    "        print(\" ✓ (Normal)\")\n",
    "    else:\n",
    "        print(\" ✗ (Not normal)\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(f\"\\n5. Descriptive Statistics:\")\n",
    "print(f\"   Mean: {residuals_train.mean():.6f} (should be ~0)\")\n",
    "print(f\"   Std Dev: {residuals_train.std():.6f}\")\n",
    "print(f\"   Skewness: {stats.skew(residuals_train):.6f} (should be ~0)\")\n",
    "print(f\"   Kurtosis: {stats.kurtosis(residuals_train):.6f} (should be ~0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption 5: No Multicollinearity\n",
    "\n",
    "**Test**: Independent variables should not be highly correlated with each other.\n",
    "\n",
    "**Methods**:\n",
    "- Correlation matrix and heatmap\n",
    "- Variance Inflation Factor (VIF)\n",
    "\n",
    "**VIF Interpretation**:\n",
    "- VIF = 1: No correlation\n",
    "- 1 < VIF < 5: Moderate correlation\n",
    "- VIF > 5: High correlation (problematic)\n",
    "- VIF > 10: Severe multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ASSUMPTION 5: NO MULTICOLLINEARITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df[feature_names].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find high correlations\n",
    "print(\"\\nHigh Correlations (|r| > 0.7):\")\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr:\n",
    "    for feat1, feat2, corr in high_corr:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"  ✓ No features with |correlation| > 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for each feature\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VARIANCE INFLATION FACTOR (VIF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = feature_names\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(len(feature_names))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"\\nVIF Values (sorted by magnitude):\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  VIF < 5:  ✓ Low multicollinearity\")\n",
    "print(\"  5 ≤ VIF < 10: ⚠ Moderate multicollinearity\")\n",
    "print(\"  VIF ≥ 10: ✗ High multicollinearity (problematic)\")\n",
    "\n",
    "problematic = vif_data[vif_data['VIF'] > 10]\n",
    "if len(problematic) > 0:\n",
    "    print(f\"\\n✗ {len(problematic)} feature(s) with VIF > 10:\")\n",
    "    print(problematic.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n✓ No features with VIF > 10\")\n",
    "\n",
    "# Visualize VIF\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['red' if x > 10 else 'orange' if x > 5 else 'green' for x in vif_data['VIF']]\n",
    "ax.barh(vif_data['Feature'], vif_data['VIF'], color=colors)\n",
    "ax.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='VIF = 5')\n",
    "ax.axvline(x=10, color='red', linestyle='--', linewidth=2, label='VIF = 10')\n",
    "ax.set_xlabel('VIF Value', fontsize=12)\n",
    "ax.set_title('Variance Inflation Factor by Feature', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Diagnostic Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADDITIONAL DIAGNOSTIC PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "axes[0, 0].scatter(y_train_pred, residuals_train, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Fitted Values', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Residuals', fontsize=11)\n",
    "axes[0, 0].set_title('1. Residuals vs Fitted', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q Plot\n",
    "stats.probplot(residuals_train, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('2. Normal Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scale-Location\n",
    "axes[1, 0].scatter(y_train_pred, np.sqrt(np.abs(std_residuals_train)), alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Fitted Values', fontsize=11)\n",
    "axes[1, 0].set_ylabel('√|Standardized Residuals|', fontsize=11)\n",
    "axes[1, 0].set_title('3. Scale-Location', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals vs Leverage (Cook's distance)\n",
    "# Calculate leverage\n",
    "from numpy.linalg import inv\n",
    "H = X_train.dot(inv(X_train.T.dot(X_train))).dot(X_train.T)\n",
    "leverage = np.diag(H)\n",
    "\n",
    "# Calculate Cook's distance\n",
    "n = len(y_train)\n",
    "p = X_train.shape[1]\n",
    "mse_train = np.mean(residuals_train**2)\n",
    "cooks_d = (std_residuals_train**2 / p) * (leverage / (1 - leverage))\n",
    "\n",
    "axes[1, 1].scatter(leverage, std_residuals_train, alpha=0.5)\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "# Mark influential points (Cook's distance > 0.5)\n",
    "influential = cooks_d > 0.5\n",
    "if np.any(influential):\n",
    "    axes[1, 1].scatter(leverage[influential], std_residuals_train[influential], \n",
    "                      color='red', s=100, label='Influential points')\n",
    "    axes[1, 1].legend()\n",
    "axes[1, 1].set_xlabel('Leverage', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Standardized Residuals', fontsize=11)\n",
    "axes[1, 1].set_title('4. Residuals vs Leverage', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify influential points\n",
    "influential_points = np.where(cooks_d > 0.5)[0]\n",
    "print(f\"\\nInfluential Points (Cook's distance > 0.5): {len(influential_points)}\")\n",
    "if len(influential_points) > 0:\n",
    "    print(f\"  Indices: {influential_points[:10]}...\" if len(influential_points) > 10 else f\"  Indices: {influential_points}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGRESSION ASSUMPTIONS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'Assumption': [],\n",
    "    'Status': [],\n",
    "    'Key Metric': []\n",
    "}\n",
    "\n",
    "# 1. Linearity\n",
    "summary['Assumption'].append('1. Linearity')\n",
    "summary['Status'].append('Check residuals vs fitted plot')\n",
    "summary['Key Metric'].append('Visual inspection')\n",
    "\n",
    "# 2. Independence\n",
    "summary['Assumption'].append('2. Independence')\n",
    "if 1.5 < dw_stat < 2.5:\n",
    "    summary['Status'].append('✓ Satisfied')\n",
    "else:\n",
    "    summary['Status'].append('✗ Violated')\n",
    "summary['Key Metric'].append(f'Durbin-Watson: {dw_stat:.4f}')\n",
    "\n",
    "# 3. Homoscedasticity\n",
    "summary['Assumption'].append('3. Homoscedasticity')\n",
    "if variance_ratio < 2:\n",
    "    summary['Status'].append('✓ Satisfied')\n",
    "else:\n",
    "    summary['Status'].append('⚠ Check scale-location plot')\n",
    "summary['Key Metric'].append(f'Variance Ratio: {variance_ratio:.4f}')\n",
    "\n",
    "# 4. Normality\n",
    "summary['Assumption'].append('4. Normality')\n",
    "if shapiro_p > 0.05:\n",
    "    summary['Status'].append('✓ Satisfied')\n",
    "else:\n",
    "    summary['Status'].append('✗ Violated')\n",
    "summary['Key Metric'].append(f'Shapiro p-value: {shapiro_p:.4f}')\n",
    "\n",
    "# 5. No Multicollinearity\n",
    "summary['Assumption'].append('5. No Multicollinearity')\n",
    "max_vif = vif_data['VIF'].max()\n",
    "if max_vif < 10:\n",
    "    summary['Status'].append('✓ Satisfied')\n",
    "else:\n",
    "    summary['Status'].append('✗ High VIF detected')\n",
    "summary['Key Metric'].append(f'Max VIF: {max_vif:.2f}')\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if shapiro_p < 0.05:\n",
    "    recommendations.append(\"- Consider transformation of target variable (log, sqrt) for normality\")\n",
    "\n",
    "if variance_ratio > 2:\n",
    "    recommendations.append(\"- Consider weighted least squares or robust regression for heteroscedasticity\")\n",
    "\n",
    "if max_vif > 10:\n",
    "    recommendations.append(\"- Remove or combine highly correlated features\")\n",
    "    recommendations.append(\"- Consider Ridge regression for multicollinearity\")\n",
    "\n",
    "if not (1.5 < dw_stat < 2.5):\n",
    "    recommendations.append(\"- Investigate potential time-series patterns in data\")\n",
    "\n",
    "if len(influential_points) > 0:\n",
    "    recommendations.append(f\"- Investigate {len(influential_points)} influential observations\")\n",
    "\n",
    "if len(recommendations) > 0:\n",
    "    print(\"\\n\".join(recommendations))\n",
    "else:\n",
    "    print(\"\\n✓ All assumptions reasonably satisfied!\")\n",
    "    print(\"  Model appears appropriate for linear regression.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
