{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN for Anomaly Detection - Educational Implementation\n",
    "\n",
    "A bare-minimum implementation of DBSCAN (Density-Based Spatial Clustering of Applications with Noise) focused on **anomaly/outlier detection**.\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "DBSCAN naturally identifies anomalies as **noise points** - data points that don't belong to any cluster. These are points that:\n",
    "- Are not dense enough to be core points\n",
    "- Are not close enough to core points to be border points\n",
    "- Represent outliers or anomalies in the dataset\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "1. **Parameter Selection**: Choose ε (epsilon) and MinPts\n",
    "2. **Select Starting Point**: Pick an arbitrary unvisited point\n",
    "3. **Examine Neighborhood**: Find all points within ε distance\n",
    "4. **Expand Cluster**: Recursively add neighbors if core point\n",
    "5. **Repeat**: Process all unvisited points\n",
    "6. **Finalize**: Points with label -1 are **anomalies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core DBSCAN Implementation with Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, epsilon, min_pts):\n",
    "        \"\"\"\n",
    "        Initialize DBSCAN with parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        epsilon : float\n",
    "            Maximum distance between two points to be neighbors\n",
    "        min_pts : int\n",
    "            Minimum number of points to form a dense region\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.min_pts = min_pts\n",
    "        self.labels = None\n",
    "        \n",
    "    def _euclidean_distance(self, point1, point2):\n",
    "        \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
    "        return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "    \n",
    "    def _get_neighbors(self, data, point_idx):\n",
    "        \"\"\"\n",
    "        Step 3: Examine the Neighborhood\n",
    "        Find all points within epsilon distance of the given point.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List of indices of neighboring points\n",
    "        \"\"\"\n",
    "        neighbors = []\n",
    "        for idx in range(len(data)):\n",
    "            if self._euclidean_distance(data[point_idx], data[idx]) <= self.epsilon:\n",
    "                neighbors.append(idx)\n",
    "        return neighbors\n",
    "    \n",
    "    def _expand_cluster(self, data, point_idx, neighbors, cluster_id, visited):\n",
    "        \"\"\"\n",
    "        Step 4: Expand the Cluster\n",
    "        Add all reachable points to the current cluster.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array\n",
    "            Dataset\n",
    "        point_idx : int\n",
    "            Index of the core point\n",
    "        neighbors : list\n",
    "            Indices of neighboring points\n",
    "        cluster_id : int\n",
    "            Current cluster ID\n",
    "        visited : set\n",
    "            Set of visited point indices\n",
    "        \"\"\"\n",
    "        # Assign the core point to the cluster\n",
    "        self.labels[point_idx] = cluster_id\n",
    "        \n",
    "        # Use a queue for breadth-first expansion\n",
    "        queue = deque(neighbors)\n",
    "        \n",
    "        while queue:\n",
    "            current_point = queue.popleft()\n",
    "            \n",
    "            # If point was noise, it's now a border point\n",
    "            if self.labels[current_point] == -1:\n",
    "                self.labels[current_point] = cluster_id\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if current_point in visited:\n",
    "                continue\n",
    "                \n",
    "            visited.add(current_point)\n",
    "            self.labels[current_point] = cluster_id\n",
    "            \n",
    "            # Find neighbors of current point\n",
    "            current_neighbors = self._get_neighbors(data, current_point)\n",
    "            \n",
    "            # If it's a core point, add its neighbors to the queue\n",
    "            if len(current_neighbors) >= self.min_pts:\n",
    "                for neighbor in current_neighbors:\n",
    "                    if neighbor not in visited:\n",
    "                        queue.append(neighbor)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Apply DBSCAN algorithm to the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like, shape (n_samples, n_features)\n",
    "            Dataset to cluster\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Cluster labels (-1 for noise/anomalies, 0+ for cluster IDs)\n",
    "        \"\"\"\n",
    "        data = np.array(data)\n",
    "        n_points = len(data)\n",
    "        \n",
    "        # Initialize all points as noise (-1)\n",
    "        self.labels = np.full(n_points, -1)\n",
    "        \n",
    "        # Track visited points\n",
    "        visited = set()\n",
    "        \n",
    "        # Current cluster ID\n",
    "        cluster_id = 0\n",
    "        \n",
    "        # Step 2 & 5: Select starting point and repeat for all points\n",
    "        for point_idx in range(n_points):\n",
    "            # Skip if already visited\n",
    "            if point_idx in visited:\n",
    "                continue\n",
    "            \n",
    "            visited.add(point_idx)\n",
    "            \n",
    "            # Step 3: Examine the neighborhood\n",
    "            neighbors = self._get_neighbors(data, point_idx)\n",
    "            \n",
    "            # If not enough neighbors, mark as noise (for now)\n",
    "            if len(neighbors) < self.min_pts:\n",
    "                self.labels[point_idx] = -1\n",
    "            else:\n",
    "                # It's a core point - expand the cluster\n",
    "                self._expand_cluster(data, point_idx, neighbors, cluster_id, visited)\n",
    "                cluster_id += 1\n",
    "        \n",
    "        return self.labels\n",
    "    \n",
    "    def get_anomalies(self, data):\n",
    "        \"\"\"\n",
    "        Identify anomalies in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        anomaly_indices : array\n",
    "            Indices of anomalous points\n",
    "        anomaly_points : array\n",
    "            Coordinates of anomalous points\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must call fit() before get_anomalies()\")\n",
    "        \n",
    "        anomaly_indices = np.where(self.labels == -1)[0]\n",
    "        anomaly_points = data[anomaly_indices]\n",
    "        \n",
    "        return anomaly_indices, anomaly_points\n",
    "    \n",
    "    def predict_anomaly(self, data, new_point):\n",
    "        \"\"\"\n",
    "        Predict if a new point is an anomaly.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array\n",
    "            Original training data\n",
    "        new_point : array\n",
    "            New point to classify\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        is_anomaly : bool\n",
    "            True if point is anomaly, False otherwise\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must call fit() before predict_anomaly()\")\n",
    "        \n",
    "        # Count neighbors in the original dataset\n",
    "        neighbors = []\n",
    "        for idx in range(len(data)):\n",
    "            if self._euclidean_distance(new_point, data[idx]) <= self.epsilon:\n",
    "                neighbors.append(idx)\n",
    "        \n",
    "        # If point has enough neighbors from normal clusters, it's not an anomaly\n",
    "        if len(neighbors) >= self.min_pts:\n",
    "            return False\n",
    "        \n",
    "        # Check if close to any cluster (border point)\n",
    "        for neighbor_idx in neighbors:\n",
    "            if self.labels[neighbor_idx] >= 0:  # Neighbor belongs to a cluster\n",
    "                return False\n",
    "        \n",
    "        return True  # It's an anomaly\n",
    "    \n",
    "    def get_anomaly_scores(self, data):\n",
    "        \"\"\"\n",
    "        Calculate anomaly scores based on distance to nearest cluster.\n",
    "        Higher score = more anomalous.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        scores : array\n",
    "            Anomaly score for each point (0 for clustered points)\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Must call fit() before get_anomaly_scores()\")\n",
    "        \n",
    "        scores = np.zeros(len(data))\n",
    "        \n",
    "        for idx in range(len(data)):\n",
    "            if self.labels[idx] == -1:  # Anomaly\n",
    "                # Find minimum distance to any clustered point\n",
    "                min_dist = float('inf')\n",
    "                for other_idx in range(len(data)):\n",
    "                    if self.labels[other_idx] >= 0:  # Clustered point\n",
    "                        dist = self._euclidean_distance(data[idx], data[other_idx])\n",
    "                        min_dist = min(min_dist, dist)\n",
    "                scores[idx] = min_dist\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data with Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create normal clusters (normal behavior)\n",
    "cluster1 = np.random.randn(100, 2) * 0.5 + np.array([0, 0])\n",
    "cluster2 = np.random.randn(100, 2) * 0.5 + np.array([5, 5])\n",
    "cluster3 = np.random.randn(80, 2) * 0.5 + np.array([0, 5])\n",
    "\n",
    "# Add anomalies (unusual/rare events)\n",
    "anomalies = np.random.uniform(-2, 7, (25, 2))\n",
    "\n",
    "# Combine all data\n",
    "data = np.vstack([cluster1, cluster2, cluster3, anomalies])\n",
    "\n",
    "# Track which points are actual anomalies\n",
    "true_anomalies = np.zeros(len(data), dtype=bool)\n",
    "true_anomalies[-25:] = True  # Last 25 points are anomalies\n",
    "\n",
    "print(f\"Generated dataset with {len(data)} points\")\n",
    "print(f\"Normal points: {len(data) - 25}\")\n",
    "print(f\"Anomalies: 25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parameter Selection for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DBSCAN parameters\n",
    "# For anomaly detection:\n",
    "# - epsilon should capture normal density\n",
    "# - min_pts should reflect minimum cluster size\n",
    "\n",
    "epsilon = 0.5      # Maximum distance for neighborhood\n",
    "min_pts = 5        # Minimum points to form dense region\n",
    "\n",
    "print(f\"Anomaly Detection Parameters:\")\n",
    "print(f\"  ε (epsilon): {epsilon}\")\n",
    "print(f\"  MinPts: {min_pts}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Points with < {min_pts} neighbors within {epsilon} distance\")\n",
    "print(f\"    will be marked as anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DBSCAN for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit DBSCAN\n",
    "dbscan = DBSCAN(epsilon=epsilon, min_pts=min_pts)\n",
    "labels = dbscan.fit(data)\n",
    "\n",
    "# Get anomalies\n",
    "anomaly_indices, anomaly_points = dbscan.get_anomalies(data)\n",
    "\n",
    "# Get anomaly scores\n",
    "anomaly_scores = dbscan.get_anomaly_scores(data)\n",
    "\n",
    "# Count clusters and anomalies\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_detected_anomalies = len(anomaly_indices)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Number of normal clusters: {n_clusters}\")\n",
    "print(f\"  Number of anomalies detected: {n_detected_anomalies}\")\n",
    "print(f\"  Anomaly rate: {n_detected_anomalies / len(data) * 100:.1f}%\")\n",
    "\n",
    "# Show top anomalies by score\n",
    "top_anomalies = np.argsort(anomaly_scores)[-5:][::-1]\n",
    "print(f\"\\nTop 5 Anomalies (by isolation score):\")\n",
    "for i, idx in enumerate(top_anomalies, 1):\n",
    "    print(f\"  {i}. Index {idx}: score = {anomaly_scores[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Anomaly Detection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Original Data with True Anomalies\n",
    "axes[0].scatter(data[~true_anomalies, 0], data[~true_anomalies, 1], \n",
    "               c='blue', alpha=0.6, s=50, label='Normal')\n",
    "axes[0].scatter(data[true_anomalies, 0], data[true_anomalies, 1], \n",
    "               c='red', alpha=0.8, s=100, marker='x', label='True Anomalies')\n",
    "axes[0].set_title('Original Data\\n(Ground Truth)')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DBSCAN Results\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    if label == -1:\n",
    "        # Detected anomalies in red\n",
    "        color = 'red'\n",
    "        marker = 'x'\n",
    "        label_name = 'Detected Anomalies'\n",
    "        size = 100\n",
    "    else:\n",
    "        marker = 'o'\n",
    "        label_name = f'Cluster {label}'\n",
    "        size = 50\n",
    "    \n",
    "    mask = labels == label\n",
    "    axes[1].scatter(data[mask, 0], data[mask, 1], \n",
    "                   c=[color], label=label_name, \n",
    "                   alpha=0.6, s=size, marker=marker)\n",
    "\n",
    "axes[1].set_title(f'DBSCAN Anomaly Detection\\n(ε={epsilon}, MinPts={min_pts})')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('Y')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Anomaly Scores Heatmap\n",
    "scatter = axes[2].scatter(data[:, 0], data[:, 1], \n",
    "                         c=anomaly_scores, cmap='Reds', \n",
    "                         s=100, alpha=0.7)\n",
    "axes[2].set_title('Anomaly Scores\\n(Distance to Nearest Cluster)')\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('Y')\n",
    "plt.colorbar(scatter, ax=axes[2], label='Anomaly Score')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Anomaly Detection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare detected anomalies with true anomalies\n",
    "detected_anomalies = labels == -1\n",
    "\n",
    "# Calculate metrics\n",
    "true_positives = np.sum(detected_anomalies & true_anomalies)\n",
    "false_positives = np.sum(detected_anomalies & ~true_anomalies)\n",
    "false_negatives = np.sum(~detected_anomalies & true_anomalies)\n",
    "true_negatives = np.sum(~detected_anomalies & ~true_anomalies)\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"Anomaly Detection Performance:\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives:  {true_positives:3d} (correctly detected anomalies)\")\n",
    "print(f\"  False Positives: {false_positives:3d} (normal points labeled as anomalies)\")\n",
    "print(f\"  False Negatives: {false_negatives:3d} (missed anomalies)\")\n",
    "print(f\"  True Negatives:  {true_negatives:3d} (correctly identified normal points)\")\n",
    "\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Precision: {precision:.3f} (of detected anomalies, how many are real?)\")\n",
    "print(f\"  Recall:    {recall:.3f} (of true anomalies, how many did we detect?)\")\n",
    "print(f\"  F1-Score:  {f1_score:.3f} (harmonic mean of precision and recall)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Anomaly Prediction on New Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on new points\n",
    "test_points = [\n",
    "    np.array([0.1, 0.1]),    # Should be normal (near cluster 1)\n",
    "    np.array([5.1, 5.1]),    # Should be normal (near cluster 2)\n",
    "    np.array([10.0, 10.0]),  # Should be anomaly (far from all clusters)\n",
    "    np.array([-3.0, -3.0]),  # Should be anomaly (far from all clusters)\n",
    "    np.array([2.5, 2.5])     # Could be anomaly (between clusters)\n",
    "]\n",
    "\n",
    "print(\"Testing New Points:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, point in enumerate(test_points, 1):\n",
    "    is_anomaly = dbscan.predict_anomaly(data, point)\n",
    "    status = \"ANOMALY\" if is_anomaly else \"NORMAL\"\n",
    "    print(f\"{i}. Point {point}: {status}\")\n",
    "\n",
    "# Visualize test points\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot original clusters\n",
    "for label in set(labels):\n",
    "    if label == -1:\n",
    "        continue\n",
    "    mask = labels == label\n",
    "    plt.scatter(data[mask, 0], data[mask, 1], alpha=0.4, s=30)\n",
    "\n",
    "# Plot detected anomalies from training\n",
    "mask = labels == -1\n",
    "plt.scatter(data[mask, 0], data[mask, 1], c='red', \n",
    "           marker='x', s=100, alpha=0.6, label='Training Anomalies')\n",
    "\n",
    "# Plot test points\n",
    "for i, point in enumerate(test_points, 1):\n",
    "    is_anomaly = dbscan.predict_anomaly(data, point)\n",
    "    color = 'darkred' if is_anomaly else 'darkgreen'\n",
    "    marker = 'X' if is_anomaly else 'P'\n",
    "    label = f'Test {i}: {\"Anomaly\" if is_anomaly else \"Normal\"}'\n",
    "    plt.scatter(point[0], point[1], c=color, marker=marker, \n",
    "               s=300, edgecolors='black', linewidths=2, \n",
    "               label=label, zorder=10)\n",
    "\n",
    "plt.title('Testing Anomaly Prediction on New Points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Anomaly Detection Example: Credit Card Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate credit card transaction data\n",
    "np.random.seed(123)\n",
    "\n",
    "# Normal transactions (low amount, frequent locations)\n",
    "normal_amount = np.random.gamma(2, 30, 400)  # Typical purchase amounts\n",
    "normal_distance = np.random.gamma(1.5, 5, 400)  # Distance from home\n",
    "\n",
    "# Fraudulent transactions (unusual amounts, unusual locations)\n",
    "fraud_amount = np.random.uniform(500, 2000, 20)  # High amounts\n",
    "fraud_distance = np.random.uniform(50, 200, 20)  # Far from home\n",
    "\n",
    "# Combine data\n",
    "amounts = np.concatenate([normal_amount, fraud_amount])\n",
    "distances = np.concatenate([normal_distance, fraud_distance])\n",
    "transactions = np.column_stack([amounts, distances])\n",
    "\n",
    "# True labels\n",
    "true_fraud = np.zeros(len(transactions), dtype=bool)\n",
    "true_fraud[-20:] = True\n",
    "\n",
    "# Fit DBSCAN\n",
    "fraud_detector = DBSCAN(epsilon=15, min_pts=5)\n",
    "fraud_labels = fraud_detector.fit(transactions)\n",
    "\n",
    "# Results\n",
    "detected_fraud = fraud_labels == -1\n",
    "tp = np.sum(detected_fraud & true_fraud)\n",
    "fp = np.sum(detected_fraud & ~true_fraud)\n",
    "fn = np.sum(~detected_fraud & true_fraud)\n",
    "\n",
    "print(\"Credit Card Fraud Detection:\")\n",
    "print(f\"  Total transactions: {len(transactions)}\")\n",
    "print(f\"  Actual fraud: {np.sum(true_fraud)}\")\n",
    "print(f\"  Detected fraud: {np.sum(detected_fraud)}\")\n",
    "print(f\"  Correctly detected: {tp}\")\n",
    "print(f\"  False alarms: {fp}\")\n",
    "print(f\"  Missed fraud: {fn}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(transactions[~true_fraud, 0], transactions[~true_fraud, 1],\n",
    "           c='green', alpha=0.5, label='Normal')\n",
    "plt.scatter(transactions[true_fraud, 0], transactions[true_fraud, 1],\n",
    "           c='red', marker='x', s=100, label='Actual Fraud')\n",
    "plt.xlabel('Transaction Amount ($)')\n",
    "plt.ylabel('Distance from Home (km)')\n",
    "plt.title('Actual Transactions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(transactions[~detected_fraud, 0], transactions[~detected_fraud, 1],\n",
    "           c='green', alpha=0.5, label='Normal')\n",
    "plt.scatter(transactions[detected_fraud, 0], transactions[detected_fraud, 1],\n",
    "           c='red', marker='x', s=100, label='Detected Fraud')\n",
    "plt.xlabel('Transaction Amount ($)')\n",
    "plt.ylabel('Distance from Home (km)')\n",
    "plt.title('DBSCAN Detection Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different parameter combinations\n",
    "param_combinations = [\n",
    "    (0.3, 5),\n",
    "    (0.5, 5),\n",
    "    (0.8, 5),\n",
    "    (0.5, 3),\n",
    "    (0.5, 8)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (eps, min_p) in enumerate(param_combinations):\n",
    "    dbscan_temp = DBSCAN(epsilon=eps, min_pts=min_p)\n",
    "    labels_temp = dbscan_temp.fit(data)\n",
    "    \n",
    "    detected = labels_temp == -1\n",
    "    tp = np.sum(detected & true_anomalies)\n",
    "    fp = np.sum(detected & ~true_anomalies)\n",
    "    fn = np.sum(~detected & true_anomalies)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(data[~detected, 0], data[~detected, 1],\n",
    "                     c='blue', alpha=0.4, s=30)\n",
    "    axes[idx].scatter(data[detected, 0], data[detected, 1],\n",
    "                     c='red', marker='x', s=100, alpha=0.7)\n",
    "    \n",
    "    axes[idx].set_title(f'ε={eps}, MinPts={min_p}\\n'\n",
    "                       f'Prec: {precision:.2f}, Rec: {recall:.2f}\\n'\n",
    "                       f'Detected: {np.sum(detected)}')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Parameter Sensitivity for Anomaly Detection', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: DBSCAN for Anomaly Detection\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Anomalies as Noise**: DBSCAN identifies anomalies as points with label -1 (noise)\n",
    "2. **Density-Based**: Anomalies are points in low-density regions\n",
    "3. **Unsupervised**: No labeled data required for training\n",
    "\n",
    "### Main Methods\n",
    "\n",
    "- `fit(data)`: Train DBSCAN and identify clusters/anomalies\n",
    "- `get_anomalies(data)`: Extract detected anomalies\n",
    "- `predict_anomaly(data, new_point)`: Classify new points\n",
    "- `get_anomaly_scores(data)`: Score anomalies by isolation\n",
    "\n",
    "### Parameter Guidelines\n",
    "\n",
    "**ε (epsilon):**\n",
    "- Too small: Many normal points become anomalies (high false positives)\n",
    "- Too large: Anomalies join clusters (high false negatives)\n",
    "- Start with: Average distance to k-th nearest neighbor\n",
    "\n",
    "**MinPts:**\n",
    "- Too small: Anomalies form small clusters\n",
    "- Too large: Small valid clusters become anomalies\n",
    "- Rule of thumb: MinPts ≥ dimensions + 1\n",
    "\n",
    "### Advantages\n",
    "\n",
    "✓ Finds arbitrarily shaped clusters\n",
    "✓ Robust to noise\n",
    "✓ No assumption about cluster shape\n",
    "✓ Automatically determines number of clusters\n",
    "\n",
    "### Limitations\n",
    "\n",
    "✗ Sensitive to parameter selection\n",
    "✗ Struggles with varying densities\n",
    "✗ Computational complexity: O(n²)\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- Fraud detection (credit cards, insurance)\n",
    "- Network intrusion detection\n",
    "- Manufacturing defect detection\n",
    "- Medical diagnosis (unusual patient profiles)\n",
    "- Sensor data anomalies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
