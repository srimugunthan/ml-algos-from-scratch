{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE from Scratch: Understanding the Core Algorithm\n",
    "\n",
    "**t-Distributed Stochastic Neighbor Embedding (t-SNE)** is a dimensionality reduction technique that's particularly good at preserving local structure in high-dimensional data.\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "1. **High-dimensional space**: Convert distances between points to probability distributions (using Gaussian)\n",
    "2. **Low-dimensional space**: Convert distances to probability distributions (using Student t-distribution)\n",
    "3. **Optimization**: Minimize the difference (KL divergence) between these two distributions\n",
    "\n",
    "## Why Student t-distribution in low dimensions?\n",
    "- Solves the \"crowding problem\" - points that are moderately far apart in high dimensions can be placed farther apart in 2D/3D\n",
    "- Heavier tails allow for better separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute Pairwise Similarities in High-Dimensional Space\n",
    "\n",
    "For each point $x_i$, we compute conditional probabilities $p_{j|i}$ that $x_i$ would pick $x_j$ as its neighbor:\n",
    "\n",
    "$$p_{j|i} = \\frac{\\exp(-||x_i - x_j||^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-||x_i - x_k||^2 / 2\\sigma_i^2)}$$\n",
    "\n",
    "Then symmetrize: $p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distances(X):\n",
    "    \"\"\"\n",
    "    Compute squared Euclidean distances between all pairs of points.\n",
    "    \n",
    "    Args:\n",
    "        X: (n_samples, n_features) array\n",
    "    \n",
    "    Returns:\n",
    "        distances: (n_samples, n_samples) array of squared distances\n",
    "    \"\"\"\n",
    "    sum_X = np.sum(X**2, axis=1)\n",
    "    # ||x_i - x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2*x_i·x_j\n",
    "    distances = sum_X[:, np.newaxis] + sum_X[np.newaxis, :] - 2 * np.dot(X, X.T)\n",
    "    distances = np.maximum(distances, 0)  # numerical stability\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity_sigma(distances, target_perplexity=30.0, tol=1e-5, max_iter=50):\n",
    "    \"\"\"\n",
    "    Binary search to find sigma that gives target perplexity.\n",
    "    \n",
    "    Perplexity is a measure of the effective number of neighbors.\n",
    "    It's defined as: Perplexity(P_i) = 2^(H(P_i))\n",
    "    where H(P_i) is the Shannon entropy of the distribution P_i\n",
    "    \n",
    "    Args:\n",
    "        distances: (n_samples,) array of squared distances from point i to all others\n",
    "        target_perplexity: desired perplexity value\n",
    "    \n",
    "    Returns:\n",
    "        best_sigma: the sigma value that achieves target perplexity\n",
    "    \"\"\"\n",
    "    n = len(distances)\n",
    "    beta_min = -np.inf\n",
    "    beta_max = np.inf\n",
    "    beta = 1.0  # beta = 1/(2*sigma^2)\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Compute P_i with current beta\n",
    "        P = np.exp(-distances * beta)\n",
    "        P[n] = 0  # set diagonal to 0 (distance to self)\n",
    "        sum_P = np.sum(P)\n",
    "        \n",
    "        if sum_P == 0:\n",
    "            P = np.ones(n + 1) / n\n",
    "            P[n] = 0\n",
    "        else:\n",
    "            P = P / sum_P\n",
    "        \n",
    "        # Compute entropy\n",
    "        H = -np.sum(P * np.log2(P + 1e-10))\n",
    "        perplexity = 2 ** H\n",
    "        \n",
    "        # Binary search\n",
    "        perplexity_diff = perplexity - target_perplexity\n",
    "        if np.abs(perplexity_diff) < tol:\n",
    "            break\n",
    "        \n",
    "        if perplexity_diff > 0:\n",
    "            beta_min = beta\n",
    "            beta = (beta + beta_max) / 2 if beta_max != np.inf else beta * 2\n",
    "        else:\n",
    "            beta_max = beta\n",
    "            beta = (beta + beta_min) / 2 if beta_min != -np.inf else beta / 2\n",
    "    \n",
    "    return np.sqrt(1 / (2 * beta))  # return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_high_dim(X, perplexity=30.0):\n",
    "    \"\"\"\n",
    "    Compute the high-dimensional probability matrix P.\n",
    "    \n",
    "    Args:\n",
    "        X: (n_samples, n_features) array\n",
    "        perplexity: target perplexity value\n",
    "    \n",
    "    Returns:\n",
    "        P: (n_samples, n_samples) symmetrized probability matrix\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    distances = compute_pairwise_distances(X)\n",
    "    P = np.zeros((n, n))\n",
    "    \n",
    "    print(\"Computing high-dimensional probabilities...\")\n",
    "    for i in range(n):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processing point {i}/{n}\")\n",
    "        \n",
    "        # Get distances from point i to all others\n",
    "        dist_i = np.concatenate([distances[i, :i], distances[i, i+1:], [0]])\n",
    "        \n",
    "        # Find sigma for this point\n",
    "        sigma = compute_perplexity_sigma(dist_i, perplexity)\n",
    "        \n",
    "        # Compute conditional probabilities\n",
    "        P[i, :] = np.exp(-distances[i, :] / (2 * sigma**2))\n",
    "        P[i, i] = 0\n",
    "        P[i, :] /= np.sum(P[i, :])\n",
    "    \n",
    "    # Symmetrize and normalize\n",
    "    P = (P + P.T) / (2 * n)\n",
    "    P = np.maximum(P, 1e-12)  # numerical stability\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute Similarities in Low-Dimensional Space\n",
    "\n",
    "In the low-dimensional space (typically 2D), we use Student t-distribution with 1 degree of freedom:\n",
    "\n",
    "$$q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}}$$\n",
    "\n",
    "This is essentially a Cauchy distribution - it has heavier tails than Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_low_dim(Y):\n",
    "    \"\"\"\n",
    "    Compute the low-dimensional probability matrix Q using Student t-distribution.\n",
    "    \n",
    "    Args:\n",
    "        Y: (n_samples, n_components) array of low-dimensional embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Q: (n_samples, n_samples) probability matrix\n",
    "    \"\"\"\n",
    "    n = Y.shape[0]\n",
    "    distances = compute_pairwise_distances(Y)\n",
    "    \n",
    "    # Student t-distribution with df=1\n",
    "    Q = 1 / (1 + distances)\n",
    "    np.fill_diagonal(Q, 0)\n",
    "    \n",
    "    # Normalize\n",
    "    Q = Q / np.sum(Q)\n",
    "    Q = np.maximum(Q, 1e-12)  # numerical stability\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimize with Gradient Descent\n",
    "\n",
    "The cost function is the Kullback-Leibler divergence:\n",
    "\n",
    "$$C = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "The gradient with respect to $y_i$ is:\n",
    "\n",
    "$$\\frac{\\delta C}{\\delta y_i} = 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + ||y_i - y_j||^2)^{-1}$$\n",
    "\n",
    "We use momentum for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(P, Q, Y):\n",
    "    \"\"\"\n",
    "    Compute gradient of KL divergence with respect to Y.\n",
    "    \n",
    "    Args:\n",
    "        P: (n_samples, n_samples) high-dimensional probabilities\n",
    "        Q: (n_samples, n_samples) low-dimensional probabilities\n",
    "        Y: (n_samples, n_components) current low-dimensional embedding\n",
    "    \n",
    "    Returns:\n",
    "        gradient: (n_samples, n_components) gradient\n",
    "    \"\"\"\n",
    "    n, n_components = Y.shape\n",
    "    \n",
    "    # Pairwise differences\n",
    "    diff = Y[:, np.newaxis, :] - Y[np.newaxis, :, :]  # (n, n, n_components)\n",
    "    \n",
    "    # Compute distances for Student-t kernel\n",
    "    distances = compute_pairwise_distances(Y)\n",
    "    inv_distances = 1 / (1 + distances)\n",
    "    np.fill_diagonal(inv_distances, 0)\n",
    "    \n",
    "    # Gradient: 4 * sum_j (p_ij - q_ij) * (y_i - y_j) * (1 + ||y_i - y_j||^2)^(-1)\n",
    "    PQ_diff = P - Q\n",
    "    gradient = 4 * np.sum(\n",
    "        (PQ_diff[:, :, np.newaxis] * diff * inv_distances[:, :, np.newaxis]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne(X, n_components=2, perplexity=30.0, n_iter=1000, learning_rate=200.0, momentum=0.8):\n",
    "    \"\"\"\n",
    "    t-SNE implementation from scratch.\n",
    "    \n",
    "    Args:\n",
    "        X: (n_samples, n_features) input data\n",
    "        n_components: dimension of embedding (usually 2)\n",
    "        perplexity: controls effective number of neighbors (5-50 typical)\n",
    "        n_iter: number of optimization iterations\n",
    "        learning_rate: gradient descent learning rate\n",
    "        momentum: momentum coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Y: (n_samples, n_components) low-dimensional embedding\n",
    "        losses: list of KL divergence values during optimization\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Step 1: Compute high-dimensional probabilities\n",
    "    P = compute_p_high_dim(X, perplexity)\n",
    "    \n",
    "    # Early exaggeration: multiply P by 4 for first 250 iterations\n",
    "    P_exaggerated = P * 4.0\n",
    "    \n",
    "    # Step 2: Initialize low-dimensional embedding\n",
    "    Y = np.random.randn(n, n_components) * 1e-4\n",
    "    \n",
    "    # For momentum\n",
    "    Y_velocity = np.zeros_like(Y)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print(\"\\nOptimizing embedding...\")\n",
    "    for iteration in range(n_iter):\n",
    "        # Use exaggerated P for first 250 iterations\n",
    "        P_current = P_exaggerated if iteration < 250 else P\n",
    "        \n",
    "        # Step 3: Compute low-dimensional probabilities\n",
    "        Q = compute_q_low_dim(Y)\n",
    "        \n",
    "        # Step 4: Compute gradient\n",
    "        gradient = compute_gradient(P_current, Q, Y)\n",
    "        \n",
    "        # Step 5: Update with momentum\n",
    "        Y_velocity = momentum * Y_velocity - learning_rate * gradient\n",
    "        Y = Y + Y_velocity\n",
    "        \n",
    "        # Center the embedding\n",
    "        Y = Y - np.mean(Y, axis=0)\n",
    "        \n",
    "        # Compute loss (KL divergence)\n",
    "        loss = np.sum(P * np.log(P / Q))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"  Iteration {iteration}: KL divergence = {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nOptimization complete! Final loss: {losses[-1]:.4f}\")\n",
    "    return Y, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Digits Dataset\n",
    "\n",
    "Let's apply t-SNE to the classic digits dataset - 64-dimensional handwritten digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_digits = scaler.fit_transform(X_digits)\n",
    "\n",
    "# Use subset for faster computation\n",
    "n_samples = 500\n",
    "indices = np.random.choice(len(X_digits), n_samples, replace=False)\n",
    "X_subset = X_digits[indices]\n",
    "y_subset = y_digits[indices]\n",
    "\n",
    "print(f\"Dataset shape: {X_subset.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_subset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-SNE\n",
    "Y_embedded, losses = tsne(\n",
    "    X_subset, \n",
    "    n_components=2, \n",
    "    perplexity=30.0, \n",
    "    n_iter=1000,\n",
    "    learning_rate=200.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(losses, linewidth=2)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('KL Divergence', fontsize=12)\n",
    "axes[0].set_title('t-SNE Optimization Progress', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Embedded points\n",
    "scatter = axes[1].scatter(\n",
    "    Y_embedded[:, 0], \n",
    "    Y_embedded[:, 1], \n",
    "    c=y_subset, \n",
    "    cmap='tab10',\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "axes[1].set_title('Digits Dataset - t-SNE Embedding', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Digit Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Synthetic Data - Clustered Structure\n",
    "\n",
    "Let's create synthetic data with clear cluster structure to see how t-SNE preserves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic clustered data\n",
    "X_synthetic, y_synthetic = make_classification(\n",
    "    n_samples=400,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "X_synthetic = StandardScaler().fit_transform(X_synthetic)\n",
    "\n",
    "print(f\"Synthetic data shape: {X_synthetic.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_synthetic))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-SNE on synthetic data\n",
    "Y_synthetic, losses_synthetic = tsne(\n",
    "    X_synthetic, \n",
    "    n_components=2, \n",
    "    perplexity=30.0, \n",
    "    n_iter=1000,\n",
    "    learning_rate=200.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetic data results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(losses_synthetic, linewidth=2, color='darkred')\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('KL Divergence', fontsize=12)\n",
    "axes[0].set_title('t-SNE Optimization Progress (Synthetic Data)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Embedded points\n",
    "scatter = axes[1].scatter(\n",
    "    Y_synthetic[:, 0], \n",
    "    Y_synthetic[:, 1], \n",
    "    c=y_synthetic, \n",
    "    cmap='viridis',\n",
    "    s=60,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "axes[1].set_title('Synthetic Data - t-SNE Embedding', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Key Parameters\n",
    "\n",
    "### 1. Perplexity\n",
    "- Controls effective number of neighbors considered for each point\n",
    "- Typical range: 5-50\n",
    "- Lower perplexity → focuses on very local structure\n",
    "- Higher perplexity → focuses on more global structure\n",
    "\n",
    "### 2. Learning Rate\n",
    "- Controls step size in gradient descent\n",
    "- Typical range: 10-1000\n",
    "- Too low → slow convergence\n",
    "- Too high → unstable optimization\n",
    "\n",
    "### 3. Early Exaggeration\n",
    "- Multiplies P by 4 in first 250 iterations\n",
    "- Creates tight clusters early on\n",
    "- Helps separate clusters before fine-tuning\n",
    "\n",
    "### 4. Momentum\n",
    "- Helps escape local minima\n",
    "- Smooths the optimization trajectory\n",
    "- Typical value: 0.5-0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Different Perplexity Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "embeddings = []\n",
    "\n",
    "# Use smaller subset for faster comparison\n",
    "n_compare = 300\n",
    "X_compare = X_digits[:n_compare]\n",
    "y_compare = y_digits[:n_compare]\n",
    "\n",
    "for perp in perplexities:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running t-SNE with perplexity = {perp}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    Y_perp, _ = tsne(\n",
    "        X_compare, \n",
    "        n_components=2, \n",
    "        perplexity=perp, \n",
    "        n_iter=500,  # Fewer iterations for comparison\n",
    "        learning_rate=200.0\n",
    "    )\n",
    "    embeddings.append(Y_perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different perplexity results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (perp, Y_emb) in enumerate(zip(perplexities, embeddings)):\n",
    "    scatter = axes[idx].scatter(\n",
    "        Y_emb[:, 0], \n",
    "        Y_emb[:, 1], \n",
    "        c=y_compare, \n",
    "        cmap='tab10',\n",
    "        s=40,\n",
    "        alpha=0.7,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    axes[idx].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
    "    axes[idx].set_title(f'Perplexity = {perp}', fontsize=13, fontweight='bold')\n",
    "    if idx == 2:\n",
    "        plt.colorbar(scatter, ax=axes[idx], label='Digit')\n",
    "\n",
    "plt.suptitle('Effect of Perplexity on t-SNE Embeddings', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **t-SNE is stochastic** - different runs produce different results (but similar structure)\n",
    "\n",
    "2. **Distances in t-SNE plots are not meaningful** - only relative positions within clusters matter\n",
    "\n",
    "3. **t-SNE preserves local structure** - points close in high-D stay close in low-D\n",
    "\n",
    "4. **Computational complexity is O(N²)** - doesn't scale well to very large datasets\n",
    "   - For larger datasets, use approximations like Barnes-Hut t-SNE\n",
    "\n",
    "5. **Choose perplexity based on dataset size**:\n",
    "   - Small datasets (N < 500): perplexity = 5-15\n",
    "   - Medium datasets (500 < N < 5000): perplexity = 30-50\n",
    "   - Larger datasets: consider higher values\n",
    "\n",
    "6. **Early exaggeration helps** - it creates tighter initial clusters that can separate better\n",
    "\n",
    "7. **Run for enough iterations** - typically 1000+ to ensure convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
