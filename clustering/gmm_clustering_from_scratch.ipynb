{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model (GMM) from Scratch\n",
    "\n",
    "Implementation of Gaussian Mixture Model for clustering using Expectation-Maximization (EM) algorithm.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Probabilistic clustering (soft assignments)\n",
    "- Expectation-Maximization (EM) algorithm\n",
    "- Multivariate Gaussian distributions\n",
    "- Maximum likelihood estimation\n",
    "- BIC/AIC for model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_blobs, load_iris, make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GAUSSIAN MIXTURE MODEL FROM SCRATCH\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundation\n",
    "\n",
    "**Gaussian Mixture Model:**\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where:\n",
    "- $K$ = number of components (clusters)\n",
    "- $\\pi_k$ = mixing coefficient (weight) for component $k$, $\\sum_{k=1}^{K} \\pi_k = 1$\n",
    "- $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ = multivariate Gaussian with mean $\\mu_k$ and covariance $\\Sigma_k$\n",
    "\n",
    "**Multivariate Gaussian:**\n",
    "$$\\mathcal{N}(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$\n",
    "\n",
    "**EM Algorithm:**\n",
    "\n",
    "**E-Step** (Expectation): Calculate responsibilities (posterior probabilities)\n",
    "$$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "**M-Step** (Maximization): Update parameters\n",
    "$$N_k = \\sum_{i=1}^{N} \\gamma_{ik}$$\n",
    "$$\\pi_k = \\frac{N_k}{N}$$\n",
    "$$\\mu_k = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} x_i$$\n",
    "$$\\Sigma_k = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T$$\n",
    "\n",
    "**Log-Likelihood:**\n",
    "$$\\log L = \\sum_{i=1}^{N} \\log \\left(\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureScratch:\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Model implemented from scratch using EM algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_components : int, default=3\n",
    "        Number of Gaussian components\n",
    "    max_iters : int, default=100\n",
    "        Maximum number of EM iterations\n",
    "    tol : float, default=1e-4\n",
    "        Convergence tolerance (change in log-likelihood)\n",
    "    reg_covar : float, default=1e-6\n",
    "        Regularization added to covariance diagonal for numerical stability\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=3, max_iters=100, tol=1e-4, \n",
    "                 reg_covar=1e-6, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.reg_covar = reg_covar\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Model parameters\n",
    "        self.weights_ = None      # Mixing coefficients (π)\n",
    "        self.means_ = None        # Component means (μ)\n",
    "        self.covariances_ = None  # Component covariances (Σ)\n",
    "        \n",
    "        # Fit results\n",
    "        self.responsibilities_ = None  # Posterior probabilities (γ)\n",
    "        self.n_iter_ = 0\n",
    "        self.converged_ = False\n",
    "        \n",
    "        # History\n",
    "        self.log_likelihood_history_ = []\n",
    "        \n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Initialize GMM parameters using k-means-like approach.\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize means by randomly selecting samples\n",
    "        indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "        self.means_ = X[indices]\n",
    "        \n",
    "        # Initialize covariances as identity matrices scaled by data variance\n",
    "        self.covariances_ = np.array([\n",
    "            np.eye(n_features) * np.var(X)\n",
    "            for _ in range(self.n_components)\n",
    "        ])\n",
    "        \n",
    "        # Initialize weights uniformly\n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "    def _multivariate_gaussian(self, X, mean, covariance):\n",
    "        \"\"\"\n",
    "        Calculate multivariate Gaussian probability density.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Add regularization to covariance for numerical stability\n",
    "        covariance_reg = covariance + self.reg_covar * np.eye(n_features)\n",
    "        \n",
    "        # Use scipy for numerical stability\n",
    "        return multivariate_normal.pdf(X, mean=mean, cov=covariance_reg)\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        \"\"\"\n",
    "        E-Step: Calculate responsibilities (posterior probabilities).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        responsibilities : array, shape (n_samples, n_components)\n",
    "            Posterior probability of each sample belonging to each component\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Calculate weighted probabilities for each component\n",
    "        for k in range(self.n_components):\n",
    "            responsibilities[:, k] = (\n",
    "                self.weights_[k] * \n",
    "                self._multivariate_gaussian(X, self.means_[k], self.covariances_[k])\n",
    "            )\n",
    "        \n",
    "        # Normalize to get probabilities (responsibilities)\n",
    "        responsibilities_sum = responsibilities.sum(axis=1, keepdims=True)\n",
    "        # Avoid division by zero\n",
    "        responsibilities_sum[responsibilities_sum == 0] = 1e-10\n",
    "        responsibilities /= responsibilities_sum\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        M-Step: Update parameters based on responsibilities.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Effective number of points assigned to each component\n",
    "        N_k = responsibilities.sum(axis=0)\n",
    "        \n",
    "        # Update weights (mixing coefficients)\n",
    "        self.weights_ = N_k / n_samples\n",
    "        \n",
    "        # Update means\n",
    "        self.means_ = (responsibilities.T @ X) / N_k[:, np.newaxis]\n",
    "        \n",
    "        # Update covariances\n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means_[k]\n",
    "            # Weighted covariance\n",
    "            weighted_diff = responsibilities[:, k:k+1] * diff\n",
    "            self.covariances_[k] = (weighted_diff.T @ diff) / N_k[k]\n",
    "            \n",
    "            # Add small regularization for numerical stability\n",
    "            self.covariances_[k] += self.reg_covar * np.eye(n_features)\n",
    "    \n",
    "    def _calculate_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        Calculate log-likelihood of the data.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_likelihood = 0\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            sample_likelihood = 0\n",
    "            for k in range(self.n_components):\n",
    "                sample_likelihood += (\n",
    "                    self.weights_[k] * \n",
    "                    self._multivariate_gaussian(\n",
    "                        X[i:i+1], self.means_[k], self.covariances_[k]\n",
    "                    )\n",
    "                )\n",
    "            log_likelihood += np.log(sample_likelihood + 1e-10)\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit GMM using EM algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X)\n",
    "        \n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        # EM iterations\n",
    "        for iteration in range(self.max_iters):\n",
    "            # E-step\n",
    "            responsibilities = self._e_step(X)\n",
    "            \n",
    "            # M-step\n",
    "            self._m_step(X, responsibilities)\n",
    "            \n",
    "            # Calculate log-likelihood\n",
    "            log_likelihood = self._calculate_log_likelihood(X)\n",
    "            self.log_likelihood_history_.append(log_likelihood)\n",
    "            \n",
    "            # Check convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                self.converged_ = True\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "            \n",
    "            prev_log_likelihood = log_likelihood\n",
    "            self.n_iter_ = iteration + 1\n",
    "        \n",
    "        # Final responsibilities\n",
    "        self.responsibilities_ = self._e_step(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels (hard assignments).\n",
    "        \"\"\"\n",
    "        responsibilities = self._e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict soft cluster assignments (responsibilities).\n",
    "        \"\"\"\n",
    "        return self._e_step(X)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Fit and return cluster labels.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.predict(X)\n",
    "    \n",
    "    def bic(self, X):\n",
    "        \"\"\"\n",
    "        Calculate Bayesian Information Criterion.\n",
    "        BIC = -2 * log_likelihood + k * log(n)\n",
    "        Lower is better.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Number of parameters\n",
    "        # weights: K-1, means: K*D, covariances: K*D*(D+1)/2\n",
    "        n_params = (\n",
    "            (self.n_components - 1) +  # weights (one is constrained)\n",
    "            self.n_components * n_features +  # means\n",
    "            self.n_components * n_features * (n_features + 1) / 2  # covariances\n",
    "        )\n",
    "        \n",
    "        log_likelihood = self._calculate_log_likelihood(X)\n",
    "        return -2 * log_likelihood + n_params * np.log(n_samples)\n",
    "    \n",
    "    def aic(self, X):\n",
    "        \"\"\"\n",
    "        Calculate Akaike Information Criterion.\n",
    "        AIC = -2 * log_likelihood + 2 * k\n",
    "        Lower is better.\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Number of parameters\n",
    "        n_params = (\n",
    "            (self.n_components - 1) +\n",
    "            self.n_components * n_features +\n",
    "            self.n_components * n_features * (n_features + 1) / 2\n",
    "        )\n",
    "        \n",
    "        log_likelihood = self._calculate_log_likelihood(X)\n",
    "        return -2 * log_likelihood + 2 * n_params\n",
    "\n",
    "print(\"\\n✓ GaussianMixtureScratch class defined\")\n",
    "print(\"  - EM algorithm implementation\")\n",
    "print(\"  - Soft (probabilistic) assignments\")\n",
    "print(\"  - BIC/AIC for model selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm_clusters(X, gmm, title=\"GMM Clustering\", true_labels=None):\n",
    "    \"\"\"\n",
    "    Plot GMM clustering results with ellipses representing Gaussian components.\n",
    "    \"\"\"\n",
    "    if X.shape[1] != 2:\n",
    "        raise ValueError(\"This function only works for 2D data\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get cluster assignments\n",
    "    labels = gmm.predict(X)\n",
    "    \n",
    "    # Plot points\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis',\n",
    "                        s=50, alpha=0.6, edgecolors='black')\n",
    "    \n",
    "    # Plot Gaussian ellipses\n",
    "    for k in range(gmm.n_components):\n",
    "        mean = gmm.means_[k]\n",
    "        covariance = gmm.covariances_[k]\n",
    "        \n",
    "        # Calculate eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "        \n",
    "        # Width and height are 2 standard deviations\n",
    "        width, height = 2 * np.sqrt(eigenvalues)\n",
    "        \n",
    "        # Plot ellipse for 1 and 2 standard deviations\n",
    "        for n_std in [1, 2]:\n",
    "            ellipse = Ellipse(\n",
    "                mean, n_std * width, n_std * height,\n",
    "                angle=angle, facecolor='none',\n",
    "                edgecolor=scatter.cmap(scatter.norm(k)),\n",
    "                linewidth=2, linestyle='--', alpha=0.7\n",
    "            )\n",
    "            ax.add_patch(ellipse)\n",
    "        \n",
    "        # Plot mean\n",
    "        ax.scatter(mean[0], mean[1], c='red', s=200, marker='X',\n",
    "                  edgecolors='black', linewidth=2, zorder=10)\n",
    "    \n",
    "    ax.set_xlabel('Feature 1', fontsize=12)\n",
    "    ax.set_ylabel('Feature 2', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "print(\"\\n✓ Visualization utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple 2D Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with clear Gaussian clusters\n",
    "X_blob, y_true = make_blobs(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    centers=3,\n",
    "    cluster_std=[1.0, 1.5, 0.8],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Synthetic 2D Dataset:\")\n",
    "print(f\"  Samples: {len(X_blob)}\")\n",
    "print(f\"  Features: {X_blob.shape[1]}\")\n",
    "print(f\"  True clusters: {len(np.unique(y_true))}\")\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_true, cmap='viridis', \n",
    "            s=50, alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('True Cluster Assignments', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm = GaussianMixtureScratch(\n",
    "    n_components=3,\n",
    "    max_iters=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gmm.fit(X_blob)\n",
    "labels_gmm = gmm.predict(X_blob)\n",
    "probabilities = gmm.predict_proba(X_blob)\n",
    "\n",
    "print(\"\\nGMM Results:\")\n",
    "print(f\"  Converged: {gmm.converged_}\")\n",
    "print(f\"  Iterations: {gmm.n_iter_}\")\n",
    "print(f\"  Final log-likelihood: {gmm.log_likelihood_history_[-1]:.2f}\")\n",
    "print(f\"  Cluster sizes: {np.bincount(labels_gmm)}\")\n",
    "print(f\"\\nMixing coefficients (weights):\")\n",
    "for k, weight in enumerate(gmm.weights_):\n",
    "    print(f\"  Component {k}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GMM clustering with Gaussian ellipses\n",
    "plot_gmm_clusters(X_blob, gmm, title=\"GMM Clustering with Gaussian Components\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Ellipses represent 1σ and 2σ contours of Gaussian components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(gmm.log_likelihood_history_, marker='o', linewidth=2, markersize=4)\n",
    "ax.set_xlabel('Iteration', fontsize=12)\n",
    "ax.set_ylabel('Log-Likelihood', fontsize=12)\n",
    "ax.set_title('EM Algorithm Convergence', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Log-likelihood should monotonically increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft vs Hard Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate soft clustering (probabilistic assignments)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SOFT vs HARD CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show probabilities for first 10 samples\n",
    "print(\"\\nResponsibilities (soft assignments) for first 10 samples:\")\n",
    "print(\"Sample | Component 0 | Component 1 | Component 2 | Hard Label\")\n",
    "print(\"-\" * 65)\n",
    "for i in range(10):\n",
    "    print(f\"  {i:3d}  |    {probabilities[i, 0]:.4f}    |    {probabilities[i, 1]:.4f}    |    {probabilities[i, 2]:.4f}    |     {labels_gmm[i]}\")\n",
    "\n",
    "print(\"\\n✓ Soft assignments show uncertainty in cluster membership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize soft clustering by coloring points by uncertainty\n",
    "# Calculate uncertainty as entropy of responsibilities\n",
    "uncertainty = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Hard clustering\n",
    "axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=labels_gmm, cmap='viridis',\n",
    "               s=50, alpha=0.6, edgecolors='black')\n",
    "axes[0].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[0].set_title('Hard Clustering (argmax)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Uncertainty visualization\n",
    "scatter = axes[1].scatter(X_blob[:, 0], X_blob[:, 1], c=uncertainty, \n",
    "                         cmap='coolwarm', s=50, alpha=0.6, edgecolors='black')\n",
    "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[1].set_title('Clustering Uncertainty (Entropy)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1], label='Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Points near cluster boundaries have higher uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: BIC and AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL SELECTION - BIC AND AIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test different numbers of components\n",
    "n_components_range = range(2, 8)\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "log_likelihoods = []\n",
    "\n",
    "for n_comp in n_components_range:\n",
    "    gmm_test = GaussianMixtureScratch(\n",
    "        n_components=n_comp,\n",
    "        max_iters=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    gmm_test.fit(X_blob)\n",
    "    \n",
    "    bic = gmm_test.bic(X_blob)\n",
    "    aic = gmm_test.aic(X_blob)\n",
    "    log_like = gmm_test.log_likelihood_history_[-1]\n",
    "    \n",
    "    bic_scores.append(bic)\n",
    "    aic_scores.append(aic)\n",
    "    log_likelihoods.append(log_like)\n",
    "    \n",
    "    print(f\"K={n_comp} | Log-Like: {log_like:8.2f} | BIC: {bic:8.2f} | AIC: {aic:8.2f}\")\n",
    "\n",
    "best_bic = n_components_range[np.argmin(bic_scores)]\n",
    "best_aic = n_components_range[np.argmin(aic_scores)]\n",
    "\n",
    "print(f\"\\nBest K by BIC: {best_bic}\")\n",
    "print(f\"Best K by AIC: {best_aic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model selection criteria\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Log-likelihood\n",
    "axes[0].plot(n_components_range, log_likelihoods, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[0].set_ylabel('Log-Likelihood', fontsize=12)\n",
    "axes[0].set_title('Log-Likelihood (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# BIC\n",
    "axes[1].plot(n_components_range, bic_scores, marker='s', linewidth=2, \n",
    "            markersize=8, color='green')\n",
    "axes[1].axvline(x=best_bic, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Best K={best_bic}')\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('BIC', fontsize=12)\n",
    "axes[1].set_title('Bayesian Information Criterion (Lower is Better)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# AIC\n",
    "axes[2].plot(n_components_range, aic_scores, marker='^', linewidth=2, \n",
    "            markersize=8, color='orange')\n",
    "axes[2].axvline(x=best_aic, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Best K={best_aic}')\n",
    "axes[2].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[2].set_ylabel('AIC', fontsize=12)\n",
    "axes[2].set_title('Akaike Information Criterion (Lower is Better)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ BIC penalizes model complexity more heavily than AIC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Non-Spherical Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with elliptical clusters\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_ellipse, y_ellipse = make_classification(\n",
    "    n_samples=400,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    n_classes=3,\n",
    "    class_sep=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add some correlation to make elliptical clusters\n",
    "rotation = np.array([[0.8, -0.6], [0.6, 0.8]])\n",
    "X_ellipse = X_ellipse.dot(rotation)\n",
    "\n",
    "print(\"\\nElliptical Clusters Dataset:\")\n",
    "print(f\"  Samples: {len(X_ellipse)}\")\n",
    "print(f\"  Features: {X_ellipse.shape[1]}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=y_ellipse, cmap='viridis',\n",
    "           s=50, alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Elliptical Clusters', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(label='True Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm_ellipse = GaussianMixtureScratch(\n",
    "    n_components=3,\n",
    "    max_iters=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gmm_ellipse.fit(X_ellipse)\n",
    "\n",
    "print(\"\\nGMM on Elliptical Clusters:\")\n",
    "print(f\"  Converged: {gmm_ellipse.converged_}\")\n",
    "print(f\"  Iterations: {gmm_ellipse.n_iter_}\")\n",
    "\n",
    "# Visualize\n",
    "plot_gmm_clusters(X_ellipse, gmm_ellipse, \n",
    "                 title=\"GMM Handles Non-Spherical Clusters Well\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ GMM can model elliptical clusters (unlike K-means which assumes spherical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(\"\\nIris Dataset:\")\n",
    "print(f\"  Samples: {len(X_iris)}\")\n",
    "print(f\"  Features: {X_iris.shape[1]}\")\n",
    "print(f\"  True classes: {len(np.unique(y_iris))}\")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm_iris = GaussianMixtureScratch(\n",
    "    n_components=3,\n",
    "    max_iters=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gmm_iris.fit(X_iris_scaled)\n",
    "labels_iris = gmm_iris.predict(X_iris_scaled)\n",
    "\n",
    "print(\"\\nGMM Results (K=3):\")\n",
    "print(f\"  Converged: {gmm_iris.converged_}\")\n",
    "print(f\"  Iterations: {gmm_iris.n_iter_}\")\n",
    "print(f\"  Cluster sizes: {np.bincount(labels_iris)}\")\n",
    "\n",
    "# Evaluation\n",
    "silhouette = silhouette_score(X_iris_scaled, labels_iris)\n",
    "ari = adjusted_rand_score(y_iris, labels_iris)\n",
    "\n",
    "print(f\"\\nEvaluation:\")\n",
    "print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "print(f\"  Adjusted Rand Index: {ari:.4f}\")\n",
    "print(f\"  BIC: {gmm_iris.bic(X_iris_scaled):.2f}\")\n",
    "print(f\"  AIC: {gmm_iris.aic(X_iris_scaled):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_iris_pca = pca.fit_transform(X_iris_scaled)\n",
    "\n",
    "# Fit GMM on 2D PCA data for visualization\n",
    "gmm_iris_2d = GaussianMixtureScratch(\n",
    "    n_components=3,\n",
    "    max_iters=100,\n",
    "    random_state=42\n",
    ")\n",
    "gmm_iris_2d.fit(X_iris_pca)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# True labels\n",
    "axes[0].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris, cmap='viridis',\n",
    "               s=50, alpha=0.6, edgecolors='black')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} var)', fontsize=11)\n",
    "axes[0].set_title('True Species Labels', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# GMM clusters with ellipses\n",
    "labels_2d = gmm_iris_2d.predict(X_iris_pca)\n",
    "scatter = axes[1].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=labels_2d, \n",
    "                         cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
    "\n",
    "# Plot Gaussian ellipses\n",
    "for k in range(gmm_iris_2d.n_components):\n",
    "    mean = gmm_iris_2d.means_[k]\n",
    "    covariance = gmm_iris_2d.covariances_[k]\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "    width, height = 2 * np.sqrt(eigenvalues)\n",
    "    \n",
    "    for n_std in [1, 2]:\n",
    "        ellipse = Ellipse(\n",
    "            mean, n_std * width, n_std * height, angle=angle,\n",
    "            facecolor='none', edgecolor=scatter.cmap(scatter.norm(k)),\n",
    "            linewidth=2, linestyle='--', alpha=0.7\n",
    "        )\n",
    "        axes[1].add_patch(ellipse)\n",
    "    \n",
    "    axes[1].scatter(mean[0], mean[1], c='red', s=200, marker='X',\n",
    "                   edgecolors='black', linewidth=2, zorder=10)\n",
    "\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} var)', fontsize=11)\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} var)', fontsize=11)\n",
    "axes[1].set_title('GMM Clusters', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM vs K-Means Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GMM vs K-MEANS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simple K-means implementation for comparison\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit K-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_kmeans = kmeans.fit_predict(X_blob)\n",
    "\n",
    "# Fit GMM\n",
    "gmm_compare = GaussianMixtureScratch(n_components=3, random_state=42)\n",
    "labels_gmm_compare = gmm_compare.fit_predict(X_blob)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# K-means\n",
    "axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=labels_kmeans, cmap='viridis',\n",
    "               s=50, alpha=0.6, edgecolors='black')\n",
    "axes[0].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "               c='red', s=200, marker='X', edgecolors='black', linewidth=2)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[0].set_title('K-Means (Hard Clustering, Spherical)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# GMM\n",
    "scatter = axes[1].scatter(X_blob[:, 0], X_blob[:, 1], c=labels_gmm_compare, \n",
    "                         cmap='viridis', s=50, alpha=0.6, edgecolors='black')\n",
    "\n",
    "for k in range(gmm_compare.n_components):\n",
    "    mean = gmm_compare.means_[k]\n",
    "    covariance = gmm_compare.covariances_[k]\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance)\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "    width, height = 2 * np.sqrt(eigenvalues)\n",
    "    \n",
    "    for n_std in [1, 2]:\n",
    "        ellipse = Ellipse(\n",
    "            mean, n_std * width, n_std * height, angle=angle,\n",
    "            facecolor='none', edgecolor=scatter.cmap(scatter.norm(k)),\n",
    "            linewidth=2, linestyle='--', alpha=0.7\n",
    "        )\n",
    "        axes[1].add_patch(ellipse)\n",
    "    \n",
    "    axes[1].scatter(mean[0], mean[1], c='red', s=200, marker='X',\n",
    "                   edgecolors='black', linewidth=2, zorder=10)\n",
    "\n",
    "axes[1].set_xlabel('Feature 1', fontsize=11)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=11)\n",
    "axes[1].set_title('GMM (Soft Clustering, Elliptical)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"  K-Means:\")\n",
    "print(\"    - Hard assignments (each point belongs to one cluster)\")\n",
    "print(\"    - Assumes spherical clusters\")\n",
    "print(\"    - Minimizes within-cluster variance\")\n",
    "print(\"\\n  GMM:\")\n",
    "print(\"    - Soft assignments (probabilistic membership)\")\n",
    "print(\"    - Can model elliptical clusters\")\n",
    "print(\"    - Maximizes likelihood\")\n",
    "print(\"    - Provides uncertainty estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Gaussian Mixture Model:**\n",
    "\n",
    "1. **Probabilistic Model**:\n",
    "   - Each cluster is a Gaussian distribution\n",
    "   - Mixture of K Gaussian components\n",
    "   - Soft assignments (probabilities)\n",
    "\n",
    "2. **EM Algorithm**:\n",
    "   - E-step: Calculate responsibilities (posterior probabilities)\n",
    "   - M-step: Update parameters (weights, means, covariances)\n",
    "   - Iterates until convergence\n",
    "   - Guarantees to increase log-likelihood\n",
    "\n",
    "3. **Model Selection**:\n",
    "   - BIC: Bayesian Information Criterion (penalizes complexity more)\n",
    "   - AIC: Akaike Information Criterion\n",
    "   - Both lower is better\n",
    "\n",
    "4. **Advantages over K-Means**:\n",
    "   - Models elliptical clusters (full covariance)\n",
    "   - Provides uncertainty (soft assignments)\n",
    "   - Probabilistic framework\n",
    "   - Better for overlapping clusters\n",
    "\n",
    "5. **Limitations**:\n",
    "   - Assumes Gaussian distributions\n",
    "   - Sensitive to initialization\n",
    "   - Computationally more expensive\n",
    "   - May converge to local optima\n",
    "   - Requires choosing number of components\n",
    "\n",
    "6. **Use Cases**:\n",
    "   - Clusters with different shapes/sizes\n",
    "   - Need uncertainty estimates\n",
    "   - Overlapping clusters\n",
    "   - Density estimation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
