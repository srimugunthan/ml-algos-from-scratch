{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation from Scratch with PyTorch-like API\n",
    "\n",
    "This notebook implements backpropagation from first principles with an API similar to PyTorch's autograd.\n",
    "\n",
    "**Learning Goals:**\n",
    "1. Understand how automatic differentiation works\n",
    "2. See the connection between math and code\n",
    "3. Build intuition for gradient flow\n",
    "4. Appreciate what PyTorch does under the hood\n",
    "\n",
    "**Key Concepts:**\n",
    "- Computational graph\n",
    "- Chain rule\n",
    "- Gradient accumulation\n",
    "- Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Starting backpropagation tutorial...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building Our Own Tensor with Autograd\n",
    "\n",
    "We'll create a `Tensor` class that:\n",
    "- Stores data (like PyTorch tensors)\n",
    "- Tracks gradients (`.grad` attribute)\n",
    "- Records operations for backprop (computational graph)\n",
    "- Has a `.backward()` method to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \"\"\"\n",
    "    A tensor that supports automatic differentiation.\n",
    "    Similar to PyTorch's torch.Tensor with requires_grad=True\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, requires_grad=False, _children=()):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: The actual data (numpy array or scalar)\n",
    "            requires_grad: Whether to track operations for gradient computation\n",
    "            _children: Parent tensors in the computational graph\n",
    "        \"\"\"\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        # Gradient will be accumulated here\n",
    "        self.grad = None\n",
    "        \n",
    "        # For building computational graph\n",
    "        self._backward = lambda: None  # Function to compute gradients\n",
    "        self._prev = set(_children)     # Parent nodes\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad})\"\n",
    "    \n",
    "    # ============================================\n",
    "    # Mathematical Operations (Forward Pass)\n",
    "    # ============================================\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication: A @ B\n",
    "        \n",
    "        Forward: out = A @ B\n",
    "        Backward: \n",
    "            dL/dA = dL/dout @ B.T\n",
    "            dL/dB = A.T @ dL/dout\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(\n",
    "            self.data @ other.data,\n",
    "            requires_grad=self.requires_grad or other.requires_grad,\n",
    "            _children=(self, other)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Chain rule: dL/dA = dL/dout @ B.T\n",
    "                grad = out.grad @ other.data.T\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            \n",
    "            if other.requires_grad:\n",
    "                # Chain rule: dL/dB = A.T @ dL/dout\n",
    "                grad = self.data.T @ out.grad\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Addition: A + B\n",
    "        \n",
    "        Forward: out = A + B\n",
    "        Backward: dL/dA = dL/dout, dL/dB = dL/dout\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(\n",
    "            self.data + other.data,\n",
    "            requires_grad=self.requires_grad or other.requires_grad,\n",
    "            _children=(self, other)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Gradient passes through unchanged\n",
    "                grad = out.grad\n",
    "                # Handle broadcasting (sum over broadcasted dimensions)\n",
    "                if self.data.shape != grad.shape:\n",
    "                    # Sum over dimensions that were broadcasted\n",
    "                    ndims_added = grad.ndim - self.data.ndim\n",
    "                    for i in range(ndims_added):\n",
    "                        grad = grad.sum(axis=0)\n",
    "                    for i, (dim_orig, dim_grad) in enumerate(zip(self.data.shape, grad.shape)):\n",
    "                        if dim_orig == 1 and dim_grad > 1:\n",
    "                            grad = grad.sum(axis=i, keepdims=True)\n",
    "                \n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            \n",
    "            if other.requires_grad:\n",
    "                grad = out.grad\n",
    "                # Handle broadcasting\n",
    "                if other.data.shape != grad.shape:\n",
    "                    ndims_added = grad.ndim - other.data.ndim\n",
    "                    for i in range(ndims_added):\n",
    "                        grad = grad.sum(axis=0)\n",
    "                    for i, (dim_orig, dim_grad) in enumerate(zip(other.data.shape, grad.shape)):\n",
    "                        if dim_orig == 1 and dim_grad > 1:\n",
    "                            grad = grad.sum(axis=i, keepdims=True)\n",
    "                \n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise multiplication: A * B\n",
    "        \n",
    "        Forward: out = A * B\n",
    "        Backward: dL/dA = dL/dout * B, dL/dB = dL/dout * A\n",
    "        \"\"\"\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(\n",
    "            self.data * other.data,\n",
    "            requires_grad=self.requires_grad or other.requires_grad,\n",
    "            _children=(self, other)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                grad = out.grad * other.data\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "            \n",
    "            if other.requires_grad:\n",
    "                grad = out.grad * self.data\n",
    "                if other.grad is None:\n",
    "                    other.grad = grad\n",
    "                else:\n",
    "                    other.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Subtraction: A - B = A + (-B)\"\"\"\n",
    "        return self + (other * -1)\n",
    "    \n",
    "    def __pow__(self, power):\n",
    "        \"\"\"\n",
    "        Power: A ** n\n",
    "        \n",
    "        Forward: out = A^n\n",
    "        Backward: dL/dA = dL/dout * n * A^(n-1)\n",
    "        \"\"\"\n",
    "        out = Tensor(\n",
    "            self.data ** power,\n",
    "            requires_grad=self.requires_grad,\n",
    "            _children=(self,)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Power rule\n",
    "                grad = out.grad * power * (self.data ** (power - 1))\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        \"\"\"\n",
    "        Sum elements\n",
    "        \n",
    "        Forward: out = sum(A)\n",
    "        Backward: dL/dA = dL/dout (broadcast to original shape)\n",
    "        \"\"\"\n",
    "        out = Tensor(\n",
    "            self.data.sum(axis=axis, keepdims=keepdims),\n",
    "            requires_grad=self.requires_grad,\n",
    "            _children=(self,)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Gradient broadcasts back to original shape\n",
    "                grad = out.grad\n",
    "                if axis is not None and not keepdims:\n",
    "                    grad = np.expand_dims(grad, axis=axis)\n",
    "                grad = np.broadcast_to(grad, self.data.shape)\n",
    "                \n",
    "                if self.grad is None:\n",
    "                    self.grad = grad.copy()\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        Mean of all elements\n",
    "        \n",
    "        Forward: out = mean(A) = sum(A) / n\n",
    "        Backward: dL/dA = dL/dout / n\n",
    "        \"\"\"\n",
    "        n = self.data.size\n",
    "        return self.sum() * (1.0 / n)\n",
    "    \n",
    "    # ============================================\n",
    "    # Activation Functions\n",
    "    # ============================================\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        Sigmoid activation: Ïƒ(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        Forward: out = Ïƒ(A)\n",
    "        Backward: dL/dA = dL/dout * Ïƒ(A) * (1 - Ïƒ(A))\n",
    "        \"\"\"\n",
    "        out = Tensor(\n",
    "            1 / (1 + np.exp(-self.data)),\n",
    "            requires_grad=self.requires_grad,\n",
    "            _children=(self,)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Derivative: Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x))\n",
    "                sigmoid_grad = out.data * (1 - out.data)\n",
    "                grad = out.grad * sigmoid_grad\n",
    "                \n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        ReLU activation: max(0, x)\n",
    "        \n",
    "        Forward: out = max(0, A)\n",
    "        Backward: dL/dA = dL/dout * (A > 0)\n",
    "        \"\"\"\n",
    "        out = Tensor(\n",
    "            np.maximum(0, self.data),\n",
    "            requires_grad=self.requires_grad,\n",
    "            _children=(self,)\n",
    "        )\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                # Gradient is 1 where input > 0, else 0\n",
    "                grad = out.grad * (self.data > 0)\n",
    "                \n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad += grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # ============================================\n",
    "    # Backpropagation\n",
    "    # ============================================\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Compute gradients via backpropagation.\n",
    "        Similar to PyTorch's loss.backward()\n",
    "        \n",
    "        This builds a topological ordering of the computational graph\n",
    "        and calls _backward() on each node in reverse order.\n",
    "        \"\"\"\n",
    "        # Build topological order (reverse of forward pass)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        \n",
    "        build_topo(self)\n",
    "        \n",
    "        # Initialize gradient of output (this tensor) to 1\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        \n",
    "        # Backpropagate through graph in reverse topological order\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reset gradients to None (like PyTorch's optimizer.zero_grad())\"\"\"\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "print(\"âœ“ Tensor class with autograd created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Test Basic Operations\n",
    "\n",
    "Let's verify our implementation works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple scalar operations\n",
    "print(\"Test 1: Scalar Operations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create tensors\n",
    "a = Tensor(2.0, requires_grad=True)\n",
    "b = Tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Forward pass: c = a * b + a\n",
    "c = a * b + a\n",
    "print(f\"a = {a.data}, b = {b.data}\")\n",
    "print(f\"c = a * b + a = {c.data}\")\n",
    "\n",
    "# Backward pass\n",
    "c.backward()\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"dc/da = {a.grad} (expected: b + 1 = 4.0)\")\n",
    "print(f\"dc/db = {b.grad} (expected: a = 2.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Matrix multiplication\n",
    "print(\"\\nTest 2: Matrix Multiplication\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create matrices\n",
    "X = Tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "W = Tensor([[0.5, 0.5], [0.5, 0.5]], requires_grad=True)\n",
    "\n",
    "# Forward: Y = X @ W\n",
    "Y = X @ W\n",
    "print(f\"X shape: {X.data.shape}\")\n",
    "print(f\"W shape: {W.data.shape}\")\n",
    "print(f\"Y = X @ W:\\n{Y.data}\")\n",
    "\n",
    "# Loss = mean(Y)\n",
    "loss = Y.mean()\n",
    "print(f\"\\nLoss (mean of Y): {loss.data}\")\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n",
    "print(f\"\\nGradient of X:\\n{X.grad}\")\n",
    "print(f\"\\nGradient of W:\\n{W.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Sigmoid activation\n",
    "print(\"\\nTest 3: Sigmoid Activation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = Tensor([0., 1., -1., 2.], requires_grad=True)\n",
    "y = x.sigmoid()\n",
    "\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"sigmoid(x) = {y.data}\")\n",
    "\n",
    "# Compute gradient\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradient of x: {x.grad}\")\n",
    "print(f\"Expected: sigmoid(x) * (1 - sigmoid(x))\")\n",
    "print(f\"Computed: {y.data * (1 - y.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build a Neural Network Class\n",
    "\n",
    "Now let's create a simple neural network using our Tensor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network.\n",
    "    Similar to PyTorch's nn.Module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize network parameters.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            hidden_size: Number of hidden neurons\n",
    "            output_size: Number of output neurons\n",
    "        \"\"\"\n",
    "        # Xavier initialization for better training\n",
    "        # Scale by sqrt(1/input_size) to keep variance stable\n",
    "        \n",
    "        self.W1 = Tensor(\n",
    "            np.random.randn(input_size, hidden_size) * np.sqrt(1.0 / input_size),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.b1 = Tensor(np.zeros((1, hidden_size)), requires_grad=True)\n",
    "        \n",
    "        self.W2 = Tensor(\n",
    "            np.random.randn(hidden_size, output_size) * np.sqrt(1.0 / hidden_size),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.b2 = Tensor(np.zeros((1, output_size)), requires_grad=True)\n",
    "        \n",
    "        print(f\"Network architecture: {input_size} -> {hidden_size} -> {output_size}\")\n",
    "        print(f\"Total parameters: {self.count_parameters()}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            X: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Hidden layer: h = sigmoid(X @ W1 + b1)\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.h = self.z1.sigmoid()\n",
    "        \n",
    "        # Output layer: y = sigmoid(h @ W2 + b2)\n",
    "        self.z2 = self.h @ self.W2 + self.b2\n",
    "        self.y = self.z2.sigmoid()\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return list of all parameters (like PyTorch's model.parameters())\"\"\"\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of parameters\"\"\"\n",
    "        return sum(p.data.size for p in self.parameters())\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out all gradients (like PyTorch's optimizer.zero_grad())\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.zero_grad()\n",
    "\n",
    "\n",
    "print(\"âœ“ Neural Network class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Create Dataset (XOR Problem)\n",
    "\n",
    "XOR is a classic problem that requires non-linearity to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XOR dataset\n",
    "X_data = np.array([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "\n",
    "y_data = np.array([\n",
    "    [0.],  # 0 XOR 0 = 0\n",
    "    [1.],  # 0 XOR 1 = 1\n",
    "    [1.],  # 1 XOR 0 = 1\n",
    "    [0.]   # 1 XOR 1 = 0\n",
    "])\n",
    "\n",
    "# Convert to our Tensor class\n",
    "X = Tensor(X_data)\n",
    "y = Tensor(y_data)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(len(X_data)):\n",
    "    print(f\"Input: {X_data[i]} -> Output: {y_data[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Loop\n",
    "\n",
    "Now we'll train our network using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "np.random.seed(42)\n",
    "model = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1.0\n",
    "n_epochs = 2000\n",
    "\n",
    "# Track loss history\n",
    "loss_history = []\n",
    "\n",
    "print(\"\\nTraining started...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model.forward(X)\n",
    "    \n",
    "    # Compute loss: MSE = mean((y_pred - y)^2)\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    loss_history.append(loss.data.item())\n",
    "    \n",
    "    # Zero gradients from previous iteration\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters using gradient descent\n",
    "    # Î¸_new = Î¸_old - learning_rate * gradient\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.data.item():.6f}\")\n",
    "\n",
    "print(\"\\nâœ“ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions\n",
    "y_pred_final = model.forward(X)\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(len(X_data)):\n",
    "    pred = y_pred_final.data[i, 0]\n",
    "    true = y_data[i, 0]\n",
    "    correct = \"âœ“\" if round(pred) == true else \"âœ—\"\n",
    "    print(f\"Input: {X_data[i]} | Predicted: {pred:.4f} | True: {true:.0f} | {correct}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions_rounded = np.round(y_pred_final.data)\n",
    "accuracy = (predictions_rounded == y_data).mean()\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.1f}%\")\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(loss_history, linewidth=2, color='#2E86AB')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "# Create mesh grid\n",
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Predict for each point in grid\n",
    "grid_points = Tensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "grid_pred = model.forward(grid_points)\n",
    "grid_pred = grid_pred.data.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "contour = axes[1].contourf(xx, yy, grid_pred, levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "plt.colorbar(contour, ax=axes[1], label='Predicted Value')\n",
    "\n",
    "# Plot training points\n",
    "colors = ['#A23B72' if label == 0 else '#F18F01' for label in y_data[:, 0]]\n",
    "axes[1].scatter(X_data[:, 0], X_data[:, 1], c=colors, s=200, \n",
    "                edgecolors='black', linewidth=2, zorder=10)\n",
    "axes[1].set_xlabel('Input 1', fontsize=12)\n",
    "axes[1].set_ylabel('Input 2', fontsize=12)\n",
    "axes[1].set_title('Decision Boundary (XOR)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Compare with PyTorch\n",
    "\n",
    "Let's verify our implementation matches PyTorch's autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create PyTorch version\n",
    "class PyTorchNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize with same random seed\n",
    "torch.manual_seed(42)\n",
    "pytorch_model = PyTorchNet(2, 4, 1)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_torch = torch.FloatTensor(X_data)\n",
    "y_torch = torch.FloatTensor(y_data)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.SGD(pytorch_model.parameters(), lr=1.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "pytorch_loss_history = []\n",
    "\n",
    "print(\"Training PyTorch model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    # Forward pass\n",
    "    y_pred_torch = pytorch_model(X_torch)\n",
    "    loss_torch = criterion(y_pred_torch, y_torch)\n",
    "    pytorch_loss_history.append(loss_torch.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss_torch.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/2000, Loss: {loss_torch.item():.6f}\")\n",
    "\n",
    "print(\"\\nâœ“ PyTorch training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\nComparison: Our Implementation vs PyTorch\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Our final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"PyTorch final loss: {pytorch_loss_history[-1]:.6f}\")\n",
    "print(f\"Difference: {abs(loss_history[-1] - pytorch_loss_history[-1]):.6f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Our Implementation', linewidth=2, alpha=0.8)\n",
    "plt.plot(pytorch_loss_history, label='PyTorch Autograd', \n",
    "         linewidth=2, linestyle='--', alpha=0.8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Our Implementation vs PyTorch', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Both implementations converge to similar solutions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "1. **Tensor class with autograd**: Automatically tracks operations and computes gradients\n",
    "2. **Computational graph**: Built dynamically during forward pass\n",
    "3. **Backpropagation**: Chain rule applied recursively through the graph\n",
    "4. **Neural network**: Using our custom tensors with PyTorch-like API\n",
    "\n",
    "### Core Concepts\n",
    "1. **Forward Pass**: Data flows through operations, building a graph\n",
    "2. **Backward Pass**: Gradients flow backwards using chain rule\n",
    "3. **Gradient Accumulation**: Gradients add up for nodes used multiple times\n",
    "4. **Parameter Updates**: Move opposite to gradient direction\n",
    "\n",
    "### API Similarities with PyTorch\n",
    "- `Tensor(data, requires_grad=True)` â†” `torch.tensor(data, requires_grad=True)`\n",
    "- `.backward()` â†” `.backward()`\n",
    "- `.zero_grad()` â†” `.zero_grad()`\n",
    "- Matrix operations: `@`, `+`, `*` work the same\n",
    "- Activations: `.sigmoid()`, `.relu()`\n",
    "\n",
    "### Why This Matters\n",
    "- **PyTorch does this automatically**: Our manual implementation shows what's happening\n",
    "- **Debugging intuition**: Understanding autograd helps debug gradient issues\n",
    "- **Foundation for deep learning**: All modern frameworks use similar principles\n",
    "- **Extensibility**: You can create custom operations by defining forward + backward\n",
    "\n",
    "This is the foundation of deep learning - just scaled to larger networks! ðŸŽ“"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
