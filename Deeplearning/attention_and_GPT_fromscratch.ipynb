{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Attention Mechanism and GPT Transformer\n",
    "\n",
    "## A Complete Educational Guide\n",
    "\n",
    "This notebook provides a barebones implementation of:\n",
    "1. **Attention Mechanism** - The core innovation\n",
    "2. **GPT-style Decoder-Only Transformer** - For language modeling\n",
    "\n",
    "### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Scaled Dot-Product Attention](#attention)\n",
    "3. [Multi-Head Attention](#multihead)\n",
    "4. [Positional Encoding](#positional)\n",
    "5. [Layer Normalization](#layernorm)\n",
    "6. [Feed-Forward Networks](#ffn)\n",
    "7. [Residual Connections](#residual)\n",
    "8. [Complete Decoder Block](#decoder)\n",
    "9. [Full GPT Model](#gpt)\n",
    "10. [Training Example](#training)\n",
    "11. [Text Generation](#generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "### What is a Transformer?\n",
    "\n",
    "The Transformer architecture, introduced in \"Attention is All You Need\" (2017), revolutionized NLP by replacing recurrence with **attention mechanisms**.\n",
    "\n",
    "### Key Innovation: Attention\n",
    "\n",
    "Instead of processing sequences sequentially (like RNNs), transformers process all positions **simultaneously** and let each position **attend** to all other positions.\n",
    "\n",
    "### GPT vs BERT\n",
    "\n",
    "- **BERT**: Encoder-only (bidirectional, for understanding)\n",
    "- **GPT**: Decoder-only (unidirectional, for generation)\n",
    "- **Original Transformer**: Encoder-decoder (for translation)\n",
    "\n",
    "We'll implement GPT-style decoder-only transformer for language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Scaled Dot-Product Attention <a id='attention'></a>\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Attention answers: \"Given a query, which values should I focus on?\"\n",
    "\n",
    "**Analogy**: You're in a library (keys/values) looking for information (query):\n",
    "1. You scan book titles (keys) to see which match your query\n",
    "2. You calculate relevance scores (attention weights)\n",
    "3. You read the most relevant books (weighted sum of values)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = Query (what we're looking for)\n",
    "- $K$ = Key (what we're comparing against)\n",
    "- $V$ = Value (what we want to retrieve)\n",
    "- $d_k$ = Dimension of keys (for scaling)\n",
    "\n",
    "### Why Scale by $\\sqrt{d_k}$?\n",
    "\n",
    "Without scaling, for large $d_k$, the dot products grow large in magnitude, pushing softmax into regions with very small gradients (saturation). Scaling keeps the variance stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    The fundamental attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch, seq_len, d_k) - What we're looking for\n",
    "        key:   (batch, seq_len, d_k) - What we're comparing against\n",
    "        value: (batch, seq_len, d_k) - What we want to retrieve\n",
    "        mask:  (batch, seq_len, seq_len) - Optional masking (for causal attention)\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, seq_len, d_k)\n",
    "        attention_weights: (batch, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)  # Dimension of key/query\n",
    "    \n",
    "    # Step 1: Calculate attention scores\n",
    "    # For each position i, calculate similarity with all positions j\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Step 5: Apply attention weights to values\n",
    "    output = torch.matmul(attention_weights, value)  # (batch, seq_len, d_k)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention\n",
    "\n",
    "Let's see attention in action with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple attention\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "\n",
    "# Create random Q, K, V\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# Apply attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "print(f\"  Query: {Q.shape}\")\n",
    "print(f\"  Key:   {K.shape}\")\n",
    "print(f\"  Value: {V.shape}\")\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {attn_weights.shape}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Attention Weights (How much each position attends to others)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Each row shows how much that position attends to all other positions.\")\n",
    "print(\"Rows sum to 1.0 (probability distribution).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Masking\n",
    "\n",
    "For **autoregressive** generation (like GPT), we need **causal masking**: position $i$ can only attend to positions $\\leq i$.\n",
    "\n",
    "This prevents the model from \"cheating\" by looking at future tokens during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create causal mask\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Lower triangular matrix - position i can only see positions <= i\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# Example\n",
    "seq_len = 5\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask (1 = can attend, 0 = cannot attend):\")\n",
    "print(mask.numpy().astype(int))\n",
    "\n",
    "# Apply attention with causal mask\n",
    "output_masked, attn_weights_masked = scaled_dot_product_attention(Q, K, V, mask=mask.unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), annot=True, fmt='.2f', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Without Causal Mask (can see future)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "\n",
    "sns.heatmap(attn_weights_masked[0].detach().numpy(), annot=True, fmt='.2f', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('With Causal Mask (cannot see future)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: With causal mask, upper triangle is zero (cannot attend to future tokens).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Multi-Head Attention <a id='multihead'></a>\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "Single attention allows the model to focus on one aspect. Multiple heads allow the model to attend to different aspects **simultaneously**:\n",
    "\n",
    "- **Head 1**: Syntactic relationships (subject-verb agreement)\n",
    "- **Head 2**: Semantic relationships (word meaning)\n",
    "- **Head 3**: Positional relationships (word order)\n",
    "- etc.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Instead of one attention with dimension $d_{model}$, we use $h$ heads with dimension $d_k = d_{model}/h$ each:\n",
    "\n",
    "```\n",
    "Input (d_model=512)\n",
    "    ↓\n",
    "Linear projections (Q, K, V)\n",
    "    ↓\n",
    "Split into h=8 heads (each d_k=64)\n",
    "    ↓\n",
    "Parallel attention for each head\n",
    "    ↓\n",
    "Concatenate heads\n",
    "    ↓\n",
    "Final linear projection\n",
    "    ↓\n",
    "Output (d_model=512)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention allows the model to attend to different aspects\n",
    "    of the sequence simultaneously.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, d_k)\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)  # (batch, num_heads, seq_len, d_k)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Inverse of split_heads\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        x = x.transpose(1, 2)  # (batch, seq_len, num_heads, d_k)\n",
    "        return x.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # Step 1: Linear projections\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # Step 2: Split into multiple heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Step 3: Apply attention for each head\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Step 4: Concatenate heads\n",
    "        output = self.combine_heads(attn_output)\n",
    "        \n",
    "        # Step 5: Final linear projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "batch_size = 1\n",
    "seq_len = 6\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mask = create_causal_mask(seq_len).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "output, attn_weights = mha(x, x, x, mask)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch, num_heads, seq_len, seq_len)\")\n",
    "\n",
    "# Visualize attention for different heads\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "for i in range(num_heads):\n",
    "    sns.heatmap(attn_weights[0, i].detach().numpy(), annot=True, fmt='.2f', \n",
    "                cmap='Blues', ax=axes[i], cbar=False)\n",
    "    axes[i].set_title(f'Head {i+1}')\n",
    "    axes[i].set_xlabel('Key')\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Attention Patterns Across Different Heads', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Different heads learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Positional Encoding <a id='positional'></a>\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Attention has **no notion of position**. It treats the sequence as a set, not a sequence:\n",
    "- \"The cat sat on the mat\" ≈ \"mat the on sat cat The\"\n",
    "\n",
    "This is bad for language! Word order matters.\n",
    "\n",
    "### The Solution: Positional Encoding\n",
    "\n",
    "Add position information to the embeddings using sine/cosine functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $pos$ = position in sequence (0, 1, 2, ...)\n",
    "- $i$ = dimension index (0, 1, 2, ..., d_model/2)\n",
    "\n",
    "### Why Sine/Cosine?\n",
    "\n",
    "1. **Different frequencies** for different dimensions (from high to low)\n",
    "2. **Deterministic** (no learning required)\n",
    "3. **Extrapolation** to longer sequences\n",
    "4. **Relative positions** can be expressed as linear combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds positional information to embeddings using sine/cosine functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encodings\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "pos_enc = PositionalEncoding(d_model, max_len)\n",
    "pe_values = pos_enc.pe[0, :max_len, :].numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot 1: Heatmap of positional encodings\n",
    "im = axes[0].imshow(pe_values.T, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[0].set_xlabel('Position in Sequence')\n",
    "axes[0].set_ylabel('Embedding Dimension')\n",
    "axes[0].set_title('Positional Encoding Matrix')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Plot 2: Encoding values for specific positions\n",
    "positions_to_plot = [0, 10, 30, 60]\n",
    "for pos in positions_to_plot:\n",
    "    axes[1].plot(pe_values[pos, :64], label=f'Position {pos}', alpha=0.7)\n",
    "axes[1].set_xlabel('Embedding Dimension (first 64)')\n",
    "axes[1].set_ylabel('Encoding Value')\n",
    "axes[1].set_title('Positional Encodings for Different Positions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. Different dimensions have different frequencies (wavelengths)\")\n",
    "print(\"2. Lower dimensions = higher frequency (change rapidly)\")\n",
    "print(\"3. Higher dimensions = lower frequency (change slowly)\")\n",
    "print(\"4. Each position has a unique encoding pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Layer Normalization <a id='layernorm'></a>\n",
    "\n",
    "### What is Layer Normalization?\n",
    "\n",
    "Layer normalization normalizes inputs across the **feature dimension** (not the batch dimension like Batch Norm).\n",
    "\n",
    "For each sample:\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean across features\n",
    "- $\\sigma^2$ = variance across features\n",
    "- $\\gamma, \\beta$ = learnable parameters (scale and shift)\n",
    "- $\\epsilon$ = small constant for numerical stability\n",
    "\n",
    "### Why Layer Norm?\n",
    "\n",
    "1. **Stabilizes training** - prevents internal covariate shift\n",
    "2. **Speeds up convergence** - allows higher learning rates\n",
    "3. **Works with variable batch sizes** (unlike Batch Norm)\n",
    "4. **Reduces gradient vanishing** in deep networks\n",
    "\n",
    "### Pre-LN vs Post-LN\n",
    "\n",
    "**Post-LN** (Original Transformer):\n",
    "```\n",
    "x = LayerNorm(x + Sublayer(x))\n",
    "```\n",
    "\n",
    "**Pre-LN** (Modern, more stable):\n",
    "```\n",
    "x = x + Sublayer(LayerNorm(x))\n",
    "```\n",
    "\n",
    "We use Pre-LN as it's more stable and easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Layer Normalization\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "# Random input with varying scales\n",
    "x = torch.randn(batch_size, seq_len, d_model) * torch.tensor([0.1, 1.0, 10.0, 0.5, 2.0, 5.0, 0.2, 8.0])\n",
    "\n",
    "# Apply layer norm\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "x_normalized = layer_norm(x)\n",
    "\n",
    "print(\"Before Layer Norm:\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"Mean per sample: {x[0].mean():.4f}, {x[1].mean():.4f}\")\n",
    "print(f\"Std per sample:  {x[0].std():.4f}, {x[1].std():.4f}\")\n",
    "print(f\"\\nFirst sample (one position):\")\n",
    "print(x[0, 0].detach().numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAfter Layer Norm:\")\n",
    "print(f\"Shape: {x_normalized.shape}\")\n",
    "print(f\"Mean per sample: {x_normalized[0].mean():.4f}, {x_normalized[1].mean():.4f}\")\n",
    "print(f\"Std per sample:  {x_normalized[0].std():.4f}, {x_normalized[1].std():.4f}\")\n",
    "print(f\"\\nFirst sample (one position):\")\n",
    "print(x_normalized[0, 0].detach().numpy())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].boxplot([x[0, :, i].detach().numpy() for i in range(d_model)])\n",
    "axes[0].set_title('Before Layer Norm (varying scales)')\n",
    "axes[0].set_xlabel('Feature Dimension')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot([x_normalized[0, :, i].detach().numpy() for i in range(d_model)])\n",
    "axes[1].set_title('After Layer Norm (normalized)')\n",
    "axes[1].set_xlabel('Feature Dimension')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Point: Layer Norm makes all features have similar scales!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feed-Forward Networks <a id='ffn'></a>\n",
    "\n",
    "### What is the Feed-Forward Network?\n",
    "\n",
    "After attention, each position passes through a simple 2-layer MLP:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "- **Layer 1**: Expand to higher dimension (typically $4 \\times d_{model}$)\n",
    "- **ReLU**: Non-linear activation\n",
    "- **Layer 2**: Project back to $d_{model}$\n",
    "\n",
    "### Why FFN?\n",
    "\n",
    "1. **Non-linearity**: Attention is linear operations; FFN adds non-linearity\n",
    "2. **Feature transformation**: Processes information gathered by attention\n",
    "3. **Position-wise**: Applied independently to each position\n",
    "4. **Capacity**: Most parameters are in FFN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 2-layer MLP applied to each position independently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = F.relu(self.linear1(x))  # (batch, seq_len, d_ff)\n",
    "        x = self.linear2(x)           # (batch, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Feed-Forward Network\n",
    "d_model = 64\n",
    "d_ff = 256  # Typically 4x d_model\n",
    "\n",
    "ffn = FeedForward(d_model, d_ff)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nFFN expands to d_ff={d_ff}, then projects back to d_model={d_model}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"\\nTotal parameters in FFN: {total_params:,}\")\n",
    "print(f\"  Linear1 (d_model → d_ff): {d_model * d_ff + d_ff:,}\")\n",
    "print(f\"  Linear2 (d_ff → d_model): {d_ff * d_model + d_model:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Residual Connections <a id='residual'></a>\n",
    "\n",
    "### What are Residual Connections?\n",
    "\n",
    "Instead of:\n",
    "```\n",
    "x = Layer(x)\n",
    "```\n",
    "\n",
    "We do:\n",
    "```\n",
    "x = x + Layer(x)  # Residual connection\n",
    "```\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "1. **Gradient flow**: Direct path for gradients to earlier layers\n",
    "2. **Deep networks**: Enables training very deep models (100+ layers)\n",
    "3. **Identity mapping**: Layer can learn to do nothing (identity)\n",
    "4. **Stable training**: Prevents gradient vanishing\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The layer only needs to learn the **residual** (difference) from the input, not the entire transformation. This is often easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate residual connections\n",
    "def simulate_deep_network(depth, use_residual=True):\n",
    "    \"\"\"Simulate gradient flow through a deep network\"\"\"\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    for _ in range(depth):\n",
    "        if use_residual:\n",
    "            # With residual: gradient flows through both paths\n",
    "            gradient = gradient * 0.5 + gradient  # Simplified model\n",
    "        else:\n",
    "            # Without residual: gradient only flows through layers\n",
    "            gradient = gradient * 0.5  # Gradient vanishing!\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Compare with and without residual connections\n",
    "depth = 50\n",
    "grads_with_residual = simulate_deep_network(depth, use_residual=True)\n",
    "grads_without_residual = simulate_deep_network(depth, use_residual=False)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(grads_without_residual, label='Without Residual (Vanishing!)', linewidth=2)\n",
    "plt.plot(grads_with_residual, label='With Residual (Stable)', linewidth=2)\n",
    "plt.xlabel('Layer Depth')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.title('Gradient Flow: With vs Without Residual Connections')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(f\"Without residual: gradient at layer {depth} = {grads_without_residual[-1]:.2e}\")\n",
    "print(f\"With residual:    gradient at layer {depth} = {grads_with_residual[-1]:.2e}\")\n",
    "print(\"\\nResidual connections prevent gradient vanishing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Complete Decoder Block <a id='decoder'></a>\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A decoder block combines everything we've learned:\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "Layer Norm\n",
    "  ↓\n",
    "Multi-Head Self-Attention (with causal mask)\n",
    "  ↓\n",
    "Residual Connection (+)\n",
    "  ↓\n",
    "Layer Norm\n",
    "  ↓\n",
    "Feed-Forward Network\n",
    "  ↓\n",
    "Residual Connection (+)\n",
    "  ↓\n",
    "Output\n",
    "```\n",
    "\n",
    "This is **Pre-LN** architecture (Layer Norm before sub-layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single decoder block: Attention + FFN with residual connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization (Pre-LN)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Sub-layer 1: Self-Attention\n",
    "        # Pre-LN: Normalize before attention\n",
    "        attn_output, _ = self.self_attention(\n",
    "            self.norm1(x), self.norm1(x), self.norm1(x), mask\n",
    "        )\n",
    "        x = x + self.dropout(attn_output)  # Residual connection\n",
    "        \n",
    "        # Sub-layer 2: Feed-Forward\n",
    "        # Pre-LN: Normalize before FFN\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(ff_output)  # Residual connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Decoder Block\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "decoder_block = DecoderBlock(d_model, num_heads, d_ff)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mask = create_causal_mask(seq_len).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "output = decoder_block(x, mask)\n",
    "\n",
    "print(f\"Input shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nShape is preserved through the decoder block!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in decoder_block.parameters())\n",
    "print(f\"\\nTotal parameters in one decoder block: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Full GPT Model <a id='gpt'></a>\n",
    "\n",
    "### Complete Architecture\n",
    "\n",
    "```\n",
    "Input Tokens\n",
    "    ↓\n",
    "Token Embedding\n",
    "    ↓\n",
    "Positional Encoding\n",
    "    ↓\n",
    "Decoder Block 1\n",
    "    ↓\n",
    "Decoder Block 2\n",
    "    ↓\n",
    "    ...\n",
    "    ↓\n",
    "Decoder Block N\n",
    "    ↓\n",
    "Layer Norm\n",
    "    ↓\n",
    "Linear Projection (to vocabulary)\n",
    "    ↓\n",
    "Output Logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-style Decoder-Only Transformer for language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, \n",
    "                 d_ff=2048, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Stack of decoder blocks\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with small random values\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len) - Token indices\n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Token embedding\n",
    "        x = self.token_embedding(x)\n",
    "        x = x * math.sqrt(self.d_model)  # Scale embeddings\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = self.create_causal_mask(seq_len).to(x.device)\n",
    "        \n",
    "        # Pass through decoder blocks\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, mask)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPT model\n",
    "vocab_size = 10000\n",
    "model = GPTTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPT TRANSFORMER MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Vocabulary Size: {vocab_size:,}\")\n",
    "print(f\"  Model Dimension: 256\")\n",
    "print(f\"  Number of Heads: 8\")\n",
    "print(f\"  Number of Layers: 4\")\n",
    "print(f\"  Feed-Forward Dimension: 1024\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Shape: {input_tokens.shape}\")\n",
    "print(f\"  Sample tokens: {input_tokens[0, :10].tolist()}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens)\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Shape: {logits.shape}\")\n",
    "print(f\"  (batch_size, seq_len, vocab_size)\")\n",
    "\n",
    "# Get predictions\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(f\"\\nPredicted tokens: {predictions[0, :10].tolist()}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Size:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Training Example <a id='training'></a>\n",
    "\n",
    "Let's create a simple training loop for a toy task: learning to copy sequences.\n",
    "\n",
    "### Task: Sequence Copying\n",
    "\n",
    "Given input: `[1, 2, 3, 4, 5]`\n",
    "Predict: `[1, 2, 3, 4, 5]` (shifted by one position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple toy dataset\n",
    "def create_toy_dataset(vocab_size, num_samples, seq_len):\n",
    "    \"\"\"Create random sequences for training\"\"\"\n",
    "    data = torch.randint(1, vocab_size, (num_samples, seq_len))\n",
    "    return data\n",
    "\n",
    "# Training configuration\n",
    "vocab_size = 100\n",
    "num_samples = 1000\n",
    "seq_len = 20\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Create dataset\n",
    "train_data = create_toy_dataset(vocab_size, num_samples, seq_len)\n",
    "\n",
    "# Create smaller model for faster training\n",
    "model = GPTTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_ff=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Vocabulary Size: {vocab_size}\")\n",
    "print(f\"  Training Samples: {num_samples}\")\n",
    "print(f\"  Sequence Length: {seq_len}\")\n",
    "print(f\"  Batch Size: {batch_size}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\nStarting training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(train_data) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Get batch\n",
    "        batch = train_data[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        # Input and target (shifted by one position)\n",
    "        input_seq = batch[:, :-1]  # All except last\n",
    "        target_seq = batch[:, 1:]  # All except first\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_seq)\n",
    "        \n",
    "        # Calculate loss\n",
    "        # Reshape for cross-entropy: (batch * seq_len, vocab_size)\n",
    "        logits_flat = logits.reshape(-1, vocab_size)\n",
    "        target_flat = target_seq.reshape(-1)\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial Loss: {losses[0]:.4f}\")\n",
    "print(f\"Final Loss: {losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(1 - losses[-1]/losses[0]) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Text Generation <a id='generation'></a>\n",
    "\n",
    "### Autoregressive Generation\n",
    "\n",
    "GPT generates text one token at a time:\n",
    "\n",
    "1. Start with a prompt (seed text)\n",
    "2. Feed prompt through model → get next token probabilities\n",
    "3. Sample next token\n",
    "4. Append to prompt\n",
    "5. Repeat\n",
    "\n",
    "### Sampling Strategies\n",
    "\n",
    "- **Greedy**: Always pick highest probability token (deterministic)\n",
    "- **Temperature Sampling**: Scale logits before softmax (higher = more random)\n",
    "- **Top-k Sampling**: Sample from top k tokens\n",
    "- **Top-p (Nucleus) Sampling**: Sample from smallest set with cumulative prob ≥ p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_tokens, max_length=50, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text autoregressively.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT model\n",
    "        start_tokens: Initial tokens (1D tensor)\n",
    "        max_length: Maximum length to generate\n",
    "        temperature: Sampling temperature (higher = more random)\n",
    "        top_k: If set, sample from top k tokens only\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = start_tokens.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_tokens)):\n",
    "            # Get predictions for current sequence\n",
    "            logits = model(generated.unsqueeze(0))[0, -1, :]  # Last position\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k is not None:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits = torch.full_like(logits, float('-inf'))\n",
    "                logits[top_k_indices] = top_k_logits\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated = torch.cat([generated, next_token])\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "model.eval()\n",
    "\n",
    "# Create a simple prompt\n",
    "prompt = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Text Generation Demo\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPrompt tokens: {prompt.tolist()}\")\n",
    "print(\"\\nGenerating with different settings:\\n\")\n",
    "\n",
    "# Generate with different temperatures\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    generated = generate_text(model, prompt, max_length=30, temperature=temp)\n",
    "    print(f\"Temperature {temp}: {generated.tolist()[:30]}\")\n",
    "\n",
    "print(\"\\nNote: Lower temperature = more conservative, Higher temperature = more random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Attention Mechanism**\n",
    "   - Allows each position to look at all others\n",
    "   - Scaled dot-product: $\\text{Attention}(Q,K,V) = \\text{softmax}(QK^T/\\sqrt{d_k})V$\n",
    "   - Causal masking for autoregressive generation\n",
    "\n",
    "2. **Multi-Head Attention**\n",
    "   - Multiple attention mechanisms in parallel\n",
    "   - Each head learns different patterns\n",
    "   - Concatenate and project outputs\n",
    "\n",
    "3. **Positional Encoding**\n",
    "   - Injects position information using sine/cosine\n",
    "   - Different frequencies for different dimensions\n",
    "   - Allows model to understand word order\n",
    "\n",
    "4. **Layer Normalization**\n",
    "   - Normalizes activations across features\n",
    "   - Stabilizes training of deep networks\n",
    "   - Pre-LN is more stable than Post-LN\n",
    "\n",
    "5. **Feed-Forward Networks**\n",
    "   - Simple 2-layer MLP\n",
    "   - Adds non-linearity\n",
    "   - Applied position-wise\n",
    "\n",
    "6. **Residual Connections**\n",
    "   - Direct gradient flow\n",
    "   - Enables very deep networks\n",
    "   - $x = x + \\text{Layer}(x)$\n",
    "\n",
    "### Why GPT Works\n",
    "\n",
    "- **Scalability**: Can train on massive datasets\n",
    "- **Parallelization**: All positions processed simultaneously\n",
    "- **Context**: Can attend to entire sequence\n",
    "- **Flexibility**: Same architecture for many tasks\n",
    "\n",
    "### Modern Improvements\n",
    "\n",
    "- **Larger models**: GPT-3 (175B params), GPT-4 (rumored 1.7T)\n",
    "- **Better tokenization**: BPE, SentencePiece\n",
    "- **Efficient attention**: Flash Attention, Sparse Attention\n",
    "- **Architecture tweaks**: RoPE, GQA, MoE\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "1. **Original Papers**:\n",
    "   - \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "   - \"Improving Language Understanding by Generative Pre-Training\" (Radford et al., 2018)\n",
    "\n",
    "2. **Resources**:\n",
    "   - The Illustrated Transformer (Jay Alammar)\n",
    "   - Andrej Karpathy's \"Let's build GPT\" video\n",
    "   - HuggingFace Transformers library\n",
    "\n",
    "3. **Advanced Topics**:\n",
    "   - Flash Attention for efficiency\n",
    "   - Rotary Position Embeddings (RoPE)\n",
    "   - Group Query Attention (GQA)\n",
    "   - Mixture of Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Try these to deepen your understanding:\n",
    "\n",
    "1. **Modify the attention mechanism**:\n",
    "   - Implement local attention (only attend to nearby positions)\n",
    "   - Try different attention patterns\n",
    "\n",
    "2. **Experiment with architecture**:\n",
    "   - Change number of heads\n",
    "   - Try different d_model/d_ff ratios\n",
    "   - Implement Post-LN instead of Pre-LN\n",
    "\n",
    "3. **Training improvements**:\n",
    "   - Add learning rate scheduling\n",
    "   - Implement gradient clipping\n",
    "   - Try different optimizers\n",
    "\n",
    "4. **Generation strategies**:\n",
    "   - Implement beam search\n",
    "   - Try nucleus (top-p) sampling\n",
    "   - Add repetition penalty\n",
    "\n",
    "5. **Visualization**:\n",
    "   - Visualize attention patterns on real text\n",
    "   - Plot embedding space with t-SNE\n",
    "   - Analyze learned positional encodings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
