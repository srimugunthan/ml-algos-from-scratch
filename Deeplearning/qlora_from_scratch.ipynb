{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA (Quantized Low-Rank Adaptation) Implementation from Scratch\n",
    "\n",
    "## Educational Notebook - Ultra Minimal Resource Requirements\n",
    "\n",
    "This notebook implements QLoRA from first principles, combining 4-bit quantization with LoRA for extreme memory efficiency.\n",
    "\n",
    "**Resource Requirements:**\n",
    "- RAM: 4GB minimum\n",
    "- GPU: 4GB VRAM (or CPU)\n",
    "- Storage: ~300MB (quantized model)\n",
    "\n",
    "**What is QLoRA?**\n",
    "\n",
    "QLoRA extends LoRA by:\n",
    "1. **4-bit NormalFloat (NF4) quantization** of base model weights\n",
    "2. **Double quantization** of quantization constants\n",
    "3. **Paged optimizers** for efficient memory management\n",
    "4. **LoRA adapters** trained in full precision (16-bit)\n",
    "\n",
    "**Key Innovation:**\n",
    "```\n",
    "Base Model: Quantized to 4-bit (W_4bit)\n",
    "LoRA: Full precision adapters (B, A in 16-bit)\n",
    "Forward: Dequantize(W_4bit) + BA\n",
    "Backward: Only update B and A\n",
    "```\n",
    "\n",
    "**Memory Reduction:**\n",
    "- FP16 model: 16 bits/param\n",
    "- 4-bit quantized: 4 bits/param (4√ó reduction)\n",
    "- With LoRA: Train <1% of params\n",
    "- **Result: ~65√ó less memory than full fine-tuning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch datasets accelerate bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if bitsandbytes is available\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"bitsandbytes version: {bnb.__version__}\")\n",
    "    print(\"‚úì 4-bit quantization available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† bitsandbytes not available - will use manual quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Quantization\n",
    "\n",
    "### 4-bit NormalFloat (NF4) Quantization\n",
    "\n",
    "NF4 is designed for weights that follow a normal distribution (common in neural networks).\n",
    "\n",
    "**Key idea:** \n",
    "- Split the normal distribution into 16 equal-probability bins\n",
    "- Each weight maps to one of 16 quantized values (4 bits)\n",
    "- Preserves more precision around zero (where most weights concentrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF4Quantizer:\n",
    "    \"\"\"\n",
    "    4-bit NormalFloat quantization.\n",
    "    \n",
    "    Maps normal distribution to 16 quantized levels.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # NF4 quantization levels (optimized for normal distribution)\n",
    "        # These values are chosen to split a standard normal distribution into equal probability bins\n",
    "        self.nf4_values = torch.tensor([\n",
    "            -1.0, -0.6961928009986877, -0.5250730514526367, -0.39491748809814453,\n",
    "            -0.28444138169288635, -0.18477343022823334, -0.09105003625154495, 0.0,\n",
    "            0.07958029955625534, 0.16093020141124725, 0.24611230194568634, 0.33791524171829224,\n",
    "            0.44070982933044434, 0.5626170039176941, 0.7229568362236023, 1.0\n",
    "        ])\n",
    "    \n",
    "    def quantize(self, weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantize weights to 4-bit NF4.\n",
    "        \n",
    "        Args:\n",
    "            weights: Float tensor to quantize\n",
    "            \n",
    "        Returns:\n",
    "            quantized: 4-bit indices (int8 tensor)\n",
    "            scale: Scaling factor for dequantization\n",
    "        \"\"\"\n",
    "        # Compute scale (max absolute value)\n",
    "        scale = weights.abs().max()\n",
    "        \n",
    "        if scale == 0:\n",
    "            # Handle zero weights\n",
    "            return torch.zeros_like(weights, dtype=torch.int8), scale\n",
    "        \n",
    "        # Normalize weights to [-1, 1]\n",
    "        normalized = weights / scale\n",
    "        \n",
    "        # Find nearest NF4 value for each weight\n",
    "        nf4_values = self.nf4_values.to(weights.device)\n",
    "        \n",
    "        # Compute distances to all NF4 values\n",
    "        distances = (normalized.unsqueeze(-1) - nf4_values).abs()\n",
    "        \n",
    "        # Get index of nearest value\n",
    "        quantized = distances.argmin(dim=-1).to(torch.int8)\n",
    "        \n",
    "        return quantized, scale\n",
    "    \n",
    "    def dequantize(self, quantized: torch.Tensor, scale: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Dequantize 4-bit values back to float.\n",
    "        \n",
    "        Args:\n",
    "            quantized: 4-bit indices\n",
    "            scale: Scaling factor\n",
    "            \n",
    "        Returns:\n",
    "            Dequantized float tensor\n",
    "        \"\"\"\n",
    "        nf4_values = self.nf4_values.to(quantized.device)\n",
    "        \n",
    "        # Map indices to NF4 values\n",
    "        dequantized = nf4_values[quantized.long()]\n",
    "        \n",
    "        # Scale back\n",
    "        return dequantized * scale\n",
    "\n",
    "\n",
    "# Test quantization\n",
    "print(\"Testing NF4 Quantization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "quantizer = NF4Quantizer()\n",
    "\n",
    "# Create sample weights (normal distribution)\n",
    "weights = torch.randn(1000)\n",
    "print(f\"Original weights: mean={weights.mean():.4f}, std={weights.std():.4f}\")\n",
    "print(f\"Original memory: {weights.numel() * weights.element_size()} bytes\")\n",
    "\n",
    "# Quantize\n",
    "quantized, scale = quantizer.quantize(weights)\n",
    "print(f\"\\nQuantized to 4-bit: {quantized.shape}\")\n",
    "print(f\"Quantized memory: {quantized.numel() * quantized.element_size()} bytes\")\n",
    "print(f\"Memory reduction: {weights.numel() * weights.element_size() / (quantized.numel() * quantized.element_size()):.1f}x\")\n",
    "\n",
    "# Dequantize\n",
    "dequantized = quantizer.dequantize(quantized, scale)\n",
    "print(f\"\\nDequantized: mean={dequantized.mean():.4f}, std={dequantized.std():.4f}\")\n",
    "\n",
    "# Measure error\n",
    "mse = ((weights - dequantized) ** 2).mean()\n",
    "print(f\"Quantization MSE: {mse:.6f}\")\n",
    "print(f\"Relative error: {(mse / weights.var()).sqrt():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Quantized Linear Layer\n",
    "\n",
    "Create a linear layer that stores weights in 4-bit but computes in full precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with 4-bit quantized weights.\n",
    "    \n",
    "    Stores weights in 4-bit, dequantizes during forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, linear: nn.Linear):\n",
    "        super().__init__()\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        \n",
    "        # Quantize the weights\n",
    "        quantizer = NF4Quantizer()\n",
    "        with torch.no_grad():\n",
    "            quantized_weight, scale = quantizer.quantize(linear.weight.data)\n",
    "        \n",
    "        # Store quantized weights and scale (not as parameters, so not trained)\n",
    "        self.register_buffer('quantized_weight', quantized_weight)\n",
    "        self.register_buffer('scale', scale)\n",
    "        \n",
    "        # Store bias if present\n",
    "        if linear.bias is not None:\n",
    "            self.register_buffer('bias', linear.bias.data.clone())\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        self.quantizer = NF4Quantizer()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: dequantize weights on-the-fly and compute.\n",
    "        \"\"\"\n",
    "        # Dequantize weights\n",
    "        weight = self.quantizer.dequantize(self.quantized_weight, self.scale)\n",
    "        \n",
    "        # Standard linear operation\n",
    "        output = F.linear(x, weight, self.bias)\n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, quantized=4-bit'\n",
    "\n",
    "\n",
    "# Test quantized linear layer\n",
    "print(\"\\nTesting Quantized Linear Layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create original linear layer\n",
    "linear = nn.Linear(512, 512)\n",
    "original_size = sum(p.numel() * p.element_size() for p in linear.parameters())\n",
    "print(f\"Original linear layer: {original_size / 1024:.2f} KB\")\n",
    "\n",
    "# Quantize it\n",
    "quantized_linear = QuantizedLinear(linear)\n",
    "quantized_size = sum(b.numel() * b.element_size() for b in quantized_linear.buffers())\n",
    "print(f\"Quantized linear layer: {quantized_size / 1024:.2f} KB\")\n",
    "print(f\"Memory reduction: {original_size / quantized_size:.1f}x\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(4, 10, 512)\n",
    "out_original = linear(x)\n",
    "out_quantized = quantized_linear(x)\n",
    "\n",
    "# Compare outputs\n",
    "mse = ((out_original - out_quantized) ** 2).mean()\n",
    "print(f\"\\nForward pass MSE: {mse:.6f}\")\n",
    "print(f\"Relative error: {(mse / out_original.var()).sqrt():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: QLoRA Layer - Combining Quantization with LoRA\n",
    "\n",
    "Now we combine 4-bit quantized weights with full-precision LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA layer for quantized linear layers.\n",
    "    \n",
    "    Base weights: 4-bit quantized (frozen)\n",
    "    LoRA adapters: Full precision (trainable)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        quantized_linear: QuantizedLinear,\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.quantized_linear = quantized_linear\n",
    "        self.in_features = quantized_linear.in_features\n",
    "        self.out_features = quantized_linear.out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # LoRA adapters in full precision (FP32 or FP16)\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, self.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.out_features, rank))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize A with Kaiming uniform\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        # B initialized to zero\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward: Dequantize base weights + LoRA adaptation\n",
    "        \"\"\"\n",
    "        # Base output (from quantized weights)\n",
    "        base_output = self.quantized_linear(x)\n",
    "        \n",
    "        # LoRA adaptation (full precision)\n",
    "        lora_output = (self.dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "        \n",
    "        return base_output + lora_output\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return (f'in_features={self.in_features}, out_features={self.out_features}, '\n",
    "                f'rank={self.rank}, alpha={self.alpha}, base_quantized=4-bit')\n",
    "\n",
    "\n",
    "# Test QLoRA layer\n",
    "print(\"\\nTesting QLoRA Layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create QLoRA layer\n",
    "qlora_layer = QLoRALayer(quantized_linear, rank=8, alpha=16)\n",
    "\n",
    "# Count parameters\n",
    "base_params = sum(b.numel() for b in qlora_layer.quantized_linear.buffers())\n",
    "lora_params = sum(p.numel() for p in [qlora_layer.lora_A, qlora_layer.lora_B])\n",
    "trainable_params = sum(p.numel() for p in qlora_layer.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Base (quantized) parameters: {base_params:,}\")\n",
    "print(f\"LoRA parameters: {lora_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable percentage: {100 * trainable_params / (base_params + lora_params):.2f}%\")\n",
    "\n",
    "# Memory calculation\n",
    "base_memory = sum(b.numel() * b.element_size() for b in qlora_layer.quantized_linear.buffers())\n",
    "lora_memory = sum(p.numel() * p.element_size() for p in [qlora_layer.lora_A, qlora_layer.lora_B])\n",
    "print(f\"\\nBase memory (4-bit): {base_memory / 1024:.2f} KB\")\n",
    "print(f\"LoRA memory (FP32): {lora_memory / 1024:.2f} KB\")\n",
    "print(f\"Total memory: {(base_memory + lora_memory) / 1024:.2f} KB\")\n",
    "\n",
    "# Compare with full precision\n",
    "full_precision_memory = (512 * 512 * 4)  # FP32\n",
    "print(f\"\\nFull precision would be: {full_precision_memory / 1024:.2f} KB\")\n",
    "print(f\"QLoRA memory reduction: {full_precision_memory / (base_memory + lora_memory):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Load Model with QLoRA\n",
    "\n",
    "Load GPT-2 and apply QLoRA to it. We'll use bitsandbytes for efficient quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qlora_to_model(\n",
    "    model: nn.Module,\n",
    "    target_modules: List[str] = ['c_attn'],\n",
    "    rank: int = 8,\n",
    "    alpha: float = 16,\n",
    "    dropout: float = 0.1,\n",
    "    use_custom_quantization: bool = False\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Apply QLoRA to specified modules in the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Pre-trained model\n",
    "        target_modules: Names of modules to apply QLoRA to\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA alpha\n",
    "        dropout: Dropout probability\n",
    "        use_custom_quantization: Use custom quantization (True) or bitsandbytes (False)\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if any(target in name for target in target_modules):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                parent_name = '.'.join(name.split('.')[:-1])\n",
    "                attr_name = name.split('.')[-1]\n",
    "                parent = model.get_submodule(parent_name) if parent_name else model\n",
    "                \n",
    "                if use_custom_quantization:\n",
    "                    # Use our custom quantization\n",
    "                    quantized_linear = QuantizedLinear(module)\n",
    "                    qlora_layer = QLoRALayer(quantized_linear, rank=rank, alpha=alpha, dropout=dropout)\n",
    "                    setattr(parent, attr_name, qlora_layer)\n",
    "                    print(f\"Applied QLoRA (custom) to: {name}\")\n",
    "                else:\n",
    "                    # Use bitsandbytes quantization (more efficient if available)\n",
    "                    try:\n",
    "                        import bitsandbytes as bnb\n",
    "                        # This would use bnb.nn.Linear4bit in practice\n",
    "                        # For simplicity, we'll use custom quantization\n",
    "                        quantized_linear = QuantizedLinear(module)\n",
    "                        qlora_layer = QLoRALayer(quantized_linear, rank=rank, alpha=alpha, dropout=dropout)\n",
    "                        setattr(parent, attr_name, qlora_layer)\n",
    "                        print(f\"Applied QLoRA to: {name}\")\n",
    "                    except ImportError:\n",
    "                        quantized_linear = QuantizedLinear(module)\n",
    "                        qlora_layer = QLoRALayer(quantized_linear, rank=rank, alpha=alpha, dropout=dropout)\n",
    "                        setattr(parent, attr_name, qlora_layer)\n",
    "                        print(f\"Applied QLoRA (custom) to: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> dict:\n",
    "    \"\"\"\n",
    "    Count total and trainable parameters.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    quantized_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    \n",
    "    # Count quantized buffers\n",
    "    for name, buffer in model.named_buffers():\n",
    "        if 'quantized_weight' in name:\n",
    "            quantized_params += buffer.numel()\n",
    "    \n",
    "    return {\n",
    "        'total': total_params,\n",
    "        'trainable': trainable_params,\n",
    "        'quantized': quantized_params,\n",
    "        'trainable_pct': 100 * trainable_params / total_params if total_params > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Load GPT-2 model\n",
    "print(\"Loading GPT-2 small model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model normally first\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Start with FP32 for custom quantization\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Model:\")\n",
    "print(\"=\" * 50)\n",
    "original_stats = count_parameters(model)\n",
    "print(f\"Total parameters: {original_stats['total']:,}\")\n",
    "\n",
    "# Calculate original memory\n",
    "original_memory = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"Original memory: {original_memory / (1024**2):.2f} MB\")\n",
    "\n",
    "# Apply QLoRA\n",
    "print(\"\\nApplying QLoRA to model...\")\n",
    "print(\"=\" * 50)\n",
    "model = apply_qlora_to_model(\n",
    "    model,\n",
    "    target_modules=['c_attn'],\n",
    "    rank=8,\n",
    "    alpha=16,\n",
    "    dropout=0.1,\n",
    "    use_custom_quantization=True\n",
    ")\n",
    "\n",
    "# Calculate stats after QLoRA\n",
    "print(\"\\nQLoRA Model:\")\n",
    "print(\"=\" * 50)\n",
    "qlora_stats = count_parameters(model)\n",
    "print(f\"Total parameters: {qlora_stats['total']:,}\")\n",
    "print(f\"Trainable parameters: {qlora_stats['trainable']:,}\")\n",
    "print(f\"Quantized parameters: {qlora_stats['quantized']:,}\")\n",
    "print(f\"Trainable percentage: {qlora_stats['trainable_pct']:.3f}%\")\n",
    "\n",
    "# Calculate memory with quantization\n",
    "trainable_memory = sum(p.numel() * p.element_size() for p in model.parameters() if p.requires_grad)\n",
    "quantized_memory = sum(b.numel() * b.element_size() for name, b in model.named_buffers() if 'quantized' in name)\n",
    "other_memory = sum(p.numel() * p.element_size() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "total_memory = trainable_memory + quantized_memory + other_memory\n",
    "\n",
    "print(f\"\\nMemory breakdown:\")\n",
    "print(f\"Trainable (LoRA): {trainable_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Quantized (4-bit): {quantized_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Other (frozen): {other_memory / (1024**2):.2f} MB\")\n",
    "print(f\"Total memory: {total_memory / (1024**2):.2f} MB\")\n",
    "print(f\"\\nMemory reduction vs original: {original_memory / total_memory:.2f}x\")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel moved to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Data Preparation\n",
    "\n",
    "Same as regular LoRA - prepare training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple text dataset for causal language modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts: List[str], tokenizer, max_length: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.encodings.append(encoded)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[0] for key, val in self.encodings[idx].items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "\n",
    "# Training data\n",
    "train_texts = [\n",
    "    \"Quantization reduces model size by representing weights with fewer bits.\",\n",
    "    \"QLoRA combines quantization with low-rank adaptation for efficient fine-tuning.\",\n",
    "    \"4-bit quantization can reduce memory usage by 4x compared to float32.\",\n",
    "    \"NormalFloat quantization is optimized for neural network weight distributions.\",\n",
    "    \"Double quantization further compresses the quantization constants themselves.\",\n",
    "    \"LoRA adapters remain in full precision while base weights are quantized.\",\n",
    "    \"Paged optimizers enable training on GPUs with limited memory.\",\n",
    "    \"Parameter-efficient fine-tuning democratizes access to large model training.\",\n",
    "    \"Gradient checkpointing trades computation for memory during training.\",\n",
    "    \"Mixed precision training uses different precisions for different operations.\",\n",
    "    \"Quantization-aware training simulates quantization during the training process.\",\n",
    "    \"Post-training quantization converts trained models to lower precision.\",\n",
    "    \"Symmetric quantization uses equal ranges for positive and negative values.\",\n",
    "    \"Asymmetric quantization can better preserve accuracy for skewed distributions.\",\n",
    "    \"Per-channel quantization uses different scales for each output channel.\",\n",
    "    \"Knowledge distillation can recover accuracy lost during quantization.\",\n",
    "    \"Quantization noise can act as a form of regularization during training.\",\n",
    "    \"Dynamic quantization determines scales at runtime based on activations.\",\n",
    "    \"Static quantization pre-determines scales using calibration data.\",\n",
    "    \"Integer-only inference eliminates floating point operations entirely.\",\n",
    "] * 5\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SimpleTextDataset(train_texts, tokenizer, max_length=64)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: QLoRA Training\n",
    "\n",
    "Train only the LoRA adapters while keeping quantized weights frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qlora(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    weight_decay: float = 0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model with QLoRA.\n",
    "    \"\"\"\n",
    "    # Only optimize LoRA parameters (full precision)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in model.parameters() if p.requires_grad],\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                      f\"Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}, \"\n",
    "                      f\"Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        epoch_loss = total_loss / num_batches\n",
    "        print(f\"\\nEpoch {epoch+1} completed. Average Loss: {epoch_loss:.4f}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "print(\"Starting QLoRA training...\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Note: Only LoRA adapters are being trained (full precision)\")\n",
    "print(\"Base model weights remain quantized and frozen\\n\")\n",
    "\n",
    "train_qlora(model, train_loader, num_epochs=3, learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Text Generation\n",
    "\n",
    "Test the fine-tuned QLoRA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 100,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.9\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"Quantization reduces\",\n",
    "    \"QLoRA combines\",\n",
    "    \"4-bit quantization\",\n",
    "    \"LoRA adapters\",\n",
    "    \"Parameter-efficient\"\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating text with QLoRA fine-tuned model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=80)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Save and Load QLoRA Weights\n",
    "\n",
    "Save only the LoRA adapters (not quantized base weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_qlora_weights(model: nn.Module, save_path: str):\n",
    "    \"\"\"\n",
    "    Save only the QLoRA adapter weights.\n",
    "    \"\"\"\n",
    "    qlora_state_dict = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' in name and param.requires_grad:\n",
    "            qlora_state_dict[name] = param.cpu()\n",
    "    \n",
    "    torch.save(qlora_state_dict, save_path)\n",
    "    print(f\"QLoRA weights saved to: {save_path}\")\n",
    "    print(f\"Number of adapter parameters: {len(qlora_state_dict)}\")\n",
    "    \n",
    "    import os\n",
    "    size_mb = os.path.getsize(save_path) / (1024 * 1024)\n",
    "    print(f\"File size: {size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "def load_qlora_weights(model: nn.Module, load_path: str):\n",
    "    \"\"\"\n",
    "    Load QLoRA adapter weights.\n",
    "    \"\"\"\n",
    "    qlora_state_dict = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(qlora_state_dict, strict=False)\n",
    "    print(f\"QLoRA weights loaded from: {load_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nSaving QLoRA weights...\")\n",
    "print(\"=\" * 50)\n",
    "save_qlora_weights(model, '/home/claude/qlora_weights.pt')\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Full GPT-2 (FP32): ~500 MB\")\n",
    "print(f\"Full GPT-2 (4-bit): ~125 MB\")\n",
    "print(f\"QLoRA adapters only: ~2-5 MB\")\n",
    "print(f\"\\nüéâ QLoRA enables training with minimal storage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: QLoRA vs LoRA vs Full Fine-tuning\n",
    "\n",
    "### Memory Comparison (GPT-2 Small - 124M params)\n",
    "\n",
    "| Method | Memory | Trainable % | Storage |\n",
    "|--------|--------|-------------|----------|\n",
    "| Full Fine-tuning (FP32) | ~500 MB | 100% | ~500 MB |\n",
    "| Full Fine-tuning (FP16) | ~250 MB | 100% | ~250 MB |\n",
    "| LoRA (r=8) | ~500 MB | 0.3% | ~2 MB |\n",
    "| QLoRA (4-bit + r=8) | ~130 MB | 0.3% | ~2 MB |\n",
    "\n",
    "### Key Advantages of QLoRA:\n",
    "\n",
    "1. **Ultra-low Memory**: 4√ó less than LoRA during training\n",
    "2. **Tiny Storage**: Only save adapter weights (~2 MB)\n",
    "3. **Same Quality**: Matches LoRA performance in most tasks\n",
    "4. **Enables Larger Models**: Can train 7B-65B models on consumer GPUs\n",
    "5. **Fast Inference**: Dequantization is very fast\n",
    "\n",
    "### When to Use QLoRA:\n",
    "\n",
    "‚úÖ **Use QLoRA when:**\n",
    "- Memory is very limited (< 8GB GPU)\n",
    "- Training large models (> 1B parameters)\n",
    "- Need to train multiple adapters\n",
    "- Consumer hardware (laptop, Colab free)\n",
    "\n",
    "‚ö†Ô∏è **Use regular LoRA when:**\n",
    "- Memory is not a constraint\n",
    "- Maximum performance needed\n",
    "- Small models (< 1B parameters)\n",
    "- Quantization overhead matters\n",
    "\n",
    "### Technical Details:\n",
    "\n",
    "**NF4 Quantization:**\n",
    "- Maps 32-bit floats ‚Üí 4-bit integers\n",
    "- Optimized for normal distributions\n",
    "- ~1% accuracy loss vs FP16\n",
    "\n",
    "**Double Quantization:**\n",
    "- Quantizes the quantization constants\n",
    "- Saves ~0.4 bits per parameter\n",
    "- Negligible accuracy impact\n",
    "\n",
    "**Paged Optimizers:**\n",
    "- Offload optimizer states to CPU RAM\n",
    "- Enable training larger batches\n",
    "- Implemented in bitsandbytes library\n",
    "\n",
    "### Hyperparameters:\n",
    "\n",
    "**Quantization:**\n",
    "- Bit width: 4-bit (NF4 or INT4)\n",
    "- Double quantization: Usually enabled\n",
    "- Compute dtype: bfloat16 preferred\n",
    "\n",
    "**LoRA:**\n",
    "- Rank: 8-64 (start with 8)\n",
    "- Alpha: 16-32 (2√ó rank typically)\n",
    "- Target modules: Attention (q_proj, v_proj)\n",
    "- Dropout: 0.05-0.1\n",
    "\n",
    "**Training:**\n",
    "- Learning rate: 1e-4 to 5e-4\n",
    "- Batch size: Larger than LoRA (more memory available)\n",
    "- Gradient accumulation: As needed\n",
    "- Warmup: 5-10% of steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Tips for Using QLoRA\n",
    "\n",
    "### 1. Model Loading:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Efficient 4-bit loading with bitsandbytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. PEFT Integration:\n",
    "\n",
    "```python\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "### 3. Common Issues:\n",
    "\n",
    "**CUDA Out of Memory:**\n",
    "- Reduce batch size\n",
    "- Enable gradient checkpointing\n",
    "- Use gradient accumulation\n",
    "\n",
    "**Slow Training:**\n",
    "- Dequantization has overhead\n",
    "- Use bfloat16 compute dtype\n",
    "- Enable Flash Attention if available\n",
    "\n",
    "**Quality Issues:**\n",
    "- Try higher rank (16-32)\n",
    "- Target more modules\n",
    "- Increase training steps\n",
    "\n",
    "### 4. Extensions:\n",
    "\n",
    "- **QLoRA + Flash Attention**: 2√ó faster training\n",
    "- **Multi-adapter QLoRA**: Multiple task adapters\n",
    "- **QLoRA for Vision**: Apply to ViT models\n",
    "- **8-bit QLoRA**: Less aggressive quantization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
