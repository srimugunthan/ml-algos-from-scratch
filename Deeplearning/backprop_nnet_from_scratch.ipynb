{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Network from Scratch\n",
    "\n",
    "This notebook implements a simple neural network with manual backpropagation to understand the core concepts without unnecessary complexity.\n",
    "\n",
    "**Key Concepts:**\n",
    "1. Forward pass: Computing predictions\n",
    "2. Loss calculation: Measuring prediction error\n",
    "3. Backward pass: Computing gradients using chain rule\n",
    "4. Weight updates: Using gradient descent\n",
    "\n",
    "We'll use PyTorch tensors but implement backprop manually to see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Create a Simple Dataset\n",
    "\n",
    "We'll use a simple classification problem: **XOR gate**\n",
    "\n",
    "XOR is not linearly separable, so we need a hidden layer.\n",
    "\n",
    "- **Input:** Two binary values\n",
    "- **Output:** 1 if inputs differ, 0 if they're the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X = torch.tensor([\n",
    "    [0.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "y = torch.tensor([\n",
    "    [0.0],  # 0 XOR 0 = 0\n",
    "    [1.0],  # 0 XOR 1 = 1\n",
    "    [1.0],  # 1 XOR 0 = 1\n",
    "    [0.0]   # 1 XOR 1 = 0\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Input shape:\", X.shape)  # (4, 2) - 4 samples, 2 features\n",
    "print(\"Output shape:\", y.shape)  # (4, 1) - 4 samples, 1 output\n",
    "print(\"\\nDataset:\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i].numpy()} -> Output: {y[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Initialize Network Parameters\n",
    "\n",
    "**Network architecture:**\n",
    "- Input layer: 2 neurons (for 2 input features)\n",
    "- Hidden layer: 3 neurons (arbitrary choice, enough for XOR)\n",
    "- Output layer: 1 neuron (for binary classification)\n",
    "\n",
    "**Parameters:**\n",
    "- `W1`: Weights from input to hidden (2 x 3)\n",
    "- `b1`: Biases for hidden layer (3)\n",
    "- `W2`: Weights from hidden to output (3 x 1)\n",
    "- `b2`: Bias for output layer (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network dimensions\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "# Initialize weights with small random values\n",
    "# Xavier initialization: helps with training stability\n",
    "W1 = torch.randn(input_size, hidden_size) * 0.5\n",
    "b1 = torch.zeros(hidden_size)\n",
    "\n",
    "W2 = torch.randn(hidden_size, output_size) * 0.5\n",
    "b2 = torch.zeros(output_size)\n",
    "\n",
    "print(\"W1 shape (input to hidden):\", W1.shape)\n",
    "print(\"b1 shape:\", b1.shape)\n",
    "print(\"W2 shape (hidden to output):\", W2.shape)\n",
    "print(\"b2 shape:\", b2.shape)\n",
    "print(f\"\\nTotal parameters: {W1.numel() + b1.numel() + W2.numel() + b2.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Define Activation Functions\n",
    "\n",
    "**Sigmoid:** σ(x) = 1 / (1 + e^(-x))\n",
    "- Squashes values to range (0, 1)\n",
    "- Used in hidden and output layers for this binary classification\n",
    "\n",
    "**Derivative:** σ'(x) = σ(x) * (1 - σ(x))\n",
    "- Needed for backpropagation\n",
    "- Computed from the sigmoid output (saves computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Apply sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    \"\"\"\n",
    "    Compute derivative of sigmoid from its output\n",
    "    This is more efficient than recomputing sigmoid\n",
    "    \"\"\"\n",
    "    return sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "# Test the functions\n",
    "test_input = torch.tensor([-2.0, 0.0, 2.0])\n",
    "test_output = sigmoid(test_input)\n",
    "print(\"Sigmoid test:\")\n",
    "print(f\"Input: {test_input.numpy()}\")\n",
    "print(f\"Output: {test_output.numpy()}\")\n",
    "print(f\"Derivative: {sigmoid_derivative(test_output).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Forward Pass\n",
    "\n",
    "Compute predictions by passing input through the network:\n",
    "\n",
    "1. Hidden layer: **h = σ(X @ W1 + b1)**\n",
    "2. Output layer: **y_pred = σ(h @ W2 + b2)**\n",
    "\n",
    "Where:\n",
    "- `@` is matrix multiplication\n",
    "- `σ` is sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward propagation through the network\n",
    "    \n",
    "    Returns:\n",
    "        y_pred: Final predictions\n",
    "        h: Hidden layer activations (needed for backprop)\n",
    "        z1: Pre-activation hidden values (needed for backprop)\n",
    "        z2: Pre-activation output values (needed for backprop)\n",
    "    \"\"\"\n",
    "    # Hidden layer\n",
    "    z1 = X @ W1 + b1              # Linear combination\n",
    "    h = sigmoid(z1)                # Apply activation\n",
    "    \n",
    "    # Output layer\n",
    "    z2 = h @ W2 + b2              # Linear combination\n",
    "    y_pred = sigmoid(z2)           # Apply activation\n",
    "    \n",
    "    return y_pred, h, z1, z2\n",
    "\n",
    "# Test forward pass with initial weights\n",
    "y_pred, h, z1, z2 = forward_pass(X, W1, b1, W2, b2)\n",
    "print(\"Predictions with random initialization:\")\n",
    "print(y_pred.squeeze().detach().numpy())\n",
    "print(\"\\nTrue labels:\")\n",
    "print(y.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Loss Function\n",
    "\n",
    "**Mean Squared Error (MSE):** Loss = mean((y_pred - y)²)\n",
    "\n",
    "For binary classification, we could use Binary Cross-Entropy, but MSE is simpler to understand for learning backprop.\n",
    "\n",
    "**Derivative:** dLoss/dy_pred = 2 * (y_pred - y) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y):\n",
    "    \"\"\"Calculate Mean Squared Error loss\"\"\"\n",
    "    return torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "# Test loss calculation\n",
    "initial_loss = compute_loss(y_pred, y)\n",
    "print(f\"Initial loss: {initial_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Backward Pass (Backpropagation)\n",
    "\n",
    "**This is the heart of neural network training!**\n",
    "\n",
    "We compute gradients using the chain rule, working backwards from loss to inputs.\n",
    "\n",
    "**Chain rule reminder:** If z = f(g(x)), then dz/dx = (dz/dg) * (dg/dx)\n",
    "\n",
    "**Gradients we need to compute:**\n",
    "1. dLoss/dW2, dLoss/db2 (output layer weights and bias)\n",
    "2. dLoss/dW1, dLoss/db1 (hidden layer weights and bias)\n",
    "\n",
    "**Working backwards:**\n",
    "- Start with loss gradient\n",
    "- Multiply by activation derivative\n",
    "- Multiply by previous layer's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, y, y_pred, h, z1, z2, W2):\n",
    "    \"\"\"\n",
    "    Compute all gradients using backpropagation\n",
    "    \n",
    "    Returns gradients for W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # === OUTPUT LAYER GRADIENTS ===\n",
    "    \n",
    "    # 1. Gradient of loss with respect to predictions\n",
    "    # dLoss/dy_pred = 2 * (y_pred - y) / n\n",
    "    dLoss_dy_pred = 2 * (y_pred - y) / n_samples\n",
    "    \n",
    "    # 2. Gradient through sigmoid activation at output\n",
    "    # dLoss/dz2 = dLoss/dy_pred * dy_pred/dz2\n",
    "    # dy_pred/dz2 = sigmoid'(z2) = y_pred * (1 - y_pred)\n",
    "    dLoss_dz2 = dLoss_dy_pred * sigmoid_derivative(y_pred)\n",
    "    \n",
    "    # 3. Gradient with respect to W2\n",
    "    # z2 = h @ W2 + b2, so dz2/dW2 = h\n",
    "    # dLoss/dW2 = h^T @ dLoss_dz2\n",
    "    dLoss_dW2 = h.T @ dLoss_dz2\n",
    "    \n",
    "    # 4. Gradient with respect to b2\n",
    "    # dLoss/db2 = sum of dLoss_dz2 across samples\n",
    "    dLoss_db2 = torch.sum(dLoss_dz2, dim=0)\n",
    "    \n",
    "    # === HIDDEN LAYER GRADIENTS ===\n",
    "    \n",
    "    # 5. Gradient flowing back to hidden layer\n",
    "    # dLoss/dh = dLoss/dz2 @ W2^T (chain rule)\n",
    "    dLoss_dh = dLoss_dz2 @ W2.T\n",
    "    \n",
    "    # 6. Gradient through sigmoid activation at hidden layer\n",
    "    # dLoss/dz1 = dLoss/dh * dh/dz1\n",
    "    # dh/dz1 = sigmoid'(z1) = h * (1 - h)\n",
    "    dLoss_dz1 = dLoss_dh * sigmoid_derivative(h)\n",
    "    \n",
    "    # 7. Gradient with respect to W1\n",
    "    # z1 = X @ W1 + b1, so dz1/dW1 = X\n",
    "    # dLoss/dW1 = X^T @ dLoss_dz1\n",
    "    dLoss_dW1 = X.T @ dLoss_dz1\n",
    "    \n",
    "    # 8. Gradient with respect to b1\n",
    "    # dLoss/db1 = sum of dLoss_dz1 across samples\n",
    "    dLoss_db1 = torch.sum(dLoss_dz1, dim=0)\n",
    "    \n",
    "    return dLoss_dW1, dLoss_db1, dLoss_dW2, dLoss_db2\n",
    "\n",
    "# Test backward pass\n",
    "dW1, db1, dW2, db2 = backward_pass(X, y, y_pred, h, z1, z2, W2)\n",
    "print(\"Gradient shapes:\")\n",
    "print(f\"dW1: {dW1.shape}, db1: {db1.shape}\")\n",
    "print(f\"dW2: {dW2.shape}, db2: {db2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Gradient Descent Update\n",
    "\n",
    "**Update rule:** θ_new = θ_old - learning_rate * gradient\n",
    "\n",
    "Learning rate controls step size:\n",
    "- **Too large:** May overshoot and diverge\n",
    "- **Too small:** Slow convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    \"\"\"\n",
    "    Update all parameters using gradient descent\n",
    "    \"\"\"\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Test update (not actually updating yet)\n",
    "learning_rate = 0.5\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"Parameter updates will move parameters in opposite direction of gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: Training Loop\n",
    "\n",
    "Combine all steps to train the network:\n",
    "1. Forward pass (compute predictions)\n",
    "2. Compute loss\n",
    "3. Backward pass (compute gradients)\n",
    "4. Update parameters\n",
    "\n",
    "Repeat for many epochs until loss converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize parameters for clean training\n",
    "torch.manual_seed(42)\n",
    "W1 = torch.randn(input_size, hidden_size) * 0.5\n",
    "b1 = torch.zeros(hidden_size)\n",
    "W2 = torch.randn(hidden_size, output_size) * 0.5\n",
    "b2 = torch.zeros(output_size)\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1.0  # Higher learning rate works for this simple problem\n",
    "n_epochs = 5000\n",
    "\n",
    "# Track loss history for visualization\n",
    "loss_history = []\n",
    "\n",
    "print(\"Training started...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred, h, z1, z2 = forward_pass(X, W1, b1, W2, b2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(y_pred, y)\n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    dW1, db1, dW2, db2 = backward_pass(X, y, y_pred, h, z1, z2, W2)\n",
    "    \n",
    "    # Update parameters\n",
    "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    # Print progress every 500 epochs\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 9: Evaluate Final Results\n",
    "\n",
    "Check how well the network learned XOR logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions\n",
    "y_pred_final, _, _, _ = forward_pass(X, W1, b1, W2, b2)\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(len(X)):\n",
    "    pred = y_pred_final[i].item()\n",
    "    true = y[i].item()\n",
    "    print(f\"Input: {X[i].numpy()} | Predicted: {pred:.4f} | True: {true:.0f} | {'✓' if round(pred) == true else '✗'}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions_rounded = torch.round(y_pred_final)\n",
    "accuracy = (predictions_rounded == y).float().mean().item()\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.1f}%\")\n",
    "print(f\"Final loss: {loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 10: Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Loss over time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see convergence better\n",
    "\n",
    "# Plot 2: Decision boundary visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Create a mesh grid to visualize decision boundary\n",
    "x_min, x_max = -0.5, 1.5\n",
    "y_min, y_max = -0.5, 1.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Compute predictions for each point in the grid\n",
    "grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "grid_pred, _, _, _ = forward_pass(grid_points, W1, b1, W2, b2)\n",
    "grid_pred = grid_pred.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, grid_pred, levels=20, cmap='RdYlBu_r', alpha=0.8)\n",
    "plt.colorbar(label='Predicted Value')\n",
    "\n",
    "# Plot training points\n",
    "colors = ['blue' if label == 0 else 'red' for label in y.squeeze().numpy()]\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
    "plt.xlabel('Input 1', fontsize=12)\n",
    "plt.ylabel('Input 2', fontsize=12)\n",
    "plt.title('Decision Boundary (XOR)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Compare with PyTorch's Autograd\n",
    "\n",
    "Let's verify our implementation matches PyTorch's automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same network using PyTorch's autograd\n",
    "torch.manual_seed(42)\n",
    "W1_auto = torch.randn(input_size, hidden_size, requires_grad=True) * 0.5\n",
    "b1_auto = torch.zeros(hidden_size, requires_grad=True)\n",
    "W2_auto = torch.randn(hidden_size, output_size, requires_grad=True) * 0.5\n",
    "b2_auto = torch.zeros(output_size, requires_grad=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1.0\n",
    "n_epochs = 5000\n",
    "loss_history_auto = []\n",
    "\n",
    "print(\"Training with PyTorch autograd...\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass (PyTorch tracks operations automatically)\n",
    "    z1 = X @ W1_auto + b1_auto\n",
    "    h = torch.sigmoid(z1)\n",
    "    z2 = h @ W2_auto + b2_auto\n",
    "    y_pred = torch.sigmoid(z2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "    loss_history_auto.append(loss.item())\n",
    "    \n",
    "    # Backward pass (PyTorch computes gradients automatically)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        W1_auto -= learning_rate * W1_auto.grad\n",
    "        b1_auto -= learning_rate * b1_auto.grad\n",
    "        W2_auto -= learning_rate * W2_auto.grad\n",
    "        b2_auto -= learning_rate * b2_auto.grad\n",
    "        \n",
    "        # Zero gradients for next iteration\n",
    "        W1_auto.grad.zero_()\n",
    "        b1_auto.grad.zero_()\n",
    "        W2_auto.grad.zero_()\n",
    "        b2_auto.grad.zero_()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\nComparing both implementations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare final losses\n",
    "print(f\"Manual backprop final loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"PyTorch autograd final loss: {loss_history_auto[-1]:.6f}\")\n",
    "print(f\"Difference: {abs(loss_history[-1] - loss_history_auto[-1]):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Manual Backprop', linewidth=2)\n",
    "plt.plot(loss_history_auto, label='PyTorch Autograd', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Manual Backprop vs PyTorch Autograd', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEY TAKEAWAYS\n",
    "\n",
    "### 1. Forward Pass\n",
    "Data flows through layers, each applying linear transform + activation\n",
    "\n",
    "### 2. Loss Function\n",
    "Measures how far predictions are from true values\n",
    "\n",
    "### 3. Backpropagation\n",
    "Chain rule applied backwards to compute gradients\n",
    "- Start at loss\n",
    "- Flow backwards through each layer\n",
    "- Multiply gradients at each step\n",
    "\n",
    "### 4. Gradient Descent\n",
    "Move parameters opposite to gradient direction\n",
    "- Reduces loss over time\n",
    "- Learning rate controls step size\n",
    "\n",
    "### 5. Why It Works\n",
    "- Gradients point in direction of steepest increase\n",
    "- Moving opposite direction decreases loss\n",
    "- Repeated small steps converge to good solution\n",
    "\n",
    "### 6. PyTorch's Role\n",
    "- **Manual:** We computed every gradient explicitly\n",
    "- **Autograd:** PyTorch tracks operations and computes gradients automatically\n",
    "- **Both give identical results!**\n",
    "\n",
    "---\n",
    "\n",
    "**This is the foundation of deep learning - just scaled up to larger networks!**\n",
    "\n",
    "✓ Notebook complete! You now understand backpropagation from first principles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
