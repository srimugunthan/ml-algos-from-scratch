{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46687107",
   "metadata": {},
   "source": [
    "# SHAP Values from Scratch\n",
    "\n",
    "## Understanding Shapley Additive Explanations\n",
    "\n",
    "This notebook implements SHAP (SHapley Additive exPlanations) values from first principles to help you understand how the algorithm works.\n",
    "\n",
    "### What are SHAP Values?\n",
    "\n",
    "SHAP values explain individual predictions by computing the contribution of each feature to the prediction. They answer the question: **\"How much did each feature push the prediction away from the baseline?\"**\n",
    "\n",
    "SHAP is based on **Shapley values** from cooperative game theory, which provide a fair way to distribute credit among team members (or in our case, features).\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Coalition**: A subset of features being considered\n",
    "2. **Marginal Contribution**: The change in prediction when adding a feature to a coalition\n",
    "3. **Shapley Value**: Weighted average of marginal contributions across all possible coalitions\n",
    "4. **Baseline**: A reference prediction (often using mean/median feature values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e94d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from typing import List\n",
    "from math import factorial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c164ea",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Shapley Values with a Simple Example\n",
    "\n",
    "Let's start with a simple linear model to build intuition:\n",
    "\n",
    "**Model**: `prediction = 2×x₁ + 3×x₂ + 1×x₃`\n",
    "\n",
    "We'll calculate how much each feature contributes to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_prediction_function(x1, x2, x3):\n",
    "    \"\"\"\n",
    "    Simple linear model for demonstration.\n",
    "    \n",
    "    prediction = 2*x1 + 3*x2 + 1*x3\n",
    "    \n",
    "    This represents our \"black box\" model that we want to explain.\n",
    "    \"\"\"\n",
    "    return 2*x1 + 3*x2 + 1*x3\n",
    "\n",
    "# Example instance to explain\n",
    "instance = {'x1': 5, 'x2': 3, 'x3': 2}\n",
    "baseline = {'x1': 0, 'x2': 0, 'x3': 0}  # All features at zero\n",
    "\n",
    "# Calculate predictions\n",
    "prediction = simple_prediction_function(**instance)\n",
    "baseline_pred = simple_prediction_function(**baseline)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE INSTANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Instance values: {instance}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Baseline prediction: {baseline_pred}\")\n",
    "print(f\"\\nDifference to explain: {prediction - baseline_pred}\")\n",
    "print(\"\\nGoal: Distribute this difference fairly among features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12bef3f",
   "metadata": {},
   "source": [
    "## Part 2: The Core SHAP Algorithm\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "To calculate the SHAP value for a feature, we:\n",
    "\n",
    "1. **Enumerate all coalitions** (subsets) that DON'T include the feature\n",
    "2. **For each coalition S**:\n",
    "   - Calculate prediction with just coalition S: `v(S)`\n",
    "   - Calculate prediction with S plus our feature: `v(S ∪ {feature})`\n",
    "   - Compute **marginal contribution**: `v(S ∪ {feature}) - v(S)`\n",
    "   - Weight it by coalition probability\n",
    "3. **Sum all weighted contributions**\n",
    "\n",
    "### The Weight Formula\n",
    "\n",
    "For a coalition of size |S| with n total features:\n",
    "\n",
    "**Weight = |S|! × (n - |S| - 1)! / n!**\n",
    "\n",
    "This ensures fair credit distribution (from game theory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_coalitions(features: List[str]) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Generate all possible coalitions (subsets) of features.\n",
    "    \n",
    "    Example: For [A, B, C], generates:\n",
    "    [], [A], [B], [C], [A,B], [A,C], [B,C], [A,B,C]\n",
    "    \"\"\"\n",
    "    coalitions = []\n",
    "    for size in range(len(features) + 1):\n",
    "        for coalition in combinations(features, size):\n",
    "            coalitions.append(coalition)\n",
    "    return coalitions\n",
    "\n",
    "# Test it\n",
    "test_features = ['A', 'B', 'C']\n",
    "all_coalitions = get_all_coalitions(test_features)\n",
    "print(f\"All coalitions for {test_features}:\")\n",
    "for i, coalition in enumerate(all_coalitions):\n",
    "    print(f\"  {i+1}. {list(coalition) if coalition else 'empty'}\")\n",
    "print(f\"\\nTotal coalitions: {len(all_coalitions)} (= 2^{len(test_features)})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c8231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_coalition(model_func, instance, coalition, baseline, all_features):\n",
    "    \"\"\"\n",
    "    Make prediction using only features in the coalition.\n",
    "    \n",
    "    - Features IN coalition: use instance values\n",
    "    - Features NOT in coalition: use baseline values (marginalize)\n",
    "    \n",
    "    This simulates: \"What if we only knew these features?\"\n",
    "    \"\"\"\n",
    "    values = {}\n",
    "    for feature in all_features:\n",
    "        if feature in coalition:\n",
    "            values[feature] = instance[feature]\n",
    "        else:\n",
    "            values[feature] = baseline[feature]\n",
    "    return model_func(**values)\n",
    "\n",
    "# Example: predict with only x1 and x3\n",
    "coalition_example = ['x1', 'x3']\n",
    "pred_example = predict_with_coalition(\n",
    "    simple_prediction_function, \n",
    "    instance, \n",
    "    coalition_example, \n",
    "    baseline, \n",
    "    ['x1', 'x2', 'x3']\n",
    ")\n",
    "\n",
    "print(f\"Coalition: {coalition_example}\")\n",
    "print(f\"Using: x1={instance['x1']}, x2={baseline['x2']} (baseline), x3={instance['x3']}\")\n",
    "print(f\"Prediction: {pred_example}\")\n",
    "print(f\"Calculation: 2×{instance['x1']} + 3×{baseline['x2']} + 1×{instance['x3']} = {pred_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shapley_value(feature: str, model_func, instance, baseline, all_features):\n",
    "    \"\"\"\n",
    "    Calculate exact Shapley value for a single feature.\n",
    "    \n",
    "    Returns:\n",
    "        shapley_value: The SHAP value for this feature\n",
    "        contributions: List of detailed marginal contributions (for debugging)\n",
    "    \"\"\"\n",
    "    n = len(all_features)\n",
    "    other_features = [f for f in all_features if f != feature]\n",
    "    \n",
    "    shapley_value = 0.0\n",
    "    contributions = []\n",
    "    \n",
    "    # Consider all coalitions that DON'T include our feature\n",
    "    for coalition_size in range(n):\n",
    "        for coalition in combinations(other_features, coalition_size):\n",
    "            coalition = list(coalition)\n",
    "            \n",
    "            # Prediction WITHOUT the feature\n",
    "            pred_without = predict_with_coalition(\n",
    "                model_func, instance, coalition, baseline, all_features\n",
    "            )\n",
    "            \n",
    "            # Prediction WITH the feature added\n",
    "            coalition_with_feature = coalition + [feature]\n",
    "            pred_with = predict_with_coalition(\n",
    "                model_func, instance, coalition_with_feature, baseline, all_features\n",
    "            )\n",
    "            \n",
    "            # Marginal contribution of adding this feature\n",
    "            marginal_contribution = pred_with - pred_without\n",
    "            \n",
    "            # Calculate Shapley weight\n",
    "            coalition_size = len(coalition)\n",
    "            weight = (\n",
    "                factorial(coalition_size) * \n",
    "                factorial(n - coalition_size - 1) / \n",
    "                factorial(n)\n",
    "            )\n",
    "            \n",
    "            weighted_contribution = weight * marginal_contribution\n",
    "            shapley_value += weighted_contribution\n",
    "            \n",
    "            contributions.append({\n",
    "                'coalition': coalition,\n",
    "                'pred_without': pred_without,\n",
    "                'pred_with': pred_with,\n",
    "                'marginal': marginal_contribution,\n",
    "                'weight': weight,\n",
    "                'weighted': weighted_contribution\n",
    "            })\n",
    "    \n",
    "    return shapley_value, contributions\n",
    "\n",
    "print(\"✓ Shapley value calculation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644bf27",
   "metadata": {},
   "source": [
    "## Part 3: Detailed Calculation for One Feature\n",
    "\n",
    "Let's trace through the calculation for `x1` step by step to see how SHAP values are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP value for x1\n",
    "all_features = ['x1', 'x2', 'x3']\n",
    "shap_x1, contributions_x1 = calculate_shapley_value(\n",
    "    'x1', \n",
    "    simple_prediction_function, \n",
    "    instance, \n",
    "    baseline, \n",
    "    all_features\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DETAILED CALCULATION FOR x1\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAll marginal contributions:\\n\")\n",
    "\n",
    "# Show all contributions\n",
    "for i, contrib in enumerate(contributions_x1):\n",
    "    coalition_str = str(contrib['coalition']) if contrib['coalition'] else 'empty'\n",
    "    print(f\"Coalition {i+1}: {coalition_str}\")\n",
    "    print(f\"  Pred without x1: {contrib['pred_without']:6.2f}\")\n",
    "    print(f\"  Pred with x1:    {contrib['pred_with']:6.2f}\")\n",
    "    print(f\"  Marginal:        {contrib['marginal']:6.2f}\")\n",
    "    print(f\"  Weight:          {contrib['weight']:6.4f}\")\n",
    "    print(f\"  Weighted:        {contrib['weighted']:6.2f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"FINAL SHAP VALUE for x1: {shap_x1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39477d1",
   "metadata": {},
   "source": [
    "## Part 4: Calculate SHAP Values for All Features\n",
    "\n",
    "Now let's calculate SHAP values for all features and verify the key property: **SHAP values sum to (prediction - baseline)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ad631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for all features\n",
    "shapley_values = {}\n",
    "\n",
    "for feature in all_features:\n",
    "    shap_val, _ = calculate_shapley_value(\n",
    "        feature, \n",
    "        simple_prediction_function, \n",
    "        instance, \n",
    "        baseline, \n",
    "        all_features\n",
    "    )\n",
    "    shapley_values[feature] = shap_val\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL SHAP VALUES\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFeature contributions:\")\n",
    "for feat, val in shapley_values.items():\n",
    "    print(f\"  {feat}: {val:7.2f}\")\n",
    "\n",
    "print(f\"\\nSum of SHAP values:        {sum(shapley_values.values()):.2f}\")\n",
    "print(f\"Prediction - Baseline:     {prediction - baseline_pred:.2f}\")\n",
    "print(f\"\\n{'✓' * 35}\")\n",
    "print(\"VERIFICATION: SHAP values sum to (prediction - baseline)!\")\n",
    "print(f\"{'✓' * 35}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd47810",
   "metadata": {},
   "source": [
    "## Part 5: Visualization\n",
    "\n",
    "A waterfall or bar plot shows how each feature contributes to moving the prediction from baseline to final value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "features = list(shapley_values.keys())\n",
    "values = list(shapley_values.values())\n",
    "colors = ['#2ecc71' if v > 0 else '#e74c3c' for v in values]\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(features, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add zero line\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('SHAP Value (contribution to prediction)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
    "ax.set_title('SHAP Values: Feature Contributions to Prediction', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (feat, val) in enumerate(zip(features, values)):\n",
    "    offset = 0.3 if val > 0 else -0.3\n",
    "    ha = 'left' if val > 0 else 'right'\n",
    "    ax.text(val + offset, i, f'{val:.2f}', \n",
    "            va='center', ha=ha, fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', alpha=0.7, label='Positive (increases prediction)'),\n",
    "    Patch(facecolor='#e74c3c', alpha=0.7, label='Negative (decreases prediction)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • Positive (green): Feature pushes prediction UP\")\n",
    "print(\"  • Negative (red): Feature pushes prediction DOWN\")\n",
    "print(\"  • Magnitude: How strongly the feature influences prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c15c83",
   "metadata": {},
   "source": [
    "## Part 6: Real Machine Learning Example\n",
    "\n",
    "Now let's apply SHAP to a real ML model (Decision Tree) trained on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create synthetic dataset\n",
    "print(\"Creating dataset and training model...\")\n",
    "X, y = make_regression(n_samples=200, n_features=4, noise=15, random_state=42)\n",
    "feature_names = ['Feature_A', 'Feature_B', 'Feature_C', 'Feature_D']\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Train decision tree\n",
    "model = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"✓ Model trained (R² score: {model.score(X, y):.3f})\")\n",
    "print(f\"\\nDataset shape: {X_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select instance to explain\n",
    "instance_idx = 5\n",
    "instance_to_explain = X_df.iloc[instance_idx].to_dict()\n",
    "\n",
    "# Use mean as baseline\n",
    "baseline_values = {feat: X_df[feat].mean() for feat in feature_names}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"INSTANCE TO EXPLAIN (index {instance_idx})\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFeature values:\")\n",
    "for feat, val in instance_to_explain.items():\n",
    "    baseline_val = baseline_values[feat]\n",
    "    diff = val - baseline_val\n",
    "    print(f\"  {feat:12} = {val:7.3f}  (baseline: {baseline_val:6.3f}, diff: {diff:+7.3f})\")\n",
    "\n",
    "# Model wrapper\n",
    "def model_predict(**kwargs):\n",
    "    \"\"\"Wrapper to make sklearn model compatible with our SHAP calculator\"\"\"\n",
    "    feature_values = [kwargs[feat] for feat in feature_names]\n",
    "    return model.predict([feature_values])[0]\n",
    "\n",
    "instance_pred = model_predict(**instance_to_explain)\n",
    "baseline_pred = model_predict(**baseline_values)\n",
    "\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"  Instance:  {instance_pred:7.3f}\")\n",
    "print(f\"  Baseline:  {baseline_pred:7.3f}\")\n",
    "print(f\"  Difference: {instance_pred - baseline_pred:+7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd036ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values for ML model\n",
    "print(\"Calculating SHAP values...\")\n",
    "print(\"(This may take 10-20 seconds due to 2^4 = 16 coalitions per feature)\\n\")\n",
    "\n",
    "ml_shapley_values = {}\n",
    "\n",
    "for i, feature in enumerate(feature_names, 1):\n",
    "    shap_val, _ = calculate_shapley_value(\n",
    "        feature, \n",
    "        model_predict, \n",
    "        instance_to_explain, \n",
    "        baseline_values, \n",
    "        feature_names\n",
    "    )\n",
    "    ml_shapley_values[feature] = shap_val\n",
    "    print(f\"[{i}/4] {feature:12} SHAP value: {shap_val:+7.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Sum of SHAP values:    {sum(ml_shapley_values.values()):7.3f}\")\n",
    "print(f\"Prediction - Baseline: {instance_pred - baseline_pred:7.3f}\")\n",
    "print(f\"Difference:            {abs(sum(ml_shapley_values.values()) - (instance_pred - baseline_pred)):.6f}\")\n",
    "print(\"\\n✓ Local accuracy property satisfied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ce580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ML model SHAP values\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ml_features = list(ml_shapley_values.keys())\n",
    "ml_values = list(ml_shapley_values.values())\n",
    "ml_colors = ['#2ecc71' if v > 0 else '#e74c3c' for v in ml_values]\n",
    "\n",
    "# Sort by absolute value\n",
    "sorted_indices = np.argsort([abs(v) for v in ml_values])\n",
    "ml_features_sorted = [ml_features[i] for i in sorted_indices]\n",
    "ml_values_sorted = [ml_values[i] for i in sorted_indices]\n",
    "ml_colors_sorted = [ml_colors[i] for i in sorted_indices]\n",
    "\n",
    "bars = ax.barh(ml_features_sorted, ml_values_sorted, \n",
    "               color=ml_colors_sorted, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('SHAP Value (contribution to prediction)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'SHAP Values for Decision Tree (Instance {instance_idx})', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for i, (feat, val) in enumerate(zip(ml_features_sorted, ml_values_sorted)):\n",
    "    offset = 0.5 if val > 0 else -0.5\n",
    "    ha = 'left' if val > 0 else 'right'\n",
    "    ax.text(val + offset, i, f'{val:.2f}', \n",
    "            va='center', ha=ha, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance ranking\n",
    "print(\"\\nFeature Importance Ranking (by absolute SHAP value):\")\n",
    "importance_rank = sorted(ml_shapley_values.items(), \n",
    "                         key=lambda x: abs(x[1]), reverse=True)\n",
    "for rank, (feat, val) in enumerate(importance_rank, 1):\n",
    "    print(f\"  {rank}. {feat:12} = {val:+7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3d24d",
   "metadata": {},
   "source": [
    "## Part 7: Understanding Computational Complexity\n",
    "\n",
    "The exact SHAP calculation requires evaluating all possible coalitions.\n",
    "\n",
    "**Complexity**: O(2^n) where n = number of features\n",
    "\n",
    "| Features | Coalitions | Approximate Time |\n",
    "|----------|-----------|------------------|\n",
    "| 4        | 16        | < 1 second      |\n",
    "| 10       | 1,024     | ~1 second       |\n",
    "| 15       | 32,768    | ~30 seconds     |\n",
    "| 20       | 1,048,576 | ~15 minutes     |\n",
    "| 30       | 1B+       | Infeasible      |\n",
    "\n",
    "This is why practical implementations use approximations:\n",
    "- **TreeSHAP**: Polynomial time for tree-based models\n",
    "- **KernelSHAP**: Sampling-based approximation\n",
    "- **Linear SHAP**: Closed-form for linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate complexity\n",
    "def count_coalitions(n_features):\n",
    "    return 2 ** n_features\n",
    "\n",
    "print(\"Number of coalitions to evaluate:\\n\")\n",
    "for n in [4, 6, 8, 10, 12, 15, 20]:\n",
    "    coalitions = count_coalitions(n)\n",
    "    print(f\"  {n:2} features: {coalitions:>10,} coalitions\")\n",
    "    \n",
    "print(\"\\nFor the simple example (3 features):\")\n",
    "print(f\"  Total coalitions: {count_coalitions(3)}\")\n",
    "print(f\"  Coalitions per feature: {count_coalitions(2)} (excluding the feature itself)\")\n",
    "print(f\"  Total evaluations for all features: {3 * count_coalitions(2)} model calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968008a2",
   "metadata": {},
   "source": [
    "## Part 8: Sampling-Based Approximation (Optional)\n",
    "\n",
    "For large feature sets, we can approximate SHAP values by sampling random permutations instead of enumerating all coalitions.\n",
    "\n",
    "**Permutation Algorithm**:\n",
    "1. Randomly order all features\n",
    "2. For each position, compute marginal contribution when our feature appears\n",
    "3. Average across many random orderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_permutation(feature: str, model_func, instance, \n",
    "                                baseline, all_features, n_samples: int = 200):\n",
    "    \"\"\"\n",
    "    Approximate SHAP using random permutations (faster for many features).\n",
    "    \n",
    "    This is similar to the approach used in KernelSHAP.\n",
    "    \"\"\"\n",
    "    n = len(all_features)\n",
    "    other_features = [f for f in all_features if f != feature]\n",
    "    \n",
    "    contributions = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random permutation of other features\n",
    "        perm = list(np.random.permutation(other_features))\n",
    "        \n",
    "        # Try each insertion position\n",
    "        for insert_pos in range(len(perm) + 1):\n",
    "            coalition_before = perm[:insert_pos]\n",
    "            \n",
    "            pred_without = predict_with_coalition(\n",
    "                model_func, instance, coalition_before, baseline, all_features\n",
    "            )\n",
    "            \n",
    "            coalition_with = coalition_before + [feature]\n",
    "            pred_with = predict_with_coalition(\n",
    "                model_func, instance, coalition_with, baseline, all_features\n",
    "            )\n",
    "            \n",
    "            contributions.append(pred_with - pred_without)\n",
    "    \n",
    "    return np.mean(contributions)\n",
    "\n",
    "# Compare exact vs approximate\n",
    "print(\"Comparing Exact vs. Approximate SHAP:\\n\")\n",
    "print(\"Feature      Exact    Approx   Difference\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for feature in all_features:\n",
    "    exact = shapley_values[feature]\n",
    "    approx = calculate_shap_permutation(\n",
    "        feature, simple_prediction_function, instance, \n",
    "        baseline, all_features, n_samples=500\n",
    "    )\n",
    "    diff = abs(exact - approx)\n",
    "    print(f\"{feature:8}  {exact:7.3f}  {approx:7.3f}  {diff:7.4f}\")\n",
    "\n",
    "print(\"\\n✓ Approximation is very close to exact values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3036ea",
   "metadata": {},
   "source": [
    "## Part 9: SHAP Properties and Theory\n",
    "\n",
    "SHAP values satisfy three important axioms from cooperative game theory:\n",
    "\n",
    "### 1. Local Accuracy (Efficiency)\n",
    "**Sum of SHAP values = Prediction - Baseline**\n",
    "\n",
    "The SHAP values completely explain the difference between the prediction and baseline.\n",
    "\n",
    "### 2. Missingness\n",
    "**If a feature is absent (at baseline), its SHAP value is 0**\n",
    "\n",
    "Features that don't differ from baseline don't contribute.\n",
    "\n",
    "### 3. Consistency\n",
    "**If a model changes so a feature has larger marginal contributions, its SHAP value doesn't decrease**\n",
    "\n",
    "SHAP values respect the feature's actual importance.\n",
    "\n",
    "### Comparison with Other Methods\n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|--------|------|------|\n",
    "| **SHAP** | Theoretically grounded, local explanations, additive | Computationally expensive |\n",
    "| **LIME** | Fast, model-agnostic | No theoretical guarantees, unstable |\n",
    "| **Feature Importance** | Fast, global view | Not instance-specific |\n",
    "| **Partial Dependence** | Shows average effect | Not for individual predictions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify local accuracy property\n",
    "print(\"=\" * 70)\n",
    "print(\"VERIFICATION OF LOCAL ACCURACY PROPERTY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Simple Linear Model:\")\n",
    "sum_shap_simple = sum(shapley_values.values())\n",
    "diff_simple = prediction - baseline_pred\n",
    "print(f\"   Sum of SHAP values:    {sum_shap_simple:.6f}\")\n",
    "print(f\"   Prediction - Baseline: {diff_simple:.6f}\")\n",
    "print(f\"   Difference:            {abs(sum_shap_simple - diff_simple):.10f}\")\n",
    "\n",
    "print(\"\\n2. ML Model (Decision Tree):\")\n",
    "sum_shap_ml = sum(ml_shapley_values.values())\n",
    "diff_ml = instance_pred - baseline_pred\n",
    "print(f\"   Sum of SHAP values:    {sum_shap_ml:.6f}\")\n",
    "print(f\"   Prediction - Baseline: {diff_ml:.6f}\")\n",
    "print(f\"   Difference:            {abs(sum_shap_ml - diff_ml):.10f}\")\n",
    "\n",
    "print(\"\\n✓ Both models satisfy local accuracy!\")\n",
    "print(\"  (Small numerical differences are due to floating-point precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaac8bc",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **SHAP Algorithm**: \n",
    "   - Enumerate all coalitions (feature subsets)\n",
    "   - Compute marginal contribution of each feature\n",
    "   - Weight by coalition probability\n",
    "   - Sum to get final SHAP value\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - Positive SHAP → feature increases prediction\n",
    "   - Negative SHAP → feature decreases prediction\n",
    "   - Magnitude → strength of influence\n",
    "\n",
    "3. **Properties**:\n",
    "   - Local accuracy: values sum to (prediction - baseline)\n",
    "   - Theoretically grounded in game theory\n",
    "   - Model-agnostic: works with any ML model\n",
    "\n",
    "4. **Computational Cost**:\n",
    "   - Exact: O(2^n) - exponential in features\n",
    "   - Approximations needed for >15 features\n",
    "   - TreeSHAP, KernelSHAP provide efficient alternatives\n",
    "\n",
    "### When to Use SHAP\n",
    "\n",
    "✅ **Good for**:\n",
    "- Explaining individual predictions\n",
    "- Understanding feature importance locally\n",
    "- Debugging model behavior\n",
    "- Building trust in model decisions\n",
    "\n",
    "❌ **Limitations**:\n",
    "- Computationally expensive for many features\n",
    "- Requires choosing appropriate baseline\n",
    "- Doesn't capture feature interactions explicitly\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Try **TreeSHAP** for tree-based models (XGBoost, Random Forest)\n",
    "2. Use **KernelSHAP** for general models with many features\n",
    "3. Compare SHAP with LIME for your use case\n",
    "4. Explore SHAP interaction values for feature pairs\n",
    "\n",
    "### References\n",
    "\n",
    "- Original paper: [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874)\n",
    "- SHAP library: https://github.com/slundberg/shap\n",
    "- Game theory background: [Shapley Value](https://en.wikipedia.org/wiki/Shapley_value)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
